<?xml version="1.0" encoding="UTF-8"?>
<robot rpa="false" generated="20191107 05:49:37.060" generator="Robot 3.1.1 (Python 2.7.5 on linux2)">
<suite source="/opt/ozone/smoketest/mapreduce.robot" id="s1" name="ozonesecure-mr-mapreduce">
<test id="s1-t1" name="Execute PI calculation">
<kw name="Execute" library="commonlib">
<arguments>
<arg>yarn jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-${hadoop.version}.jar pi 3 3</arg>
</arguments>
<assign>
<var>${output}</var>
</assign>
<kw name="Run And Return Rc And Output" library="OperatingSystem">
<doc>Runs the given command in the system and returns the RC and output.</doc>
<arguments>
<arg>${command}</arg>
</arguments>
<assign>
<var>${rc}</var>
<var>${output}</var>
</assign>
<msg timestamp="20191107 05:49:37.120" level="INFO">Running command 'yarn jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar pi 3 3 2&gt;&amp;1'.</msg>
<msg timestamp="20191107 05:49:52.108" level="INFO">${rc} = 1</msg>
<msg timestamp="20191107 05:49:52.108" level="INFO">${output} = Number of Maps  = 3
Samples per Map = 3
2019-11-07 05:49:40 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
2019-11-07 05:49:40 INFO  MetricsSystemImpl:374 - Scheduled Metr...</msg>
<status status="PASS" endtime="20191107 05:49:52.108" starttime="20191107 05:49:37.118"></status>
</kw>
<kw name="Log" library="BuiltIn">
<doc>Logs the given message with the given level.</doc>
<arguments>
<arg>${output}</arg>
</arguments>
<msg timestamp="20191107 05:49:52.111" level="INFO">Number of Maps  = 3
Samples per Map = 3
2019-11-07 05:49:40 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
2019-11-07 05:49:40 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
2019-11-07 05:49:40 INFO  MetricsSystemImpl:191 - XceiverClientMetrics metrics system started
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Starting Job
2019-11-07 05:49:42 INFO  RMProxy:133 - Connecting to ResourceManager at rm/172.18.0.11:8032
2019-11-07 05:49:42 INFO  AHSProxy:42 - Connecting to Application History server at jhs/172.18.0.5:10200
2019-11-07 05:49:42 INFO  KMSClientProvider:1041 - New token created: (Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105782872, maxDate=1573710582872, sequenceNumber=1, masterKeyId=2))
2019-11-07 05:49:42 INFO  TokenCache:147 - Got dt for o3fs://bucket1.vol1; Kind: OzoneToken, Service: 172.18.0.4:9862, Ident: (OzoneToken owner=hadoop/rm@EXAMPLE.COM, renewer=rm, realUser=, issueDate=1573105782561, maxDate=1573710582561, sequenceNumber=1, masterKeyId=1, strToSign=null, signature=null, awsAccessKeyId=null)
2019-11-07 05:49:42 INFO  TokenCache:147 - Got dt for o3fs://bucket1.vol1; Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105782872, maxDate=1573710582872, sequenceNumber=1, masterKeyId=2)
2019-11-07 05:49:43 INFO  FileInputFormat:292 - Total input files to process : 3
2019-11-07 05:49:43 INFO  JobSubmitter:202 - number of splits:3
2019-11-07 05:49:43 INFO  deprecation:1394 - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2019-11-07 05:49:43 INFO  JobSubmitter:298 - Submitting tokens for job: job_1573105732943_0001
2019-11-07 05:49:43 INFO  JobSubmitter:299 - Executing with tokens: [Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105782872, maxDate=1573710582872, sequenceNumber=1, masterKeyId=2), Kind: OzoneToken, Service: 172.18.0.4:9862, Ident: (OzoneToken owner=hadoop/rm@EXAMPLE.COM, renewer=rm, realUser=, issueDate=1573105782561, maxDate=1573710582561, sequenceNumber=1, masterKeyId=1, strToSign=null, signature=null, awsAccessKeyId=null)]
2019-11-07 05:49:44 INFO  Configuration:2752 - resource-types.xml not found
2019-11-07 05:49:44 INFO  ResourceUtils:419 - Unable to find 'resource-types.xml'.
2019-11-07 05:49:44 INFO  TimelineClientImpl:129 - Timeline service address: jhs:8188
2019-11-07 05:49:46 INFO  YarnClientImpl:311 - Submitted application application_1573105732943_0001
2019-11-07 05:49:46 INFO  Job:1574 - The url to track the job: http://rm:8088/proxy/application_1573105732943_0001/
2019-11-07 05:49:46 INFO  Job:1619 - Running job: job_1573105732943_0001
2019-11-07 05:49:51 INFO  Job:1640 - Job job_1573105732943_0001 running in uber mode : false
2019-11-07 05:49:51 INFO  Job:1647 -  map 0% reduce 0%
2019-11-07 05:49:51 INFO  Job:1660 - Job job_1573105732943_0001 failed with state FAILED due to: Application application_1573105732943_0001 failed 2 times due to AM Container for appattempt_1573105732943_0001_000002 exited with  exitCode: -1000
Failing this attempt.Diagnostics: [2019-11-07 05:49:50.969]
java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1748)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	... 10 more
Caused by: org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2146)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:378)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	... 17 more
Caused by: Download and unpack failed
org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2146)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:378)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	... 17 more
Caused by: Failed to authenticate with GRPC XceiverServer with Ozone block token.
org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2146)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:378)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

For more detailed output, check the application tracking page: http://rm:8088/cluster/app/application_1573105732943_0001 Then click on links to logs of each attempt.
. Failing the application.
2019-11-07 05:49:51 INFO  Job:1665 - Counters: 0
Job job_1573105732943_0001 failed!</msg>
<status status="PASS" endtime="20191107 05:49:52.111" starttime="20191107 05:49:52.109"></status>
</kw>
<kw name="Should Be Equal As Integers" library="BuiltIn">
<doc>Fails if objects are unequal after converting them to integers.</doc>
<arguments>
<arg>${rc}</arg>
<arg>0</arg>
</arguments>
<msg timestamp="20191107 05:49:52.112" level="INFO">Argument types are:
&lt;type 'int'&gt;
&lt;type 'unicode'&gt;</msg>
<msg timestamp="20191107 05:49:52.113" level="FAIL">1 != 0</msg>
<status status="FAIL" endtime="20191107 05:49:52.114" starttime="20191107 05:49:52.112"></status>
</kw>
<status status="FAIL" endtime="20191107 05:49:52.114" starttime="20191107 05:49:37.117"></status>
</kw>
<timeout value="4 minutes"></timeout>
<status status="FAIL" endtime="20191107 05:49:52.114" critical="yes" starttime="20191107 05:49:37.116">1 != 0</status>
</test>
<test id="s1-t2" name="Execute WordCount">
<kw name="Generate Random String" library="String">
<doc>Generates a string with a desired ``length`` from the given ``chars``.</doc>
<arguments>
<arg>2</arg>
<arg>[NUMBERS]</arg>
</arguments>
<assign>
<var>${random}</var>
</assign>
<msg timestamp="20191107 05:49:52.117" level="INFO">${random} = 26</msg>
<status status="PASS" endtime="20191107 05:49:52.117" starttime="20191107 05:49:52.116"></status>
</kw>
<kw name="Execute" library="commonlib">
<arguments>
<arg>yarn jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-${hadoop.version}.jar wordcount o3fs://bucket1.vol1/key1 o3fs://bucket1.vol1/key1-${random}.count</arg>
</arguments>
<assign>
<var>${output}</var>
</assign>
<kw name="Run And Return Rc And Output" library="OperatingSystem">
<doc>Runs the given command in the system and returns the RC and output.</doc>
<arguments>
<arg>${command}</arg>
</arguments>
<assign>
<var>${rc}</var>
<var>${output}</var>
</assign>
<msg timestamp="20191107 05:49:52.120" level="INFO">Running command 'yarn jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar wordcount o3fs://bucket1.vol1/key1 o3fs://bucket1.vol1/key1-26.count 2&gt;&amp;1'.</msg>
<msg timestamp="20191107 05:50:01.294" level="INFO">${rc} = 1</msg>
<msg timestamp="20191107 05:50:01.294" level="INFO">${output} = 2019-11-07 05:49:54 INFO  RMProxy:133 - Connecting to ResourceManager at rm/172.18.0.11:8032
2019-11-07 05:49:55 INFO  AHSProxy:42 - Connecting to Application History server at jhs/172.18.0.5:10200
20...</msg>
<status status="PASS" endtime="20191107 05:50:01.294" starttime="20191107 05:49:52.118"></status>
</kw>
<kw name="Log" library="BuiltIn">
<doc>Logs the given message with the given level.</doc>
<arguments>
<arg>${output}</arg>
</arguments>
<msg timestamp="20191107 05:50:01.296" level="INFO">2019-11-07 05:49:54 INFO  RMProxy:133 - Connecting to ResourceManager at rm/172.18.0.11:8032
2019-11-07 05:49:55 INFO  AHSProxy:42 - Connecting to Application History server at jhs/172.18.0.5:10200
2019-11-07 05:49:55 INFO  KMSClientProvider:1041 - New token created: (Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105795251, maxDate=1573710595251, sequenceNumber=2, masterKeyId=2))
2019-11-07 05:49:55 INFO  TokenCache:147 - Got dt for o3fs://bucket1.vol1; Kind: OzoneToken, Service: 172.18.0.4:9862, Ident: (OzoneToken owner=hadoop/rm@EXAMPLE.COM, renewer=rm, realUser=, issueDate=1573105795094, maxDate=1573710595094, sequenceNumber=2, masterKeyId=1, strToSign=null, signature=null, awsAccessKeyId=null)
2019-11-07 05:49:55 INFO  TokenCache:147 - Got dt for o3fs://bucket1.vol1; Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105795251, maxDate=1573710595251, sequenceNumber=2, masterKeyId=2)
2019-11-07 05:49:55 INFO  JobSubmissionFiles:156 - Permissions on staging directory /user/hadoop/.staging are incorrect: rwxrwxrwx. Fixing permissions to correct value rwx------
2019-11-07 05:49:55 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
2019-11-07 05:49:55 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
2019-11-07 05:49:55 INFO  MetricsSystemImpl:191 - XceiverClientMetrics metrics system started
2019-11-07 05:49:56 INFO  FileInputFormat:292 - Total input files to process : 1
2019-11-07 05:49:57 INFO  JobSubmitter:202 - number of splits:1
2019-11-07 05:49:57 INFO  deprecation:1394 - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2019-11-07 05:49:57 INFO  JobSubmitter:298 - Submitting tokens for job: job_1573105732943_0002
2019-11-07 05:49:57 INFO  JobSubmitter:299 - Executing with tokens: [Kind: kms-dt, Service: kms://http@kms:9600/kms, Ident: (kms-dt owner=hadoop, renewer=rm, realUser=, issueDate=1573105795251, maxDate=1573710595251, sequenceNumber=2, masterKeyId=2), Kind: OzoneToken, Service: 172.18.0.4:9862, Ident: (OzoneToken owner=hadoop/rm@EXAMPLE.COM, renewer=rm, realUser=, issueDate=1573105795094, maxDate=1573710595094, sequenceNumber=2, masterKeyId=1, strToSign=null, signature=null, awsAccessKeyId=null)]
2019-11-07 05:49:57 INFO  Configuration:2752 - resource-types.xml not found
2019-11-07 05:49:57 INFO  ResourceUtils:419 - Unable to find 'resource-types.xml'.
2019-11-07 05:49:57 INFO  TimelineClientImpl:129 - Timeline service address: jhs:8188
2019-11-07 05:49:58 INFO  YarnClientImpl:311 - Submitted application application_1573105732943_0002
2019-11-07 05:49:58 INFO  Job:1574 - The url to track the job: http://rm:8088/proxy/application_1573105732943_0002/
2019-11-07 05:49:58 INFO  Job:1619 - Running job: job_1573105732943_0002
2019-11-07 05:50:00 INFO  Job:1640 - Job job_1573105732943_0002 running in uber mode : false
2019-11-07 05:50:00 INFO  Job:1647 -  map 0% reduce 0%
2019-11-07 05:50:00 INFO  Job:1660 - Job job_1573105732943_0002 failed with state FAILED due to: Application application_1573105732943_0002 failed 2 times due to AM Container for appattempt_1573105732943_0002_000002 exited with  exitCode: -1000
Failing this attempt.Diagnostics: java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	... 10 more
Caused by: org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.commons.io.input.ProxyInputStream.read(ProxyInputStream.java:100)
	at org.apache.commons.io.input.TeeInputStream.read(TeeInputStream.java:129)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:186)
	at java.util.zip.ZipInputStream.readFully(ZipInputStream.java:403)
	at java.util.zip.ZipInputStream.readLOC(ZipInputStream.java:278)
	at java.util.zip.ZipInputStream.getNextEntry(ZipInputStream.java:122)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:83)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:62)
	at org.apache.hadoop.util.RunJar.unJar(RunJar.java:118)
	at org.apache.hadoop.util.RunJar.unJarAndSave(RunJar.java:171)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:354)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	... 17 more
Caused by: Download and unpack failed
org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.commons.io.input.ProxyInputStream.read(ProxyInputStream.java:100)
	at org.apache.commons.io.input.TeeInputStream.read(TeeInputStream.java:129)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:186)
	at java.util.zip.ZipInputStream.readFully(ZipInputStream.java:403)
	at java.util.zip.ZipInputStream.readLOC(ZipInputStream.java:278)
	at java.util.zip.ZipInputStream.getNextEntry(ZipInputStream.java:122)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:83)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:62)
	at org.apache.hadoop.util.RunJar.unJar(RunJar.java:118)
	at org.apache.hadoop.util.RunJar.unJarAndSave(RunJar.java:171)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:354)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	... 17 more
Caused by: Failed to authenticate with GRPC XceiverServer with Ozone block token.
org.apache.hadoop.hdds.security.exception.SCMSecurityException: Failed to authenticate with GRPC XceiverServer with Ozone block token.
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:340)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:268)
	at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:251)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.commons.io.input.ProxyInputStream.read(ProxyInputStream.java:100)
	at org.apache.commons.io.input.TeeInputStream.read(TeeInputStream.java:129)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:186)
	at java.util.zip.ZipInputStream.readFully(ZipInputStream.java:403)
	at java.util.zip.ZipInputStream.readLOC(ZipInputStream.java:278)
	at java.util.zip.ZipInputStream.getNextEntry(ZipInputStream.java:122)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:83)
	at java.util.jar.JarInputStream.&lt;init&gt;(JarInputStream.java:62)
	at org.apache.hadoop.util.RunJar.unJar(RunJar.java:118)
	at org.apache.hadoop.util.RunJar.unJarAndSave(RunJar.java:171)
	at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:354)
	at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:303)
	at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

For more detailed output, check the application tracking page: http://rm:8088/cluster/app/application_1573105732943_0002 Then click on links to logs of each attempt.
. Failing the application.
2019-11-07 05:50:00 INFO  Job:1665 - Counters: 0</msg>
<status status="PASS" endtime="20191107 05:50:01.297" starttime="20191107 05:50:01.295"></status>
</kw>
<kw name="Should Be Equal As Integers" library="BuiltIn">
<doc>Fails if objects are unequal after converting them to integers.</doc>
<arguments>
<arg>${rc}</arg>
<arg>0</arg>
</arguments>
<msg timestamp="20191107 05:50:01.298" level="INFO">Argument types are:
&lt;type 'int'&gt;
&lt;type 'unicode'&gt;</msg>
<msg timestamp="20191107 05:50:01.299" level="FAIL">1 != 0</msg>
<status status="FAIL" endtime="20191107 05:50:01.299" starttime="20191107 05:50:01.297"></status>
</kw>
<status status="FAIL" endtime="20191107 05:50:01.299" starttime="20191107 05:49:52.117"></status>
</kw>
<timeout value="4 minutes"></timeout>
<status status="FAIL" endtime="20191107 05:50:01.300" critical="yes" starttime="20191107 05:49:52.115">1 != 0</status>
</test>
<doc>Execute MR jobs</doc>
<status status="FAIL" endtime="20191107 05:50:01.301" starttime="20191107 05:49:37.061"></status>
</suite>
<statistics>
<total>
<stat fail="2" pass="0">Critical Tests</stat>
<stat fail="2" pass="0">All Tests</stat>
</total>
<tag>
</tag>
<suite>
<stat fail="2" id="s1" name="ozonesecure-mr-mapreduce" pass="0">ozonesecure-mr-mapreduce</stat>
</suite>
</statistics>
<errors>
</errors>
</robot>
