2019-10-29 03:26:50,009 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/hdds of  storage type : DISK and capacity : 1482555576320
2019-10-29 03:26:50,018 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/hdds to VolumeSet
2019-10-29 03:26:50,027 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/hdds
2019-10-29 03:26:50,049 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(204)) - Scheduled health check for volume /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/hdds
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
2019-10-29 03:26:51,254 [main] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(134)) - Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
2019-10-29 03:26:51,276 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 10 second(s).
2019-10-29 03:26:51,276 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - XceiverClientMetrics metrics system started
2019-10-29 03:26:52,220 [grpc-default-executor-1] ERROR impl.ChunkManagerImpl (ChunkUtils.java:validateBufferSize(134)) - data array does not match the length specified. DataLen: 1024 Byte Array: 0
2019-10-29 03:26:52,285 [grpc-default-executor-1] INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: WriteChunk : Trace ID:  : Message: data array does not match the length specified. DataLen: 1024 Byte Array: 0 : Result: INVALID_WRITE_SIZE
03:26:52.309 [grpc-default-executor-1] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 1572319609697 locID: 1572319612181 bcsId: 0} | ret=FAILURE
java.lang.Exception: data array does not match the length specified. DataLen: 1024 Byte Array: 0
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:332) [hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:150) [hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73) [hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61) [hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46) [hadoop-hdds-common-0.5.0-SNAPSHOT.jar:?]
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) [ratis-thirdparty-misc-0.2.0.jar:0.2.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_222]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_222]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
2019-10-29 03:26:52,364 [shutdown-hook-0] WARN  volume.VolumeUsage (VolumeUsage.java:saveScmUsed(176)) - Failed to write scmUsed to /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/scmUsed
java.io.FileNotFoundException: /workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ/scmUsed (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at org.apache.hadoop.ozone.container.common.volume.VolumeUsage.saveScmUsed(VolumeUsage.java:165)
	at org.apache.hadoop.ozone.container.common.volume.VolumeUsage.shutdown(VolumeUsage.java:101)
	at org.apache.hadoop.ozone.container.common.volume.VolumeInfo.shutdownUsageThread(VolumeInfo.java:115)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.shutdown(HddsVolume.java:404)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.saveVolumeSetUsed(VolumeSet.java:412)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.lambda$initializeVolumeSet$2(VolumeSet.java:198)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-29 03:26:52,369 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/UsqOVpHQqZ] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
