Attaching to ozone-om-ha_datanode_1, ozone-om-ha_datanode_3, ozone-om-ha_om3_1, ozone-om-ha_datanode_2, ozone-om-ha_om1_1, ozone-om-ha_scm_1, ozone-om-ha_om2_1
datanode_1  | 2019-11-14 05:58:32 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = 19850cfb177b/172.18.0.7
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 3.2.0
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om3_1       | 2019-11-14 05:58:32 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
datanode_1  | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2019-11-14 05:58:33 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_3  | /************************************************************
om3_1       | /************************************************************
datanode_1  | ************************************************************/
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
om3_1       | STARTUP_MSG: Starting OzoneManager
datanode_1  | 2019-11-14 05:58:32 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | STARTUP_MSG:   host = bf1667f6f477/172.18.0.8
om3_1       | STARTUP_MSG:   host = 1d0f33446472/172.18.0.6
datanode_2  | 2019-11-14 05:58:31 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_1  | 2019-11-14 05:58:32 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3  | STARTUP_MSG:   args = []
om3_1       | STARTUP_MSG:   args = [--init]
datanode_2  | /************************************************************
om1_1       | 2019-11-14 05:58:31 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
datanode_3  | STARTUP_MSG:   version = 3.2.0
datanode_1  | 2019-11-14 05:58:32 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om3_1       | STARTUP_MSG:   version = 3.2.0
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
om1_1       | /************************************************************
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
scm_1       | 2019-11-14 05:58:32 INFO  StorageContainerManagerStarter:51 - STARTUP_MSG: 
om2_1       | 2019-11-14 05:58:30 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   host = 4b9d8a37cd03/172.18.0.5
om1_1       | STARTUP_MSG: Starting OzoneManager
datanode_1  | 2019-11-14 05:58:32 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | /************************************************************
om2_1       | /************************************************************
om3_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2  | STARTUP_MSG:   args = []
datanode_1  | 2019-11-14 05:58:33 INFO  HddsDatanodeService:186 - HddsDatanodeService host:19850cfb177b ip:172.18.0.7
om1_1       | STARTUP_MSG:   host = d53a61f11aa1/172.18.0.4
datanode_3  | STARTUP_MSG:   java = 11.0.3
scm_1       | STARTUP_MSG: Starting StorageContainerManager
om2_1       | STARTUP_MSG: Starting OzoneManager
om3_1       | STARTUP_MSG:   java = 11.0.3
datanode_2  | STARTUP_MSG:   version = 3.2.0
datanode_1  | 2019-11-14 05:58:33 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
om1_1       | STARTUP_MSG:   args = [--init]
datanode_3  | ************************************************************/
scm_1       | STARTUP_MSG:   host = 2a59cb6a22f9/172.18.0.3
om2_1       | STARTUP_MSG:   host = 1beeed8b4e70/172.18.0.2
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
om3_1       | ************************************************************/
datanode_1  | 2019-11-14 05:58:33 INFO  VolumeSet:170 - Added Volume : /data/hdds/hdds to VolumeSet
om1_1       | STARTUP_MSG:   version = 3.2.0
datanode_3  | 2019-11-14 05:58:33 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | STARTUP_MSG:   args = [--init]
om2_1       | STARTUP_MSG:   args = [--init]
om2_1       | STARTUP_MSG:   version = 3.2.0
om3_1       | 2019-11-14 05:58:32 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2019-11-14 05:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar
datanode_3  | 2019-11-14 05:58:33 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1       | STARTUP_MSG:   version = 3.2.0
om3_1       | 2019-11-14 05:58:33 INFO  OMHANodeDetails:165 - Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
datanode_1  | 2019-11-14 05:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar
om1_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hamcrest-all-1.3.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar
datanode_3  | 2019-11-14 05:58:33 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2019-11-14 05:58:34 WARN  HddsServerUtil:354 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om1_1       | STARTUP_MSG:   java = 11.0.3
datanode_2  | STARTUP_MSG:   java = 11.0.3
om3_1       | 2019-11-14 05:58:33 INFO  OMHANodeDetails:298 - Setting configuration key ozone.om.address with value of key ozone.om.address.om3: om3
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1  | 2019-11-14 05:58:34 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
datanode_3  | 2019-11-14 05:58:33 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
om2_1       | STARTUP_MSG:   java = 11.0.3
om1_1       | ************************************************************/
om1_1       | 2019-11-14 05:58:31 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | 2019-11-14 05:58:33 WARN  ServerUtils:222 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | STARTUP_MSG:   java = 11.0.3
datanode_1  | 2019-11-14 05:58:34 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
om2_1       | ************************************************************/
datanode_3  | 2019-11-14 05:58:33 INFO  HddsDatanodeService:186 - HddsDatanodeService host:bf1667f6f477 ip:172.18.0.8
datanode_2  | ************************************************************/
om3_1       | 2019-11-14 05:58:34 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2019-11-14 05:58:32 INFO  OMHANodeDetails:165 - Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
scm_1       | ************************************************************/
datanode_1  | 2019-11-14 05:58:34 INFO  GrpcService:43 - raft.grpc.message.size.max = 33570816 (custom)
om2_1       | 2019-11-14 05:58:30 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2019-11-14 05:58:33 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
datanode_2  | 2019-11-14 05:58:31 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | 2019-11-14 05:58:35 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2019-11-14 05:58:34 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2019-11-14 05:58:32 INFO  OMHANodeDetails:298 - Setting configuration key ozone.om.address with value of key ozone.om.address.om1: om1
scm_1       | 2019-11-14 05:58:32 INFO  StorageContainerManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
om2_1       | 2019-11-14 05:58:31 INFO  OMHANodeDetails:165 - Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
datanode_3  | 2019-11-14 05:58:33 INFO  VolumeSet:170 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2019-11-14 05:58:31 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om3_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-10ce908a-fa6a-4fff-a7e7-971fd046464b
datanode_1  | 2019-11-14 05:58:34 INFO  GrpcService:43 - raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1       | 2019-11-14 05:58:32 WARN  ServerUtils:222 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2019-11-14 05:58:32 WARN  ServerUtils:145 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_3  | 2019-11-14 05:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:31 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2019-11-14 05:58:34 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
om3_1       | 2019-11-14 05:58:36 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
om2_1       | 2019-11-14 05:58:31 INFO  OMHANodeDetails:298 - Setting configuration key ozone.om.address with value of key ozone.om.address.om2: om2
om1_1       | 2019-11-14 05:58:33 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
scm_1       | SCM initialization succeeded.Current cluster id for sd=/data/metadata/scm;cid=CID-10ce908a-fa6a-4fff-a7e7-971fd046464b
datanode_3  | 2019-11-14 05:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:31 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1  | 2019-11-14 05:58:34 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | /************************************************************
om2_1       | 2019-11-14 05:58:31 WARN  ServerUtils:222 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2019-11-14 05:58:32 INFO  StorageContainerManagerStarter:51 - SHUTDOWN_MSG: 
datanode_3  | 2019-11-14 05:58:34 WARN  HddsServerUtil:354 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 1d0f33446472/172.18.0.6
datanode_1  | 2019-11-14 05:58:34 INFO  DFSUtil:1641 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2  | 2019-11-14 05:58:32 INFO  HddsDatanodeService:186 - HddsDatanodeService host:4b9d8a37cd03 ip:172.18.0.5
om1_1       | 2019-11-14 05:58:34 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
scm_1       | /************************************************************
datanode_3  | 2019-11-14 05:58:34 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
om3_1       | ************************************************************/
datanode_1  | 2019-11-14 05:58:34 INFO  log:192 - Logging initialized @3417ms
om1_1       | 2019-11-14 05:58:35 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_2  | 2019-11-14 05:58:32 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 2a59cb6a22f9/172.18.0.3
om2_1       | 2019-11-14 05:58:33 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_3  | 2019-11-14 05:58:35 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
datanode_1  | 2019-11-14 05:58:34 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-10ce908a-fa6a-4fff-a7e7-971fd046464b
datanode_2  | 2019-11-14 05:58:32 INFO  VolumeSet:170 - Added Volume : /data/hdds/hdds to VolumeSet
scm_1       | ************************************************************/
om2_1       | 2019-11-14 05:58:34 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_3  | 2019-11-14 05:58:35 INFO  GrpcService:43 - raft.grpc.message.size.max = 33570816 (custom)
datanode_1  | 2019-11-14 05:58:34 INFO  HttpRequestLog:81 - Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2019-11-14 05:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
om1_1       | 2019-11-14 05:58:36 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
scm_1       | 2019-11-14 05:58:33 INFO  StorageContainerManagerStarter:51 - STARTUP_MSG: 
om2_1       | 2019-11-14 05:58:35 INFO  Client:948 - Retrying connect to server: scm/172.18.0.3:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_3  | 2019-11-14 05:58:35 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2019-11-14 05:58:34 INFO  HttpServer2:975 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2019-11-14 05:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
om1_1       | /************************************************************
scm_1       | /************************************************************
om2_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-10ce908a-fa6a-4fff-a7e7-971fd046464b
datanode_3  | 2019-11-14 05:58:35 INFO  GrpcService:43 - raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_1  | 2019-11-14 05:58:34 INFO  HttpServer2:948 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2019-11-14 05:58:33 WARN  HddsServerUtil:354 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at d53a61f11aa1/172.18.0.4
scm_1       | STARTUP_MSG: Starting StorageContainerManager
om2_1       | 2019-11-14 05:58:36 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
datanode_3  | 2019-11-14 05:58:35 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
datanode_1  | 2019-11-14 05:58:34 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2019-11-14 05:58:33 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
om1_1       | ************************************************************/
scm_1       | STARTUP_MSG:   host = 2a59cb6a22f9/172.18.0.3
om2_1       | /************************************************************
datanode_3  | 2019-11-14 05:58:35 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2019-11-14 05:58:34 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2019-11-14 05:58:33 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
scm_1       | STARTUP_MSG:   args = []
datanode_3  | 2019-11-14 05:58:35 INFO  DFSUtil:1641 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om2_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 1beeed8b4e70/172.18.0.2
datanode_1  | 2019-11-14 05:58:34 WARN  BaseHttpServer:111 - /prof java profiling servlet is activated. Not safe for production!
datanode_2  | 2019-11-14 05:58:33 INFO  GrpcService:43 - raft.grpc.message.size.max = 33570816 (custom)
scm_1       | STARTUP_MSG:   version = 3.2.0
datanode_3  | 2019-11-14 05:58:35 INFO  log:192 - Logging initialized @3032ms
om2_1       | ************************************************************/
datanode_1  | 2019-11-14 05:58:34 INFO  HttpServer2:1191 - Jetty bound to port 9882
datanode_2  | 2019-11-14 05:58:33 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hamcrest-all-1.3.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar
datanode_3  | 2019-11-14 05:58:35 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1  | 2019-11-14 05:58:34 INFO  Server:351 - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_2  | 2019-11-14 05:58:33 INFO  GrpcService:43 - raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_3  | 2019-11-14 05:58:35 INFO  HttpRequestLog:81 - Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2019-11-14 05:58:34 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@4c1bdcc2{/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | 2019-11-14 05:58:35 INFO  HttpServer2:975 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2019-11-14 05:58:33 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
scm_1       | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2019-11-14 05:58:35 INFO  HttpServer2:948 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2019-11-14 05:58:33 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2019-11-14 05:58:34 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@252a8aae{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1       | ************************************************************/
datanode_3  | 2019-11-14 05:58:35 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2019-11-14 05:58:33 INFO  StorageContainerManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2019-11-14 05:58:33 INFO  DFSUtil:1641 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3  | 2019-11-14 05:58:35 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.w.WebAppContext@4601203a{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-15331450709810569222.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_2  | 2019-11-14 05:58:33 INFO  log:192 - Logging initialized @3516ms
scm_1       | 2019-11-14 05:58:33 WARN  ServerUtils:145 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_3  | 2019-11-14 05:58:35 WARN  BaseHttpServer:111 - /prof java profiling servlet is activated. Not safe for production!
datanode_1  | 2019-11-14 05:58:35 INFO  AbstractConnector:278 - Started ServerConnector@10db09{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2019-11-14 05:58:33 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2019-11-14 05:58:33 WARN  ServerUtils:145 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_3  | 2019-11-14 05:58:35 INFO  HttpServer2:1191 - Jetty bound to port 9882
datanode_1  | 2019-11-14 05:58:35 INFO  Server:419 - Started @3712ms
datanode_2  | 2019-11-14 05:58:33 INFO  HttpRequestLog:81 - Http request log for http.requests.hddsDatanode is not defined
scm_1       | 2019-11-14 05:58:33 INFO  log:192 - Logging initialized @1226ms
datanode_3  | 2019-11-14 05:58:35 INFO  Server:351 - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_1  | 2019-11-14 05:58:35 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2  | 2019-11-14 05:58:33 INFO  HttpServer2:975 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:127 - using custom profile for table: deletedBlocks
datanode_3  | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@4c1bdcc2{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2019-11-14 05:58:35 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2  | 2019-11-14 05:58:33 INFO  HttpServer2:948 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:183 - Using default column profile:DBProfile.DISK for Table:deletedBlocks
datanode_1  | 2019-11-14 05:58:35 INFO  BaseHttpServer:215 - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
datanode_2  | 2019-11-14 05:58:33 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:127 - using custom profile for table: validCerts
datanode_1  | 2019-11-14 05:58:35 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2  | 2019-11-14 05:58:33 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:183 - Using default column profile:DBProfile.DISK for Table:validCerts
datanode_1  | 2019-11-14 05:58:35 INFO  InitDatanodeState:140 - DatanodeDetails is persisted to /data/datanode.id
datanode_3  | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@252a8aae{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2019-11-14 05:58:34 WARN  BaseHttpServer:111 - /prof java profiling servlet is activated. Not safe for production!
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:127 - using custom profile for table: revokedCerts
datanode_1  | 2019-11-14 05:58:37 INFO  OzoneContainer:230 - Attempting to start container services.
datanode_3  | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.w.WebAppContext@4601203a{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-1497322455939206647.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_2  | 2019-11-14 05:58:34 INFO  HttpServer2:1191 - Jetty bound to port 9882
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:183 - Using default column profile:DBProfile.DISK for Table:revokedCerts
datanode_1  | 2019-11-14 05:58:37 INFO  OzoneContainer:194 - Background container scanner has been disabled.
datanode_3  | 2019-11-14 05:58:35 INFO  AbstractConnector:278 - Started ServerConnector@2d7a9786{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2019-11-14 05:58:34 INFO  Server:351 - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:127 - using custom profile for table: default
datanode_1  | 2019-11-14 05:58:37 INFO  XceiverServerRatis:421 - Starting XceiverServerRatis e6ea9929-07dd-4975-97c6-56877771eb24 at port 9858
datanode_2  | 2019-11-14 05:58:34 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@378cfecf{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2019-11-14 05:58:35 INFO  Server:419 - Started @3270ms
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:189 - Using default column profile:DBProfile.DISK for Table:default
datanode_1  | 2019-11-14 05:58:37 INFO  RaftServerProxy:299 - e6ea9929-07dd-4975-97c6-56877771eb24: start RPC server
datanode_2  | 2019-11-14 05:58:34 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@762637be{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2019-11-14 05:58:35 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1       | 2019-11-14 05:58:33 INFO  DBStoreBuilder:220 - Using default options. DBProfile.DISK
datanode_2  | 2019-11-14 05:58:34 INFO  ContextHandler:781 - Started o.e.j.w.WebAppContext@529c2a9a{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-13685259984755769826.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_1  | 2019-11-14 05:58:37 INFO  GrpcService:158 - e6ea9929-07dd-4975-97c6-56877771eb24: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3  | 2019-11-14 05:58:35 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1       | 2019-11-14 05:58:34 INFO  NodeSchemaLoader:125 - Loading file from java.lang.CompoundEnumeration@64e7619d
datanode_2  | 2019-11-14 05:58:34 INFO  AbstractConnector:278 - Started ServerConnector@6a5c20a8{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1  | WARNING: An illegal reflective access operation has occurred
datanode_3  | 2019-11-14 05:58:35 INFO  BaseHttpServer:215 - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
datanode_2  | 2019-11-14 05:58:34 INFO  Server:419 - Started @3776ms
datanode_1  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
datanode_3  | 2019-11-14 05:58:35 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1       | 2019-11-14 05:58:34 INFO  NodeSchemaLoader:171 - Loading network topology layer schema file
datanode_1  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
datanode_2  | 2019-11-14 05:58:34 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3  | 2019-11-14 05:58:35 INFO  InitDatanodeState:140 - DatanodeDetails is persisted to /data/datanode.id
scm_1       | 2019-11-14 05:58:34 INFO  SCMNodeManager:116 - Entering startup safe mode.
datanode_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_2  | 2019-11-14 05:58:34 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3  | 2019-11-14 05:58:37 INFO  OzoneContainer:230 - Attempting to start container services.
scm_1       | 2019-11-14 05:58:34 INFO  ContainerPlacementPolicyFactory:57 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
datanode_1  | WARNING: All illegal access operations will be denied in a future release
datanode_2  | 2019-11-14 05:58:34 INFO  BaseHttpServer:215 - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
datanode_3  | 2019-11-14 05:58:37 INFO  OzoneContainer:194 - Background container scanner has been disabled.
scm_1       | 2019-11-14 05:58:34 WARN  ServerUtils:145 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerProxy:89 - e6ea9929-07dd-4975-97c6-56877771eb24: addNew group-EC6822CEF7A3:[e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] returns group-EC6822CEF7A3:java.util.concurrent.CompletableFuture@75e370c4[Not completed]
datanode_2  | 2019-11-14 05:58:34 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3  | 2019-11-14 05:58:37 INFO  XceiverServerRatis:421 - Starting XceiverServerRatis 3d21e566-33d2-4f69-bdaa-44614fe8b8ef at port 9858
scm_1       | 2019-11-14 05:58:34 INFO  SCMPipelineManager:132 - No pipeline exists in current db
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerImpl:98 - e6ea9929-07dd-4975-97c6-56877771eb24: new RaftServerImpl for group-EC6822CEF7A3:[e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2019-11-14 05:58:34 INFO  InitDatanodeState:140 - DatanodeDetails is persisted to /data/datanode.id
datanode_3  | 2019-11-14 05:58:37 INFO  RaftServerProxy:299 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start RPC server
scm_1       | 2019-11-14 05:58:34 WARN  ServerUtils:145 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2019-11-14 05:58:36 INFO  OzoneContainer:230 - Attempting to start container services.
datanode_3  | 2019-11-14 05:58:37 INFO  GrpcService:158 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
scm_1       | 2019-11-14 05:58:34 WARN  EventQueue:183 - No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2019-11-14 05:58:36 INFO  OzoneContainer:194 - Background container scanner has been disabled.
datanode_3  | WARNING: An illegal reflective access operation has occurred
scm_1       | 2019-11-14 05:58:35 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
datanode_3  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
scm_1       | 2019-11-14 05:58:35 INFO  Server:1074 - Starting Socket Reader #1 for port 9861
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2019-11-14 05:58:36 INFO  XceiverServerRatis:421 - Starting XceiverServerRatis 978f0922-7db7-4a1b-93f5-6c85d90fed92 at port 9858
scm_1       | 2019-11-14 05:58:35 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2019-11-14 05:58:36 INFO  RaftServerProxy:299 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start RPC server
scm_1       | 2019-11-14 05:58:35 INFO  Server:1074 - Starting Socket Reader #1 for port 9863
datanode_3  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerImpl:103 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: ConfigurationManager, init=-1: [e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null, confs=<EMPTY_MAP>
scm_1       | 2019-11-14 05:58:35 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3  | WARNING: All illegal access operations will be denied in a future release
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2019-11-14 05:58:36 INFO  GrpcService:158 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
scm_1       | 2019-11-14 05:58:35 INFO  Server:1074 - Starting Socket Reader #1 for port 9860
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerProxy:89 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: addNew group-CD59DC0AA6F3:[3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858] returns group-CD59DC0AA6F3:java.util.concurrent.CompletableFuture@76210e37[Not completed]
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2019-11-14 05:58:35 INFO  DFSUtil:1641 - Starting Web-server for scm at: http://0.0.0.0:9876
datanode_2  | WARNING: An illegal reflective access operation has occurred
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:98 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: new RaftServerImpl for group-CD59DC0AA6F3:[3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2019-11-14 05:58:39 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/7c057615-cf72-4f79-b233-ec6822cef7a3 does not exist. Creating ...
scm_1       | 2019-11-14 05:58:35 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/7c057615-cf72-4f79-b233-ec6822cef7a3/in_use.lock acquired by nodename 16@19850cfb177b
datanode_2  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
scm_1       | 2019-11-14 05:58:35 INFO  HttpRequestLog:81 - Http request log for http.requests.scm is not defined
datanode_1  | 2019-11-14 05:58:39 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/7c057615-cf72-4f79-b233-ec6822cef7a3 has been successfully formatted.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
scm_1       | 2019-11-14 05:58:35 INFO  HttpServer2:975 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2019-11-14 05:58:39 INFO  ContainerStateMachine:223 - group-EC6822CEF7A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
datanode_2  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1       | 2019-11-14 05:58:35 INFO  HttpServer2:948 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | WARNING: All illegal access operations will be denied in a future release
scm_1       | 2019-11-14 05:58:35 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerProxy:89 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: addNew group-6C2E61C44D1D:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858] returns group-6C2E61C44D1D:java.util.concurrent.CompletableFuture@58739dec[Not completed]
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2019-11-14 05:58:35 INFO  HttpServer2:958 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerImpl:98 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: new RaftServerImpl for group-6C2E61C44D1D:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:103 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: ConfigurationManager, init=-1: [3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858], old=null, confs=<EMPTY_MAP>
scm_1       | 2019-11-14 05:58:35 WARN  BaseHttpServer:111 - /prof java profiling servlet is activated. Not safe for production!
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2019-11-14 05:58:35 INFO  StorageContainerManager:764 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
scm_1       | 2019-11-14 05:58:35 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1  | 2019-11-14 05:58:39 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/0f38953f-38d6-41a7-9f17-cd59dc0aa6f3 does not exist. Creating ...
scm_1       | 2019-11-14 05:58:35 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2019-11-14 05:58:39 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/0f38953f-38d6-41a7-9f17-cd59dc0aa6f3/in_use.lock acquired by nodename 16@bf1667f6f477
scm_1       | 2019-11-14 05:58:35 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerImpl:103 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: ConfigurationManager, init=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/0f38953f-38d6-41a7-9f17-cd59dc0aa6f3 has been successfully formatted.
scm_1       | 2019-11-14 05:58:35 INFO  SCMClientProtocolServer:159 - RPC server for Client  is listening at /0.0.0.0:9860
datanode_1  | 2019-11-14 05:58:39 INFO  SegmentedRaftLogWorker:173 - new e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/7c057615-cf72-4f79-b233-ec6822cef7a3
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  ContainerStateMachine:223 - group-CD59DC0AA6F3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1314 - IPC Server Responder: starting
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1153 - IPC Server listener on 9860: starting
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/3631be01-9523-4598-959d-6c2e61c44d1d does not exist. Creating ...
scm_1       | 2019-11-14 05:58:35 INFO  StorageContainerManager:774 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
datanode_2  | 2019-11-14 05:58:39 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/3631be01-9523-4598-959d-6c2e61c44d1d/in_use.lock acquired by nodename 16@4b9d8a37cd03
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/3631be01-9523-4598-959d-6c2e61c44d1d has been successfully formatted.
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
scm_1       | 2019-11-14 05:58:35 INFO  SCMBlockProtocolServer:146 - RPC server for Block Protocol is listening at /0.0.0.0:9863
datanode_2  | 2019-11-14 05:58:39 INFO  ContainerStateMachine:223 - group-6C2E61C44D1D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1314 - IPC Server Responder: starting
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1153 - IPC Server listener on 9863: starting
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  StorageContainerManager:778 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2019-11-14 05:58:40 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1       | 2019-11-14 05:58:35 INFO  SCMDatanodeProtocolServer:169 - RPC server for DataNodes is listening at /0.0.0.0:9861
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1314 - IPC Server Responder: starting
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  Server:1153 - IPC Server listener on 9861: starting
datanode_1  | 2019-11-14 05:58:39 INFO  SegmentedRaftLogWorker:128 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:173 - new 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/0f38953f-38d6-41a7-9f17-cd59dc0aa6f3
datanode_2  | 2019-11-14 05:58:39 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1       | 2019-11-14 05:58:35 INFO  HttpServer2:1191 - Jetty bound to port 9876
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2019-11-14 05:58:39 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-14 05:58:35 INFO  Server:351 - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
scm_1       | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@1e6dad8{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  SegmentedRaftLogWorker:173 - new 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3631be01-9523-4598-959d-6c2e61c44d1d
datanode_1  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 2019-11-14 05:58:35 INFO  ContextHandler:781 - Started o.e.j.s.ServletContextHandler@1be59f28{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
scm_1       | 2019-11-14 05:58:36 INFO  ContextHandler:781 - Started o.e.j.w.WebAppContext@14982a82{/,file:///tmp/jetty-0.0.0.0-9876-scm-_-any-7922394305674743381.dir/webapp/,AVAILABLE}{/scm}
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
scm_1       | 2019-11-14 05:58:36 INFO  AbstractConnector:278 - Started ServerConnector@78e22d35{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:184 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: start as a follower, conf=-1: [e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-14 05:58:36 INFO  Server:419 - Started @3416ms
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 2019-11-14 05:58:36 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start FollowerState
scm_1       | 2019-11-14 05:58:36 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2019-11-14 05:58:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EC6822CEF7A3,id=e6ea9929-07dd-4975-97c6-56877771eb24
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
scm_1       | 2019-11-14 05:58:36 INFO  BaseHttpServer:215 - HTTP server of SCM is listening at http://0.0.0.0:9876
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2019-11-14 05:58:36 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:128 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerProxy:89 - e6ea9929-07dd-4975-97c6-56877771eb24: addNew group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] returns group-D554529DCCF3:java.util.concurrent.CompletableFuture@5e5d8616[Not completed]
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:98 - e6ea9929-07dd-4975-97c6-56877771eb24: new RaftServerImpl for group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2019-11-14 05:58:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2019-11-14 05:58:38 INFO  SCMNodeManager:267 - Registered Data node : 978f0922-7db7-4a1b-93f5-6c85d90fed92{ip: 172.18.0.5, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  SegmentedRaftLogWorker:128 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       | 2019-11-14 05:58:38 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 1 required.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | 2019-11-14 05:58:38 INFO  SCMSafeModeManager:177 - ScmSafeModeManager, all rules are successfully validated
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       | 2019-11-14 05:58:38 INFO  SCMSafeModeManager:193 - SCM exiting safe mode.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
scm_1       | WARNING: An illegal reflective access operation has occurred
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:103 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: ConfigurationManager, init=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:184 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: start as a follower, conf=-1: [3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858], old=null
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1       | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2019-11-14 05:58:39 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
datanode_3  | 2019-11-14 05:58:40 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start FollowerState
datanode_2  | 2019-11-14 05:58:39 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 does not exist. Creating ...
scm_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_3  | 2019-11-14 05:58:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CD59DC0AA6F3,id=3d21e566-33d2-4f69-bdaa-44614fe8b8ef
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerImpl:184 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: start as a follower, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858], old=null
datanode_1  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/in_use.lock acquired by nodename 16@19850cfb177b
scm_1       | WARNING: All illegal access operations will be denied in a future release
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:39 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2019-11-14 05:58:40 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 has been successfully formatted.
scm_1       | 2019-11-14 05:58:39 INFO  NetworkTopology:111 - Added a new node: /default-rack/e6ea9929-07dd-4975-97c6-56877771eb24
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerProxy:89 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: addNew group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] returns group-D554529DCCF3:java.util.concurrent.CompletableFuture@42d1b6ee[Not completed]
datanode_2  | 2019-11-14 05:58:39 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start FollowerState
datanode_1  | 2019-11-14 05:58:40 INFO  ContainerStateMachine:223 - group-D554529DCCF3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm_1       | 2019-11-14 05:58:39 INFO  SCMNodeManager:267 - Registered Data node : e6ea9929-07dd-4975-97c6-56877771eb24{ip: 172.18.0.7, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2019-11-14 05:58:39 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6C2E61C44D1D,id=978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:98 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: new RaftServerImpl for group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
datanode_2  | 2019-11-14 05:58:39 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
scm_1       | 2019-11-14 05:58:39 INFO  PipelineStateManager:55 - Created pipeline Pipeline[ Id: 3631be01-9523-4598-959d-6c2e61c44d1d, Nodes: 978f0922-7db7-4a1b-93f5-6c85d90fed92{ip: 172.18.0.5, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerProxy:89 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: addNew group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] returns group-D554529DCCF3:java.util.concurrent.CompletableFuture@54c33f8[Not completed]
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2019-11-14 05:58:39 INFO  NetworkTopology:111 - Added a new node: /default-rack/3d21e566-33d2-4f69-bdaa-44614fe8b8ef
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerImpl:98 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: new RaftServerImpl for group-D554529DCCF3:[978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858] with ContainerStateMachine:uninitialized
scm_1       | 2019-11-14 05:58:39 INFO  SCMNodeManager:267 - Registered Data node : 3d21e566-33d2-4f69-bdaa-44614fe8b8ef{ip: 172.18.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
scm_1       | 2019-11-14 05:58:40 INFO  PipelineStateManager:55 - Created pipeline Pipeline[ Id: 7c057615-cf72-4f79-b233-ec6822cef7a3, Nodes: e6ea9929-07dd-4975-97c6-56877771eb24{ip: 172.18.0.7, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2019-11-14 05:58:40 INFO  PipelineStateManager:55 - Created pipeline Pipeline[ Id: 0f38953f-38d6-41a7-9f17-cd59dc0aa6f3, Nodes: 3d21e566-33d2-4f69-bdaa-44614fe8b8ef{ip: 172.18.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:103 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: ConfigurationManager, init=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 120s (custom)
scm_1       | 2019-11-14 05:58:40 INFO  PipelineStateManager:55 - Created pipeline Pipeline[ Id: 0eb5d23a-6d6f-4a46-9201-d554529dccf3, Nodes: 3d21e566-33d2-4f69-bdaa-44614fe8b8ef{ip: 172.18.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}978f0922-7db7-4a1b-93f5-6c85d90fed92{ip: 172.18.0.5, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}e6ea9929-07dd-4975-97c6-56877771eb24{ip: 172.18.0.7, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED]
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:173 - new e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
scm_1       | 2019-11-14 05:58:44 INFO  PipelineStateManager:134 - Pipeline Pipeline[ Id: 3631be01-9523-4598-959d-6c2e61c44d1d, Nodes: 978f0922-7db7-4a1b-93f5-6c85d90fed92{ip: 172.18.0.5, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 does not exist. Creating ...
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerImpl:103 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: ConfigurationManager, init=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null, confs=<EMPTY_MAP>
scm_1       | 2019-11-14 05:58:45 INFO  PipelineStateManager:134 - Pipeline Pipeline[ Id: 0f38953f-38d6-41a7-9f17-cd59dc0aa6f3, Nodes: 3d21e566-33d2-4f69-bdaa-44614fe8b8ef{ip: 172.18.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/in_use.lock acquired by nodename 16@bf1667f6f477
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1       | 2019-11-14 05:58:45 INFO  PipelineStateManager:134 - Pipeline Pipeline[ Id: 7c057615-cf72-4f79-b233-ec6822cef7a3, Nodes: e6ea9929-07dd-4975-97c6-56877771eb24{ip: 172.18.0.7, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state
datanode_3  | 2019-11-14 05:58:40 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 has been successfully formatted.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2019-11-14 05:58:51 INFO  PipelineStateManager:134 - Pipeline Pipeline[ Id: 0eb5d23a-6d6f-4a46-9201-d554529dccf3, Nodes: 3d21e566-33d2-4f69-bdaa-44614fe8b8ef{ip: 172.18.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}978f0922-7db7-4a1b-93f5-6c85d90fed92{ip: 172.18.0.5, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}e6ea9929-07dd-4975-97c6-56877771eb24{ip: 172.18.0.7, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] moved to OPEN state
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  ContainerStateMachine:223 - group-D554529DCCF3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:246 - The storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 does not exist. Creating ...
scm_1       | 2019-11-14 06:03:38 INFO  ReplicationManager:162 - Starting Replication Monitor Thread.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftStorageDirectory:328 - Lock on /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/in_use.lock acquired by nodename 16@4b9d8a37cd03
scm_1       | 2019-11-14 06:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 13 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3 has been successfully formatted.
scm_1       | 2019-11-14 06:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  ContainerStateMachine:223 - group-D554529DCCF3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 120s (custom)
scm_1       | 2019-11-14 06:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
scm_1       | 2019-11-14 06:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 2019-11-14 06:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:173 - new 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3
scm_1       | 2019-11-14 06:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:128 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
scm_1       | 2019-11-14 06:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
scm_1       | 2019-11-14 06:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 2019-11-14 06:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:173 - new 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-14 06:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
scm_1       | 2019-11-14 06:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
scm_1       | 2019-11-14 06:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-14 07:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:184 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: start as a follower, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
scm_1       | 2019-11-14 07:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2019-11-14 05:58:40 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 2019-11-14 07:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 33554432 (custom)
datanode_1  | 2019-11-14 05:58:40 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start FollowerState
scm_1       | 2019-11-14 07:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
scm_1       | 2019-11-14 07:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2019-11-14 05:58:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D554529DCCF3,id=e6ea9929-07dd-4975-97c6-56877771eb24
scm_1       | 2019-11-14 07:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:128 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-14 07:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2019-11-14 05:58:45 INFO  FollowerState:111 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-FollowerState: change to CANDIDATE, lastRpcTime:5181ms, electionTimeout:5180ms
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       | 2019-11-14 07:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:121 - e6ea9929-07dd-4975-97c6-56877771eb24: shutdown FollowerState
datanode_2  | 2019-11-14 05:58:40 INFO  SegmentedRaftLogWorker:128 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2019-11-14 07:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start LeaderElection
scm_1       | 2019-11-14 07:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 05:58:45 INFO  LeaderElection:178 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-LeaderElection1: begin an election at term 1 for -1: [e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
scm_1       | 2019-11-14 07:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:134 - e6ea9929-07dd-4975-97c6-56877771eb24: shutdown LeaderElection
scm_1       | 2019-11-14 07:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:184 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: start as a follower, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1       | 2019-11-14 08:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-14 05:58:40 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2019-11-14 05:58:45 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-EC6822CEF7A3 with new leaderId: e6ea9929-07dd-4975-97c6-56877771eb24
scm_1       | 2019-11-14 08:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-14 05:58:40 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start FollowerState
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:255 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: change Leader from null to e6ea9929-07dd-4975-97c6-56877771eb24 at term 1 for becomeLeader, leader elected after 5776ms
scm_1       | 2019-11-14 08:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerImpl:184 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: start as a follower, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_3  | 2019-11-14 05:58:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D554529DCCF3,id=3d21e566-33d2-4f69-bdaa-44614fe8b8ef
datanode_1  | 2019-11-14 05:58:45 INFO  FollowerState:111 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-FollowerState: change to CANDIDATE, lastRpcTime:5058ms, electionTimeout:5058ms
scm_1       | 2019-11-14 08:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:121 - e6ea9929-07dd-4975-97c6-56877771eb24: shutdown FollowerState
scm_1       | 2019-11-14 08:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start FollowerState
datanode_3  | 2019-11-14 05:58:45 INFO  FollowerState:111 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-FollowerState: change to CANDIDATE, lastRpcTime:5096ms, electionTimeout:5095ms
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2019-11-14 08:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D554529DCCF3,id=978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:121 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown FollowerState
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start LeaderElection
scm_1       | 2019-11-14 08:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:40 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
scm_1       | 2019-11-14 08:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  FollowerState:111 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-FollowerState: change to CANDIDATE, lastRpcTime:5151ms, electionTimeout:5150ms
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start LeaderElection
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
scm_1       | 2019-11-14 08:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RoleInfo:121 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown FollowerState
datanode_3  | 2019-11-14 05:58:45 INFO  LeaderElection:178 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-LeaderElection1: begin an election at term 1 for -1: [3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858], old=null
datanode_1  | 2019-11-14 05:58:45 INFO  LeaderElection:178 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-LeaderElection2: begin an election at term 1 for -1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
scm_1       | 2019-11-14 08:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:134 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown LeaderElection
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 4096 (default)
datanode_2  | 2019-11-14 05:58:44 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start LeaderElection
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
scm_1       | 2019-11-14 08:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  LeaderElection:178 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-LeaderElection1: begin an election at term 1 for -1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858], old=null
datanode_3  | 2019-11-14 05:58:45 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-CD59DC0AA6F3 with new leaderId: 3d21e566-33d2-4f69-bdaa-44614fe8b8ef
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 2019-11-14 08:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RoleInfo:134 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown LeaderElection
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerImpl:255 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: change Leader from null to 3d21e566-33d2-4f69-bdaa-44614fe8b8ef at term 1 for becomeLeader, leader elected after 5318ms
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
scm_1       | 2019-11-14 09:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start LeaderState
scm_1       | 2019-11-14 09:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-6C2E61C44D1D with new leaderId: 978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2019-11-14 05:58:45 INFO  SegmentedRaftLogWorker:385 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 2019-11-14 09:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerImpl:255 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: change Leader from null to 978f0922-7db7-4a1b-93f5-6c85d90fed92 at term 1 for becomeLeader, leader elected after 5276ms
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:356 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3: set configuration 0: [e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null at 0
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 4096 (default)
scm_1       | 2019-11-14 09:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2019-11-14 05:58:45 INFO  SegmentedRaftLogWorker:576 - e6ea9929-07dd-4975-97c6-56877771eb24@group-EC6822CEF7A3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7c057615-cf72-4f79-b233-ec6822cef7a3/current/log_inprogress_0
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2019-11-14 05:58:45 INFO  LeaderElection:56 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-LeaderElection2: Election REJECTED; received 2 response(s) [e6ea9929-07dd-4975-97c6-56877771eb24<-978f0922-7db7-4a1b-93f5-6c85d90fed92#0:FAIL-t1, e6ea9929-07dd-4975-97c6-56877771eb24<-3d21e566-33d2-4f69-bdaa-44614fe8b8ef#0:FAIL-t1] and 0 exception(s); e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3:t1, leader=null, voted=e6ea9929-07dd-4975-97c6-56877771eb24, raftlog=e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
scm_1       | 2019-11-14 09:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 4096 (default)
datanode_1  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
scm_1       | 2019-11-14 09:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:134 - e6ea9929-07dd-4975-97c6-56877771eb24: shutdown LeaderElection
scm_1       | 2019-11-14 09:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start LeaderState
datanode_1  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start FollowerState
scm_1       | 2019-11-14 09:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2019-11-14 05:58:45 INFO  SegmentedRaftLogWorker:385 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 2019-11-14 09:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start LeaderState
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerImpl:356 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3: set configuration 0: [3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858], old=null at 0
datanode_1  | 2019-11-14 05:58:51 INFO  RaftServerImpl:173 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:978f0922-7db7-4a1b-93f5-6c85d90fed92
scm_1       | 2019-11-14 09:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:44 INFO  SegmentedRaftLogWorker:385 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2019-11-14 05:58:45 INFO  FollowerState:111 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-FollowerState: change to CANDIDATE, lastRpcTime:5184ms, electionTimeout:5184ms
scm_1       | 2019-11-14 09:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:121 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown FollowerState
datanode_2  | 2019-11-14 05:58:44 INFO  RaftServerImpl:356 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D: set configuration 0: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858], old=null at 0
datanode_1  | 2019-11-14 05:58:51 INFO  RoleInfo:121 - e6ea9929-07dd-4975-97c6-56877771eb24: shutdown FollowerState
scm_1       | 2019-11-14 09:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2019-11-14 05:58:44 INFO  SegmentedRaftLogWorker:576 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-6C2E61C44D1D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3631be01-9523-4598-959d-6c2e61c44d1d/current/log_inprogress_0
datanode_1  | 2019-11-14 05:58:51 INFO  RoleInfo:143 - e6ea9929-07dd-4975-97c6-56877771eb24: start FollowerState
scm_1       | 2019-11-14 10:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start LeaderElection
datanode_2  | 2019-11-14 05:58:45 INFO  FollowerState:111 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-FollowerState: change to CANDIDATE, lastRpcTime:5102ms, electionTimeout:5102ms
datanode_1  | 2019-11-14 05:58:51 INFO  FollowerState:120 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
scm_1       | 2019-11-14 10:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  LeaderElection:178 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-LeaderElection2: begin an election at term 1 for -1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_2  | 2019-11-14 05:58:45 INFO  RoleInfo:121 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown FollowerState
datanode_1  | 2019-11-14 05:58:51 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-D554529DCCF3 with new leaderId: 978f0922-7db7-4a1b-93f5-6c85d90fed92
scm_1       | 2019-11-14 10:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:45 INFO  SegmentedRaftLogWorker:576 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-CD59DC0AA6F3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0f38953f-38d6-41a7-9f17-cd59dc0aa6f3/current/log_inprogress_0
datanode_2  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2019-11-14 10:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 05:58:51 INFO  RaftServerImpl:255 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: change Leader from null to 978f0922-7db7-4a1b-93f5-6c85d90fed92 at term 2 for appendEntries, leader elected after 10520ms
datanode_3  | 2019-11-14 05:58:46 INFO  LeaderElection:56 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-LeaderElection2: Election REJECTED; received 2 response(s) [3d21e566-33d2-4f69-bdaa-44614fe8b8ef<-978f0922-7db7-4a1b-93f5-6c85d90fed92#0:FAIL-t1, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef<-e6ea9929-07dd-4975-97c6-56877771eb24#0:FAIL-t1] and 0 exception(s); 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3:t1, leader=null, voted=3d21e566-33d2-4f69-bdaa-44614fe8b8ef, raftlog=3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_2  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start LeaderElection
scm_1       | 2019-11-14 10:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:46 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
datanode_2  | 2019-11-14 05:58:45 INFO  LeaderElection:178 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-LeaderElection2: begin an election at term 1 for -1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_1  | 2019-11-14 05:58:51 INFO  RaftServerImpl:356 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: set configuration 0: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null at 0
scm_1       | 2019-11-14 10:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:45 INFO  LeaderElection:56 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-LeaderElection2: Election REJECTED; received 2 response(s) [978f0922-7db7-4a1b-93f5-6c85d90fed92<-3d21e566-33d2-4f69-bdaa-44614fe8b8ef#0:FAIL-t1, 978f0922-7db7-4a1b-93f5-6c85d90fed92<-e6ea9929-07dd-4975-97c6-56877771eb24#0:FAIL-t1] and 0 exception(s); 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3:t1, leader=null, voted=978f0922-7db7-4a1b-93f5-6c85d90fed92, raftlog=978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
datanode_3  | 2019-11-14 05:58:46 INFO  RoleInfo:134 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown LeaderElection
datanode_1  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:385 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 2019-11-14 10:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:45 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
datanode_3  | 2019-11-14 05:58:46 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start FollowerState
datanode_1  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:576 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/current/log_inprogress_0
scm_1       | 2019-11-14 10:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:45 INFO  RoleInfo:134 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown LeaderElection
datanode_1  | 2019-11-14 06:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 10:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:45 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start FollowerState
datanode_1  | 2019-11-14 06:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 10:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:51 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_2  | 2019-11-14 05:58:51 INFO  FollowerState:111 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-FollowerState: change to CANDIDATE, lastRpcTime:5068ms, electionTimeout:5068ms
datanode_1  | 2019-11-14 06:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 05:58:51 INFO  RoleInfo:121 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown FollowerState
scm_1       | 2019-11-14 10:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 06:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 05:58:51 INFO  FollowerState:120 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2019-11-14 05:58:51 INFO  RoleInfo:121 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown FollowerState
scm_1       | 2019-11-14 10:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 06:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 05:58:51 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start FollowerState
scm_1       | 2019-11-14 11:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 06:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 05:58:51 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-D554529DCCF3 with new leaderId: 978f0922-7db7-4a1b-93f5-6c85d90fed92
scm_1       | 2019-11-14 11:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:51 INFO  RaftServerImpl:255 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: change Leader from null to 978f0922-7db7-4a1b-93f5-6c85d90fed92 at term 2 for appendEntries, leader elected after 10519ms
datanode_1  | 2019-11-14 07:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1       | 2019-11-14 11:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:51 INFO  RaftServerImpl:356 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: set configuration 0: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null at 0
datanode_1  | 2019-11-14 07:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start LeaderElection
scm_1       | 2019-11-14 11:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:385 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2019-11-14 07:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  LeaderElection:178 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-LeaderElection3: begin an election at term 2 for -1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
scm_1       | 2019-11-14 11:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 07:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:576 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/current/log_inprogress_0
datanode_2  | 2019-11-14 05:58:51 INFO  LeaderElection:56 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-LeaderElection3: Election PASSED; received 1 response(s) [978f0922-7db7-4a1b-93f5-6c85d90fed92<-3d21e566-33d2-4f69-bdaa-44614fe8b8ef#0:OK-t2] and 0 exception(s); 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3:t2, leader=null, voted=978f0922-7db7-4a1b-93f5-6c85d90fed92, raftlog=978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null
scm_1       | 2019-11-14 11:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 07:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 06:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 11:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RoleInfo:134 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: shutdown LeaderElection
datanode_1  | 2019-11-14 07:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 06:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 11:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerImpl:173 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_1  | 2019-11-14 07:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 06:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 11:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  XceiverServerRatis:697 - Leader change notification received for group: group-D554529DCCF3 with new leaderId: 978f0922-7db7-4a1b-93f5-6c85d90fed92
datanode_1  | 2019-11-14 07:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 11:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerImpl:255 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: change Leader from null to 978f0922-7db7-4a1b-93f5-6c85d90fed92 at term 2 for becomeLeader, leader elected after 10449ms
datanode_1  | 2019-11-14 08:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 11:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2019-11-14 08:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 11:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 06:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2019-11-14 08:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 12:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 06:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 4096 (default)
datanode_1  | 2019-11-14 08:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 12:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 06:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
datanode_1  | 2019-11-14 08:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 12:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2019-11-14 08:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2019-11-14 08:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm_1       | 2019-11-14 12:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 08:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2019-11-14 12:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2019-11-14 09:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 12:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2019-11-14 09:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 07:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 12:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
datanode_1  | 2019-11-14 09:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 12:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2019-11-14 09:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 12:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 WARN  MetricRegistriesImpl:61 - First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-14 09:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 12:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 09:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm_1       | 2019-11-14 12:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 07:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 09:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2019-11-14 12:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 09:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 08:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
scm_1       | 2019-11-14 13:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 10:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 08:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2019-11-14 10:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
datanode_1  | 2019-11-14 10:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 13:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2019-11-14 10:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RoleInfo:143 - 978f0922-7db7-4a1b-93f5-6c85d90fed92: start LeaderState
datanode_1  | 2019-11-14 10:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 13:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:385 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2019-11-14 10:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  RaftServerImpl:356 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3: set configuration 0: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null at 0
datanode_1  | 2019-11-14 10:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 13:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 08:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 05:58:51 INFO  SegmentedRaftLogWorker:576 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0eb5d23a-6d6f-4a46-9201-d554529dccf3/current/log_inprogress_0
datanode_1  | 2019-11-14 10:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 06:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 11:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 09:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 13:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 06:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 11:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 09:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 06:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 09:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 11:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 13:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 06:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 11:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 13:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 06:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 11:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 09:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 11:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 09:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 06:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 11:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 09:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 07:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 11:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 09:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 07:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 12:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 09:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 07:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 12:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 10:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 12:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 07:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 10:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 07:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 12:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 10:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 07:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 12:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 10:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 12:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 07:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 10:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 12:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 07:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 10:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 12:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 10:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 13:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 14:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 08:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 13:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 14:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 08:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 10:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 13:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 08:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 11:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 13:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 08:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 11:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 13:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 08:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 11:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 08:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 13:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 11:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 08:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 13:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 11:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 08:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 13:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 11:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 09:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 14:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 11:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 09:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 14:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 09:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 11:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 14:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 09:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 12:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 14:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 15:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 09:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 12:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 14:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 15:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 09:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 12:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 14:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 09:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 12:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 09:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 12:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 12:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm_1       | 2019-11-14 16:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 10:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 14:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 12:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 16:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 10:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 12:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 14:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 10:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 13:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 15:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 16:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 10:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 13:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 15:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 10:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 15:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 16:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 13:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 10:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 15:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 13:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 10:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 15:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 16:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 13:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 10:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 15:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 11:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 15:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 13:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 16:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 11:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 15:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 13:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 16:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 11:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 16:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 11:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 16:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 13:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 11:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 16:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 14:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 11:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 16:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 14:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 11:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 16:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 14:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 11:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 16:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 14:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 12:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 16:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 12:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 16:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 14:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 12:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 17:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 14:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 12:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 17:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 14:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 12:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 17:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 12:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 14:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 17:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 12:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 15:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 17:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 17:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 12:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 15:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 17:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 13:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 15:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 13:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 17:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:00:33 WARN  JvmPauseMonitor:201 - Detected pause in JVM or host machine (eg GC): pause of approximately 13188ms
datanode_2  | 2019-11-14 13:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 15:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 17:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 13:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | No GCs detected
datanode_3  | 2019-11-14 15:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 17:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 15:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 18:00:33 WARN  JavaUtils:246 - Unexpected long sleep: sleep(5014ms) actually took 14319ms which is over the threshold 300ms
scm_1       | 2019-11-14 18:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 15:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 18:00:33 WARN  JvmPauseMonitor:201 - Detected pause in JVM or host machine (eg GC): pause of approximately 10275ms
datanode_2  | 2019-11-14 13:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | No GCs detected
datanode_3  | 2019-11-14 15:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 13:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 18:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 16:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 13:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 18:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 16:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 13:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 18:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 16:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 14:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 18:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 16:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 14:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 18:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 16:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 14:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 18:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 16:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 18:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 18:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 16:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 18:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 14:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 18:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 16:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 14:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 19:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 17:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 14:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 19:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 17:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 14:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 19:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 17:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 14:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 19:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 17:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 15:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 19:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 17:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 15:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 17:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 19:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 15:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 17:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 19:48:17 INFO  JvmPauseMonitor:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1244ms
datanode_2  | 2019-11-14 15:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 17:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | No GCs detected
datanode_2  | 2019-11-14 15:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 18:00:33 INFO  FollowerState:111 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3-FollowerState: change to CANDIDATE, lastRpcTime:14536ms, electionTimeout:5103ms
datanode_1  | 2019-11-14 19:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 19:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 15:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RoleInfo:121 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown FollowerState
datanode_1  | 2019-11-14 19:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 19:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 15:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
scm_1       | 2019-11-14 20:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 15:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start LeaderElection
scm_1       | 2019-11-14 20:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 16:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  JvmPauseMonitor:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 4043ms
scm_1       | 2019-11-14 20:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 16:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | No GCs detected
scm_1       | 2019-11-14 20:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 16:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RaftServerImpl:173 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef@group-D554529DCCF3: changes role from CANDIDATE to FOLLOWER at term 2 for appendEntries
scm_1       | 2019-11-14 20:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 16:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RoleInfo:134 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: shutdown LeaderElection
scm_1       | 2019-11-14 20:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 16:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:00:33 INFO  RoleInfo:143 - 3d21e566-33d2-4f69-bdaa-44614fe8b8ef: start FollowerState
scm_1       | 2019-11-14 20:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 16:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 20:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 20:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 16:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 20:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 16:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 20:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 17:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 20:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 18:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 17:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 18:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 20:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 17:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 21:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 17:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 21:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 18:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 17:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 21:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 19:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 21:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 17:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 19:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 17:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 19:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 21:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:45:50 INFO  JvmPauseMonitor:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 4707ms
datanode_2  | 2019-11-14 17:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 19:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | No GCs detected
datanode_2  | 2019-11-14 18:00:33 WARN  JvmPauseMonitor:201 - Detected pause in JVM or host machine (eg GC): pause of approximately 11925ms
scm_1       | 2019-11-14 21:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 19:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | No GCs detected
datanode_3  | 2019-11-14 19:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:45:50 WARN  JavaUtils:246 - Unexpected long sleep: sleep(5082ms) actually took 5580ms which is over the threshold 300ms
datanode_2  | 2019-11-14 18:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 21:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 19:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 18:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:45:50 WARN  GrpcServerProtocolService:134 - e6ea9929-07dd-4975-97c6-56877771eb24: installSnapshot onError, lastRequest: 978f0922-7db7-4a1b-93f5-6c85d90fed92->e6ea9929-07dd-4975-97c6-56877771eb24#22712-t2, previous=(t:2, i:0), leaderCommit=0, initializing? false, entries: <empty>: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: cancelled before receiving half close
datanode_2  | 2019-11-14 18:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 19:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 21:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 18:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 20:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | Nov 14, 2019 9:45:50 PM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run
scm_1       | 2019-11-14 21:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 20:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 18:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | SEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed@3a188fc9
scm_1       | 2019-11-14 22:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 20:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 18:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: call already cancelled
scm_1       | 2019-11-14 22:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 20:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 18:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:517)
datanode_3  | 2019-11-14 20:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 18:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onCompleted(ServerCalls.java:356)
datanode_3  | 2019-11-14 20:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 22:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 19:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onError(GrpcServerProtocolService.java:121)
datanode_3  | 2019-11-14 20:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 22:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onCancel(ServerCalls.java:269)
datanode_2  | 2019-11-14 19:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 20:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 22:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.closed(ServerCallImpl.java:293)
datanode_2  | 2019-11-14 19:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 21:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed.runInContext(ServerImpl.java:741)
scm_1       | 2019-11-14 22:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 19:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 21:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm_1       | 2019-11-14 22:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 19:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 21:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
scm_1       | 2019-11-14 22:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 19:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 22:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm_1       | 2019-11-14 22:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 2019-11-14 22:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 2019-11-14 22:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 21:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2019-11-14 19:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 21:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 23:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 
datanode_2  | 2019-11-14 19:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 21:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:45:51 INFO  RaftServerImpl:356 - e6ea9929-07dd-4975-97c6-56877771eb24@group-D554529DCCF3: set configuration 0: [978f0922-7db7-4a1b-93f5-6c85d90fed92:172.18.0.5:9858, 3d21e566-33d2-4f69-bdaa-44614fe8b8ef:172.18.0.8:9858, e6ea9929-07dd-4975-97c6-56877771eb24:172.18.0.7:9858], old=null at 0
datanode_2  | 2019-11-14 20:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 23:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 21:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 20:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 21:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 21:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 20:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 23:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-14 22:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 22:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 20:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 22:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 22:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 20:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 23:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 22:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 22:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 20:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-14 22:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 22:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 20:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 22:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 22:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-14 23:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 20:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 22:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 22:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-14 23:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 22:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 22:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 22:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 22:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 23:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 23:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 23:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 23:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 23:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 23:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 23:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 23:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:45:49 WARN  GrpcLogAppender:211 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3->e6ea9929-07dd-4975-97c6-56877771eb24-GrpcLogAppender: appendEntries Timeout, request=AppendEntriesRequest:cid=22711,entriesCount=0,lastEntry=null
datanode_1  | 2019-11-14 23:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 23:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:45:50 WARN  GrpcLogAppender:134 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3->e6ea9929-07dd-4975-97c6-56877771eb24-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: Failed to read message.
datanode_1  | 2019-11-14 23:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-14 23:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:45:50 INFO  FollowerInfo:51 - 978f0922-7db7-4a1b-93f5-6c85d90fed92@group-D554529DCCF3->e6ea9929-07dd-4975-97c6-56877771eb24: nextIndex: updateUnconditionally 1 -> 0
datanode_3  | 2019-11-14 23:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-14 23:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-14 23:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 21:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-14 23:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 00:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 00:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 00:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 00:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 00:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 00:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 00:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 00:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 00:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 00:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 22:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 00:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 00:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 22:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 00:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 00:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 00:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 00:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 00:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 00:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 22:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 01:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 01:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 23:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 01:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 01:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-14 23:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 01:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 01:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 23:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 01:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 01:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 01:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 23:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 01:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 01:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 01:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 23:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 01:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 01:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 01:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 23:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 01:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 01:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-14 23:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 01:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 02:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-14 23:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 02:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 02:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 00:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 02:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 02:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 00:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 02:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 02:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 02:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 00:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 02:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 02:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 00:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 02:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 02:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 00:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 02:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 02:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 00:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 02:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 02:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 02:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 00:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 02:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 02:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 00:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 03:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 03:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 02:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 01:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 03:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 03:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 02:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 01:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 03:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 03:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 01:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 02:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 03:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 03:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 01:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 03:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 03:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 01:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 03:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 03:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 01:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 03:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 03:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 03:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 01:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 03:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 2 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 03:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 01:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 03:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 03:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 04:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 02:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 04:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 04:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 02:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 04:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 04:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 04:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 04:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 03:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 04:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 04:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 04:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 04:42:38 INFO  JvmPauseMonitor:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1100ms
scm_1       | 2019-11-15 03:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 04:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | No GCs detected
scm_1       | 2019-11-15 03:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 04:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 04:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 03:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_3  | 2019-11-15 05:13:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 04:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 02:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 05:13:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 04:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 05:28:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 04:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 05:28:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 05:13:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 05:43:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 05:13:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 05:43:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 05:28:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2019-11-15 05:58:34 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 05:28:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2019-11-15 05:58:34 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 05:43:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2019-11-15 05:43:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 04:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 03:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2019-11-15 05:58:33 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_1  | 2019-11-15 05:58:33 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 04:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 04:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 05:03:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 05:08:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 05:13:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 05:18:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 05:23:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
scm_1       | 2019-11-15 05:28:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 04:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 04:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 05:33:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 05:13:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       | 2019-11-15 05:38:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm_1       | 2019-11-15 05:43:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm_1       | 2019-11-15 05:48:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
scm_1       | 2019-11-15 05:53:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 0 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 05:13:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2019-11-15 05:58:38 INFO  ReplicationManager:225 - Replication Monitor Thread took 1 milliseconds for processing 3 containers.
datanode_2  | 2019-11-15 05:28:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 05:28:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 05:43:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 05:43:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 05:58:32 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2019-11-15 05:58:32 INFO  HddsVolumeChecker:204 - Scheduled health check for volume /data/hdds/hdds
