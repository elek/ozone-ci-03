Attaching to hadoop27_datanode_3, hadoop27_datanode_1, hadoop27_rm_1, hadoop27_datanode_2, hadoop27_scm_1, hadoop27_s3g_1, hadoop27_om_1, hadoop27_nm_1
datanode_3  | 2019-11-15 10:28:20,931 INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | /************************************************************
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3  | STARTUP_MSG:   host = a9d6018f5bef/172.18.0.9
datanode_3  | STARTUP_MSG:   args = []
datanode_3  | STARTUP_MSG:   version = 3.2.0
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | STARTUP_MSG:   java = 11.0.3
datanode_3  | ************************************************************/
datanode_3  | 2019-11-15 10:28:20,945 INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2019-11-15 10:28:21,314 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2019-11-15 10:28:19,682 INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | 2019-11-15 10:28:21,491 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | /************************************************************
datanode_3  | 2019-11-15 10:28:21,492 INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3  | 2019-11-15 10:28:21,694 INFO ozone.HddsDatanodeService: HddsDatanodeService host:a9d6018f5bef ip:172.18.0.9
datanode_1  | STARTUP_MSG:   host = 65f0630e46c9/172.18.0.7
datanode_3  | 2019-11-15 10:28:21,758 INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 3.2.0
datanode_3  | 2019-11-15 10:28:21,760 INFO volume.VolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2019-11-15 10:28:19,310 INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
s3g_1       | 2019-11-15 10:28:19,799 INFO hdfs.DFSUtil: Starting Web-server for s3gateway at: http://0.0.0.0:9878
datanode_3  | 2019-11-15 10:28:21,769 INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2  | /************************************************************
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
rm_1        | Flokkr launcher script 1.1-7-gef45e4c
s3g_1       | 2019-11-15 10:28:19,836 INFO util.log: Logging initialized @1029ms
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2019-11-15 10:28:21,789 INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
s3g_1       | 2019-11-15 10:28:19,982 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | STARTUP_MSG:   host = bfd1a25ed5e9/172.18.0.6
rm_1        | 
datanode_1  | ************************************************************/
datanode_3  | 2019-11-15 10:28:22,814 WARN scm.HddsServerUtil: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
s3g_1       | 2019-11-15 10:28:20,026 INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
datanode_2  | STARTUP_MSG:   args = []
rm_1        | ===== Plugin is activated ENVTOCONF =====
scm_1       | 2019-11-15 10:28:18,921 INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
nm_1        | Flokkr launcher script 1.1-7-gef45e4c
datanode_1  | 2019-11-15 10:28:19,700 INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2019-11-15 10:28:22,849 INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_2  | STARTUP_MSG:   version = 3.2.0
s3g_1       | 2019-11-15 10:28:20,037 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
scm_1       | /************************************************************
rm_1        | capacity-scheduler.xml
datanode_3  | 2019-11-15 10:28:23,078 INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9858 (custom)
nm_1        | 
s3g_1       | 2019-11-15 10:28:20,040 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
om_1        | Waiting for the service scm:9876
datanode_1  | 2019-11-15 10:28:20,022 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1       | STARTUP_MSG: Starting StorageContainerManager
rm_1        | File capacity-scheduler.xml has been written out successfullly.
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.5.0-SNAPSHOT.jar
datanode_3  | 2019-11-15 10:28:23,079 INFO server.GrpcService: raft.grpc.message.size.max = 33570816 (custom)
nm_1        | ===== Plugin is activated ENVTOCONF =====
s3g_1       | 2019-11-15 10:28:20,040 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1        | 2019-11-15 10:28:24,686 INFO om.OzoneManagerStarter: STARTUP_MSG: 
datanode_1  | 2019-11-15 10:28:20,278 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1       | STARTUP_MSG:   host = scm/172.18.0.3
rm_1        | mapred-site.xml
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | 2019-11-15 10:28:23,081 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
nm_1        | mapred-site.xml
s3g_1       | 2019-11-15 10:28:20,040 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1        | /************************************************************
datanode_1  | 2019-11-15 10:28:20,279 INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
scm_1       | STARTUP_MSG:   args = [--init]
rm_1        | File mapred-site.xml has been written out successfullly.
datanode_3  | 2019-11-15 10:28:23,082 INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_2  | STARTUP_MSG:   java = 11.0.3
nm_1        | File mapred-site.xml has been written out successfullly.
s3g_1       | 2019-11-15 10:28:20,060 INFO s3.Gateway: Starting Ozone S3 gateway
om_1        | STARTUP_MSG: Starting OzoneManager
datanode_1  | 2019-11-15 10:28:20,561 INFO ozone.HddsDatanodeService: HddsDatanodeService host:65f0630e46c9 ip:172.18.0.7
scm_1       | STARTUP_MSG:   version = 3.2.0
rm_1        | hdfs-site.xml
datanode_3  | 2019-11-15 10:28:23,083 INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_2  | ************************************************************/
nm_1        | yarn-site.xml
om_1        | STARTUP_MSG:   host = om/172.18.0.4
datanode_1  | 2019-11-15 10:28:20,624 INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
s3g_1       | 2019-11-15 10:28:20,071 INFO http.HttpServer2: Jetty bound to port 9878
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hamcrest-all-1.3.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar
rm_1        | File hdfs-site.xml has been written out successfullly.
datanode_3  | 2019-11-15 10:28:23,234 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2019-11-15 10:28:19,319 INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
om_1        | STARTUP_MSG:   args = [--init]
nm_1        | File yarn-site.xml has been written out successfullly.
datanode_1  | 2019-11-15 10:28:20,625 INFO volume.VolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
s3g_1       | 2019-11-15 10:28:20,073 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
rm_1        | ozone-site.xml
datanode_3  | 2019-11-15 10:28:23,386 INFO hdfs.DFSUtil: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om_1        | STARTUP_MSG:   version = 3.2.0
datanode_2  | 2019-11-15 10:28:19,602 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
nm_1        | hdfs-site.xml
datanode_1  | 2019-11-15 10:28:20,635 INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
scm_1       | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2019-11-15 10:28:23,410 INFO util.log: Logging initialized @3443ms
rm_1        | File ozone-site.xml has been written out successfullly.
s3g_1       | 2019-11-15 10:28:20,115 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1b919693{/logs,file:///var/log/hadoop/,AVAILABLE}
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar
nm_1        | File hdfs-site.xml has been written out successfullly.
datanode_2  | 2019-11-15 10:28:19,819 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1       | 2019-11-15 10:28:20,116 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6933b6c6{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2019-11-15 10:28:23,513 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | ************************************************************/
rm_1        | core-site.xml
om_1        | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
nm_1        | core-site.xml
datanode_1  | 2019-11-15 10:28:20,661 INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2019-11-15 10:28:19,819 INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
s3g_1       | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
datanode_3  | 2019-11-15 10:28:23,517 INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
scm_1       | 2019-11-15 10:28:18,927 INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
rm_1        | File core-site.xml has been written out successfullly.
om_1        | STARTUP_MSG:   java = 11.0.3
nm_1        | File core-site.xml has been written out successfullly.
datanode_1  | 2019-11-15 10:28:21,852 WARN scm.HddsServerUtil: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2019-11-15 10:28:20,005 INFO ozone.HddsDatanodeService: HddsDatanodeService host:bfd1a25ed5e9 ip:172.18.0.6
s3g_1       | WARNING: An illegal reflective access operation has occurred
datanode_3  | 2019-11-15 10:28:23,528 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
scm_1       | 2019-11-15 10:28:18,977 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | ************************************************************/
rm_1        | yarn-site.xml
nm_1        | ozone-site.xml
datanode_1  | 2019-11-15 10:28:21,885 INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_2  | 2019-11-15 10:28:20,069 INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of  storage type : DISK and capacity : 10908285698048
datanode_3  | 2019-11-15 10:28:23,530 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
s3g_1       | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
scm_1       | SCM initialization succeeded.Current cluster id for sd=/data/metadata/scm;cid=CID-3377b9a7-6d61-4925-acd4-001a519da4fe
rm_1        | File yarn-site.xml has been written out successfullly.
nm_1        | File ozone-site.xml has been written out successfullly.
datanode_1  | 2019-11-15 10:28:22,187 INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9858 (custom)
om_1        | 2019-11-15 10:28:24,696 INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2019-11-15 10:28:20,072 INFO volume.VolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3  | 2019-11-15 10:28:23,531 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1       | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
scm_1       | 2019-11-15 10:28:19,040 INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
rm_1        | ======================================
nm_1        | capacity-scheduler.xml
datanode_1  | 2019-11-15 10:28:22,188 INFO server.GrpcService: raft.grpc.message.size.max = 33570816 (custom)
om_1        | 2019-11-15 10:28:25,476 INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.18.0.4:9862
datanode_3  | 2019-11-15 10:28:23,531 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_2  | 2019-11-15 10:28:20,085 INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
scm_1       | /************************************************************
rm_1        | *** Launching "yarn resourcemanager"
nm_1        | File capacity-scheduler.xml has been written out successfullly.
om_1        | 2019-11-15 10:28:25,476 INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
datanode_1  | 2019-11-15 10:28:22,190 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2019-11-15 10:28:23,562 INFO http.HttpServer2: Jetty bound to port 9882
s3g_1       | WARNING: All illegal access operations will be denied in a future release
datanode_2  | 2019-11-15 10:28:20,110 INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.18.0.3
rm_1        | 2019-11-15 10:28:19 INFO  ResourceManager:45 - STARTUP_MSG: 
nm_1        | ======================================
om_1        | 2019-11-15 10:28:25,480 WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 2019-11-15 10:28:22,190 INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_3  | 2019-11-15 10:28:23,564 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
s3g_1       | Nov 15, 2019 10:28:23 AM org.glassfish.jersey.internal.Errors logErrors
datanode_2  | 2019-11-15 10:28:21,320 WARN scm.HddsServerUtil: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1       | ************************************************************/
rm_1        | /************************************************************
nm_1        | *** Launching "yarn nodemanager"
om_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-3377b9a7-6d61-4925-acd4-001a519da4fe
datanode_1  | 2019-11-15 10:28:22,191 INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_3  | 2019-11-15 10:28:23,592 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@715b886f{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2019-11-15 10:28:21,349 INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
s3g_1       | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
scm_1       | 2019-11-15 10:28:20,285 INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
rm_1        | STARTUP_MSG: Starting ResourceManager
nm_1        | 2019-11-15 10:28:17 INFO  NodeManager:45 - STARTUP_MSG: 
om_1        | 2019-11-15 10:28:25,863 INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
datanode_1  | 2019-11-15 10:28:22,354 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2019-11-15 10:28:23,593 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1859e2a4{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2019-11-15 10:28:21,507 INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9858 (custom)
s3g_1       | 
scm_1       | /************************************************************
nm_1        | /************************************************************
rm_1        | STARTUP_MSG:   host = rm/172.18.0.8
om_1        | /************************************************************
datanode_1  | 2019-11-15 10:28:22,531 INFO hdfs.DFSUtil: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3  | 2019-11-15 10:28:23,687 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c723f2d{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-4445882219251308687.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_2  | 2019-11-15 10:28:21,508 INFO server.GrpcService: raft.grpc.message.size.max = 33570816 (custom)
s3g_1       | 2019-11-15 10:28:23,322 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5f0a2638{/,file:///tmp/jetty-0.0.0.0-9878-s3gateway-_-any-15125132291046449107.dir/webapp/,AVAILABLE}{/s3gateway}
scm_1       | STARTUP_MSG: Starting StorageContainerManager
nm_1        | STARTUP_MSG: Starting NodeManager
rm_1        | STARTUP_MSG:   args = []
om_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om/172.18.0.4
datanode_1  | 2019-11-15 10:28:22,559 INFO util.log: Logging initialized @3642ms
datanode_3  | 2019-11-15 10:28:23,693 INFO server.AbstractConnector: Started ServerConnector@53b09ba7{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2019-11-15 10:28:21,510 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1       | 2019-11-15 10:28:23,329 INFO server.AbstractConnector: Started ServerConnector@7c90b7b7{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
scm_1       | STARTUP_MSG:   host = scm/172.18.0.3
nm_1        | STARTUP_MSG:   host = nm/172.18.0.2
rm_1        | STARTUP_MSG:   version = 2.7.7
om_1        | ************************************************************/
datanode_1  | 2019-11-15 10:28:22,646 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2019-11-15 10:28:23,694 INFO server.Server: Started @3727ms
datanode_2  | 2019-11-15 10:28:21,510 INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
s3g_1       | 2019-11-15 10:28:23,329 INFO server.Server: Started @4525ms
scm_1       | STARTUP_MSG:   args = []
nm_1        | STARTUP_MSG:   args = []
om_1        | 2019-11-15 10:28:27,060 INFO om.OzoneManagerStarter: STARTUP_MSG: 
datanode_1  | 2019-11-15 10:28:22,650 INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3  | 2019-11-15 10:28:23,698 INFO impl.MetricsSinkAdapter: Sink prometheus started
rm_1        | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.7.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/opt/ozone/share/ozone/lib/hadoop-ozone-filesystem-lib-legacy-0.5.0-SNAPSHOT.jar:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/etc/hadoop/rm-config/log4j.properties
datanode_2  | 2019-11-15 10:28:21,511 INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
s3g_1       | 2019-11-15 10:28:23,332 INFO server.BaseHttpServer: HTTP server of S3GATEWAY is listening at http://0.0.0.0:9878
scm_1       | STARTUP_MSG:   version = 3.2.0
nm_1        | STARTUP_MSG:   version = 2.7.7
om_1        | /************************************************************
datanode_1  | 2019-11-15 10:28:22,660 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_3  | 2019-11-15 10:28:23,698 INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2  | 2019-11-15 10:28:21,665 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
rm_1        | STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hamcrest-all-1.3.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar
datanode_1  | 2019-11-15 10:28:22,662 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2019-11-15 10:28:21,836 INFO hdfs.DFSUtil: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om_1        | STARTUP_MSG: Starting OzoneManager
datanode_3  | 2019-11-15 10:28:23,700 INFO server.BaseHttpServer: HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
rm_1        | STARTUP_MSG:   java = 1.8.0_181
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1  | 2019-11-15 10:28:22,662 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2019-11-15 10:28:21,859 INFO util.log: Logging initialized @3272ms
om_1        | STARTUP_MSG:   host = om/172.18.0.4
nm_1        | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.7.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/opt/ozone/share/ozone/lib/hadoop-ozone-filesystem-lib-legacy-0.5.0-SNAPSHOT.jar:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/etc/hadoop/nm-config/log4j.properties
datanode_3  | 2019-11-15 10:28:23,716 INFO util.JvmPauseMonitor: Starting JVM pause monitor
rm_1        | ************************************************************/
scm_1       | STARTUP_MSG:   java = 11.0.3
datanode_1  | 2019-11-15 10:28:22,662 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2019-11-15 10:28:21,980 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1        | STARTUP_MSG:   args = []
datanode_3  | 2019-11-15 10:28:23,868 INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/metadata/datanode.id
nm_1        | STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
rm_1        | 2019-11-15 10:28:19 INFO  ResourceManager:45 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2019-11-15 10:28:22,696 INFO http.HttpServer2: Jetty bound to port 9882
datanode_2  | 2019-11-15 10:28:21,985 INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
om_1        | STARTUP_MSG:   version = 3.2.0
scm_1       | ************************************************************/
datanode_3  | 2019-11-15 10:28:25,859 INFO ozoneimpl.OzoneContainer: Attempting to start container services.
nm_1        | STARTUP_MSG:   java = 1.8.0_181
rm_1        | 2019-11-15 10:28:20 INFO  Configuration:2427 - found resource core-site.xml at file:/opt/hadoop/etc/hadoop/core-site.xml
datanode_1  | 2019-11-15 10:28:22,699 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_2  | 2019-11-15 10:28:21,997 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/guava-11.0.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.9.9.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.9.9.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.5.0-d6d58d0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.3.25.v20180904.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar
datanode_3  | 2019-11-15 10:28:25,861 INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
nm_1        | ************************************************************/
rm_1        | 2019-11-15 10:28:20 INFO  Groups:387 - clearing userToGroupsMap cache
datanode_1  | 2019-11-15 10:28:22,728 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73844119{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2019-11-15 10:28:22,000 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
scm_1       | 2019-11-15 10:28:20,296 INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2019-11-15 10:28:25,862 INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 875be6b7-c5fb-4532-80cc-37eaca7f756f at port 9858
rm_1        | 2019-11-15 10:28:20 INFO  Configuration:2427 - found resource yarn-site.xml at file:/opt/hadoop/etc/hadoop/yarn-site.xml
nm_1        | 2019-11-15 10:28:17 INFO  NodeManager:45 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2019-11-15 10:28:22,729 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@748d2277{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2019-11-15 10:28:22,000 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2019-11-15 10:28:20,437 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | 2019-11-15 10:28:25,886 INFO impl.RaftServerProxy: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start RPC server
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
datanode_1  | 2019-11-15 10:28:22,831 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@feb98ef{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-1285339938286140839.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_2  | 2019-11-15 10:28:22,000 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2019-11-15 10:28:20,456 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2019-11-15 10:28:25,996 INFO server.GrpcService: 875be6b7-c5fb-4532-80cc-37eaca7f756f: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
rm_1        | 2019-11-15 10:28:20 INFO  NMTokenSecretManagerInRM:75 - NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
datanode_1  | 2019-11-15 10:28:22,840 INFO server.AbstractConnector: Started ServerConnector@db82437{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2019-11-15 10:28:22,038 INFO http.HttpServer2: Jetty bound to port 9882
scm_1       | 2019-11-15 10:28:20,518 INFO util.log: Logging initialized @1316ms
om_1        | ************************************************************/
datanode_3  | 2019-11-15 10:28:30,747 INFO impl.RaftServerProxy: 875be6b7-c5fb-4532-80cc-37eaca7f756f: addNew group-EDE257EB84A1:[875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858] returns group-EDE257EB84A1:java.util.concurrent.CompletableFuture@329f6d93[Not completed]
rm_1        | 2019-11-15 10:28:20 INFO  RMContainerTokenSecretManager:77 - ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService
datanode_1  | 2019-11-15 10:28:22,841 INFO server.Server: Started @3926ms
rm_1        | 2019-11-15 10:28:20 INFO  AMRMTokenSecretManager:94 - AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
scm_1       | 2019-11-15 10:28:20,648 INFO db.DBStoreBuilder: using custom profile for table: deletedBlocks
om_1        | 2019-11-15 10:28:27,067 INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2019-11-15 10:28:30,769 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f: new RaftServerImpl for group-EDE257EB84A1:[875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2019-11-15 10:28:22,845 INFO impl.MetricsSinkAdapter: Sink prometheus started
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
scm_1       | 2019-11-15 10:28:20,648 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedBlocks
om_1        | 2019-11-15 10:28:27,901 INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.18.0.4:9862
datanode_2  | 2019-11-15 10:28:22,039 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_3  | 2019-11-15 10:28:30,771 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
datanode_1  | 2019-11-15 10:28:22,845 INFO impl.MetricsSystemImpl: Registered sink prometheus
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
scm_1       | 2019-11-15 10:28:20,649 INFO db.DBStoreBuilder: using custom profile for table: validCerts
om_1        | 2019-11-15 10:28:27,901 INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
datanode_2  | 2019-11-15 10:28:22,087 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@44f24a20{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2019-11-15 10:28:30,771 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
datanode_1  | 2019-11-15 10:28:22,848 INFO server.BaseHttpServer: HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
rm_1        | 2019-11-15 10:28:20 INFO  ResourceManager:293 - Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
scm_1       | 2019-11-15 10:28:20,649 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:validCerts
om_1        | 2019-11-15 10:28:27,905 WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2  | 2019-11-15 10:28:22,088 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2f897dab{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2019-11-15 10:28:30,771 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
datanode_1  | 2019-11-15 10:28:22,863 INFO util.JvmPauseMonitor: Starting JVM pause monitor
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
scm_1       | 2019-11-15 10:28:20,650 INFO db.DBStoreBuilder: using custom profile for table: revokedCerts
om_1        | 2019-11-15 10:28:27,914 WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2  | 2019-11-15 10:28:22,187 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7654f833{/,file:///tmp/jetty-0.0.0.0-9882-hddsDatanode-_-any-8396136801285849514.dir/webapp/,AVAILABLE}{/hddsDatanode}
datanode_3  | 2019-11-15 10:28:30,772 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
datanode_1  | 2019-11-15 10:28:23,038 INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/metadata/datanode.id
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
scm_1       | 2019-11-15 10:28:20,650 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:revokedCerts
om_1        | 2019-11-15 10:28:28,719 WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_2  | 2019-11-15 10:28:22,195 INFO server.AbstractConnector: Started ServerConnector@414ebb4b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3  | 2019-11-15 10:28:30,773 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
nm_1        | 2019-11-15 10:28:18 INFO  MetricsConfig:112 - loaded properties from hadoop-metrics2.properties
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
datanode_1  | 2019-11-15 10:28:24,969 INFO ozoneimpl.OzoneContainer: Attempting to start container services.
scm_1       | 2019-11-15 10:28:20,668 INFO db.DBStoreBuilder: using custom profile for table: default
om_1        | 2019-11-15 10:28:28,745 INFO util.log: Logging initialized @2696ms
datanode_2  | 2019-11-15 10:28:22,196 INFO server.Server: Started @3609ms
datanode_3  | 2019-11-15 10:28:30,780 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: ConfigurationManager, init=-1: [875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858], old=null, confs=<EMPTY_MAP>
rm_1        | 2019-11-15 10:28:20 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
datanode_1  | 2019-11-15 10:28:24,971 INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
nm_1        | 2019-11-15 10:28:18 INFO  MetricsSystemImpl:376 - Scheduled snapshot period at 10 second(s).
scm_1       | 2019-11-15 10:28:20,668 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default
om_1        | 2019-11-15 10:28:28,835 INFO db.DBStoreBuilder: using custom profile for table: userTable
datanode_3  | 2019-11-15 10:28:30,780 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2019-11-15 10:28:22,200 INFO impl.MetricsSinkAdapter: Sink prometheus started
rm_1        | 2019-11-15 10:28:21 INFO  MetricsConfig:112 - loaded properties from hadoop-metrics2.properties
datanode_1  | 2019-11-15 10:28:24,971 INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 90a6e036-396c-4a54-9c5c-b27150a4cad0 at port 9858
nm_1        | 2019-11-15 10:28:18 INFO  MetricsSystemImpl:192 - NodeManager metrics system started
scm_1       | 2019-11-15 10:28:20,669 INFO db.DBStoreBuilder: Using default options. DBProfile.DISK
om_1        | 2019-11-15 10:28:28,836 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable
datanode_3  | 2019-11-15 10:28:30,786 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2019-11-15 10:28:22,200 INFO impl.MetricsSystemImpl: Registered sink prometheus
rm_1        | 2019-11-15 10:28:21 INFO  MetricsSystemImpl:376 - Scheduled snapshot period at 10 second(s).
datanode_1  | 2019-11-15 10:28:24,992 INFO impl.RaftServerProxy: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start RPC server
scm_1       | 2019-11-15 10:28:20,941 INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@589da3f3
om_1        | 2019-11-15 10:28:28,836 INFO db.DBStoreBuilder: using custom profile for table: volumeTable
datanode_3  | 2019-11-15 10:28:30,787 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ad181c1b-8514-4aa5-a236-ede257eb84a1 does not exist. Creating ...
datanode_2  | 2019-11-15 10:28:22,202 INFO server.BaseHttpServer: HTTP server of HDDSDATANODE is listening at http://0.0.0.0:9882
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
datanode_1  | 2019-11-15 10:28:25,096 INFO server.GrpcService: 90a6e036-396c-4a54-9c5c-b27150a4cad0: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
scm_1       | 2019-11-15 10:28:20,943 INFO net.NodeSchemaLoader: Loading network topology layer schema file
rm_1        | 2019-11-15 10:28:21 INFO  MetricsSystemImpl:192 - ResourceManager metrics system started
datanode_3  | 2019-11-15 10:28:30,807 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ad181c1b-8514-4aa5-a236-ede257eb84a1/in_use.lock acquired by nodename 7@a9d6018f5bef
datanode_2  | 2019-11-15 10:28:22,215 INFO util.JvmPauseMonitor: Starting JVM pause monitor
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
datanode_1  | 2019-11-15 10:28:29,883 INFO impl.RaftServerProxy: 90a6e036-396c-4a54-9c5c-b27150a4cad0: addNew group-7A5565A46256:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858] returns group-7A5565A46256:java.util.concurrent.CompletableFuture@201d38da[Not completed]
scm_1       | 2019-11-15 10:28:21,091 INFO node.SCMNodeManager: Entering startup safe mode.
rm_1        | 2019-11-15 10:28:21 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
om_1        | 2019-11-15 10:28:28,836 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable
datanode_3  | 2019-11-15 10:28:30,836 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ad181c1b-8514-4aa5-a236-ede257eb84a1 has been successfully formatted.
datanode_2  | 2019-11-15 10:28:22,393 INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/metadata/datanode.id
nm_1        | 2019-11-15 10:28:18 INFO  ResourceLocalizationService:219 - per directory file limit = 8192
datanode_1  | 2019-11-15 10:28:29,898 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0: new RaftServerImpl for group-7A5565A46256:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858] with ContainerStateMachine:uninitialized
scm_1       | 2019-11-15 10:28:21,250 INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
rm_1        | 2019-11-15 10:28:21 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
om_1        | 2019-11-15 10:28:28,836 INFO db.DBStoreBuilder: using custom profile for table: bucketTable
datanode_3  | 2019-11-15 10:28:30,841 INFO ratis.ContainerStateMachine: group-EDE257EB84A1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2019-11-15 10:28:24,477 INFO ozoneimpl.OzoneContainer: Attempting to start container services.
nm_1        | 2019-11-15 10:28:18 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
datanode_1  | 2019-11-15 10:28:29,900 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1       | 2019-11-15 10:28:21,267 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
rm_1        | 2019-11-15 10:28:21 INFO  RMNMInfo:63 - Registered RMNMInfo MBean
om_1        | 2019-11-15 10:28:28,836 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable
datanode_3  | 2019-11-15 10:28:30,841 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode_2  | 2019-11-15 10:28:24,478 INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1  | 2019-11-15 10:28:29,900 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
rm_1        | 2019-11-15 10:28:21 INFO  YarnAuthorizationProvider:57 - org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: using custom profile for table: keyTable
datanode_3  | 2019-11-15 10:28:30,846 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2019-11-15 10:28:24,479 INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b3b9007b-f7fa-4b53-9c52-6da6d084318c at port 9858
datanode_1  | 2019-11-15 10:28:29,900 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
scm_1       | 2019-11-15 10:28:21,344 INFO pipeline.SCMPipelineManager: No pipeline exists in current db
nm_1        | 2019-11-15 10:28:18 WARN  AuxServices:130 - The Auxilurary Service named 'mapreduce_shuffle' in the configuration is for class org.apache.hadoop.mapred.ShuffleHandler which has a name of 'httpshuffle'. Because these are not the same tools trying to send ServiceData and read Service Meta Data may have issues unless the refer to the name in the config.
rm_1        | 2019-11-15 10:28:21 INFO  HostsFileReader:131 - Refreshing hosts (include/exclude) list
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable
datanode_3  | 2019-11-15 10:28:30,854 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2019-11-15 10:28:24,506 INFO impl.RaftServerProxy: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start RPC server
datanode_1  | 2019-11-15 10:28:29,901 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
scm_1       | 2019-11-15 10:28:21,347 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
nm_1        | 2019-11-15 10:28:18 INFO  AuxServices:72 - Adding auxiliary service httpshuffle, "mapreduce_shuffle"
rm_1        | 2019-11-15 10:28:21 INFO  Configuration:2427 - found resource capacity-scheduler.xml at file:/opt/hadoop/etc/hadoop/capacity-scheduler.xml
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: using custom profile for table: deletedTable
datanode_3  | 2019-11-15 10:28:30,855 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2019-11-15 10:28:24,623 INFO server.GrpcService: b3b9007b-f7fa-4b53-9c52-6da6d084318c: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1  | 2019-11-15 10:28:29,901 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2019-11-15 10:28:21,572 INFO safemode.HealthyPipelineSafeModeRule:  Total pipeline count is 0, healthy pipeline threshold count is 1
nm_1        | 2019-11-15 10:28:18 INFO  ContainersMonitorImpl:106 -  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin@264f218
rm_1        | 2019-11-15 10:28:21 INFO  CapacitySchedulerConfiguration:603 - max alloc mb per queue for root is undefined
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable
datanode_3  | 2019-11-15 10:28:30,858 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-15 10:28:29,254 INFO impl.RaftServerProxy: b3b9007b-f7fa-4b53-9c52-6da6d084318c: addNew group-A8CA3ED0F381:[b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] returns group-A8CA3ED0F381:java.util.concurrent.CompletableFuture@4768e285[Not completed]
datanode_1  | 2019-11-15 10:28:29,907 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: ConfigurationManager, init=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858], old=null, confs=<EMPTY_MAP>
scm_1       | 2019-11-15 10:28:21,575 INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
rm_1        | 2019-11-15 10:28:21 INFO  CapacitySchedulerConfiguration:607 - max alloc vcore per queue for root is undefined
nm_1        | 2019-11-15 10:28:18 INFO  ContainersMonitorImpl:111 -  Using ResourceCalculatorProcessTree : null
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: using custom profile for table: openKeyTable
datanode_3  | 2019-11-15 10:28:30,869 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2019-11-15 10:28:29,907 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2019-11-15 10:28:29,279 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c: new RaftServerImpl for group-A8CA3ED0F381:[b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] with ContainerStateMachine:uninitialized
rm_1        | 2019-11-15 10:28:21 INFO  ParentQueue:121 - root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
nm_1        | 2019-11-15 10:28:18 INFO  ContainersMonitorImpl:151 - Physical memory check enabled: false
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable
datanode_3  | 2019-11-15 10:28:30,874 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-15 10:28:29,911 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2019-11-15 10:28:21,575 WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
datanode_2  | 2019-11-15 10:28:29,280 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
rm_1        | , reservationsContinueLooking=true
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: using custom profile for table: s3Table
nm_1        | 2019-11-15 10:28:18 INFO  ContainersMonitorImpl:152 - Virtual memory check enabled: false
datanode_3  | 2019-11-15 10:28:30,881 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2019-11-15 10:28:29,913 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/794f1888-dc9b-4333-bb31-7a5565a46256 does not exist. Creating ...
scm_1       | 2019-11-15 10:28:22,248 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2  | 2019-11-15 10:28:29,281 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
rm_1        | 2019-11-15 10:28:21 INFO  ParentQueue:100 - Initialized parent-queue root name=root, fullname=root
om_1        | 2019-11-15 10:28:28,837 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table
nm_1        | 2019-11-15 10:28:18 INFO  NodeStatusUpdaterImpl:181 - Initialized nodemanager for null: physical-memory=8192 virtual-memory=17204 virtual-cores=8
datanode_3  | 2019-11-15 10:28:30,888 INFO segmented.SegmentedRaftLogWorker: new 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/ad181c1b-8514-4aa5-a236-ede257eb84a1
datanode_1  | 2019-11-15 10:28:29,967 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/794f1888-dc9b-4333-bb31-7a5565a46256/in_use.lock acquired by nodename 7@65f0630e46c9
scm_1       | 2019-11-15 10:28:22,271 INFO ipc.Server: Starting Socket Reader #1 for port 9861
datanode_2  | 2019-11-15 10:28:29,281 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
rm_1        | 2019-11-15 10:28:21 INFO  CapacitySchedulerConfiguration:603 - max alloc mb per queue for root.default is undefined
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable
nm_1        | 2019-11-15 10:28:18 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000
datanode_3  | 2019-11-15 10:28:30,889 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2019-11-15 10:28:30,020 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/794f1888-dc9b-4333-bb31-7a5565a46256 has been successfully formatted.
scm_1       | 2019-11-15 10:28:22,307 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2  | 2019-11-15 10:28:29,281 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
rm_1        | 2019-11-15 10:28:21 INFO  CapacitySchedulerConfiguration:607 - max alloc vcore per queue for root.default is undefined
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable
nm_1        | 2019-11-15 10:28:18 INFO  Server:722 - Starting Socket Reader #1 for port 42530
datanode_3  | 2019-11-15 10:28:30,889 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2019-11-15 10:28:30,025 INFO ratis.ContainerStateMachine: group-7A5565A46256: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2019-11-15 10:28:29,282 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
rm_1        | 2019-11-15 10:28:21 INFO  LeafQueue:215 - Initializing default
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: using custom profile for table: dTokenTable
nm_1        | 2019-11-15 10:28:18 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
datanode_3  | 2019-11-15 10:28:30,891 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2019-11-15 10:28:22,308 INFO ipc.Server: Starting Socket Reader #1 for port 9863
datanode_1  | 2019-11-15 10:28:30,025 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode_2  | 2019-11-15 10:28:29,288 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: ConfigurationManager, init=-1: [b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null, confs=<EMPTY_MAP>
rm_1        | capacity = 1.0 [= (float) configuredCapacity / 100 ]
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable
nm_1        | 2019-11-15 10:28:18 INFO  ContainerManagerImpl:427 - Blocking new container-requests as container manager rpc server is still starting.
datanode_3  | 2019-11-15 10:28:30,891 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm_1       | 2019-11-15 10:28:22,320 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1  | 2019-11-15 10:28:30,052 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2019-11-15 10:28:29,288 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
rm_1        | asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
nm_1        | 2019-11-15 10:28:18 INFO  Server:962 - IPC Server Responder: starting
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable
datanode_3  | 2019-11-15 10:28:30,892 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
scm_1       | 2019-11-15 10:28:22,321 INFO ipc.Server: Starting Socket Reader #1 for port 9860
datanode_1  | 2019-11-15 10:28:30,058 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2019-11-15 10:28:29,293 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
rm_1        | maxCapacity = 1.0 [= configuredMaxCapacity ]
nm_1        | 2019-11-15 10:28:18 INFO  Server:801 - IPC Server listener on 42530: starting
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable
datanode_3  | 2019-11-15 10:28:30,892 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1       | 2019-11-15 10:28:22,414 INFO hdfs.DFSUtil: Starting Web-server for scm at: http://0.0.0.0:9876
datanode_2  | 2019-11-15 10:28:29,294 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ea248aff-1952-46bf-9756-a8ca3ed0f381 does not exist. Creating ...
datanode_1  | 2019-11-15 10:28:30,058 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
rm_1        | absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
nm_1        | 2019-11-15 10:28:18 INFO  NMContainerTokenSecretManager:260 - Updating node address : nm:42530
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: using custom profile for table: prefixTable
datanode_3  | 2019-11-15 10:28:30,894 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2019-11-15 10:28:22,554 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2019-11-15 10:28:29,368 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ea248aff-1952-46bf-9756-a8ca3ed0f381/in_use.lock acquired by nodename 7@bfd1a25ed5e9
datanode_1  | 2019-11-15 10:28:30,061 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
rm_1        | userLimit = 100 [= configuredUserLimit ]
nm_1        | 2019-11-15 10:28:18 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500
om_1        | 2019-11-15 10:28:28,838 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable
datanode_3  | 2019-11-15 10:28:30,894 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       | 2019-11-15 10:28:22,577 INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
datanode_2  | 2019-11-15 10:28:29,412 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ea248aff-1952-46bf-9756-a8ca3ed0f381 has been successfully formatted.
rm_1        | userLimitFactor = 1.0 [= configuredUserLimitFactor ]
nm_1        | 2019-11-15 10:28:18 INFO  Server:722 - Starting Socket Reader #1 for port 8040
datanode_1  | 2019-11-15 10:28:30,070 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1        | 2019-11-15 10:28:28,851 INFO db.DBStoreBuilder: using custom profile for table: default
datanode_3  | 2019-11-15 10:28:30,895 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2019-11-15 10:28:22,586 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2019-11-15 10:28:29,418 INFO ratis.ContainerStateMachine: group-A8CA3ED0F381: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
rm_1        | maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
nm_1        | 2019-11-15 10:28:18 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
datanode_1  | 2019-11-15 10:28:30,088 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
om_1        | 2019-11-15 10:28:28,851 INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default
datanode_3  | 2019-11-15 10:28:30,905 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2019-11-15 10:28:22,589 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
datanode_2  | 2019-11-15 10:28:29,418 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
rm_1        | maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
datanode_1  | 2019-11-15 10:28:30,092 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  Server:962 - IPC Server Responder: starting
om_1        | 2019-11-15 10:28:28,852 INFO db.DBStoreBuilder: Using default options. DBProfile.DISK
datanode_3  | 2019-11-15 10:28:30,910 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       | 2019-11-15 10:28:22,589 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2019-11-15 10:28:29,422 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
rm_1        | usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
datanode_1  | 2019-11-15 10:28:30,099 INFO segmented.SegmentedRaftLogWorker: new 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/794f1888-dc9b-4333-bb31-7a5565a46256
nm_1        | 2019-11-15 10:28:18 INFO  Server:801 - IPC Server listener on 8040: starting
om_1        | 2019-11-15 10:28:29,581 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3  | 2019-11-15 10:28:30,917 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2019-11-15 10:28:29,429 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1       | 2019-11-15 10:28:22,589 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
rm_1        | absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
datanode_1  | 2019-11-15 10:28:30,100 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  ResourceLocalizationService:344 - Localizer started on port 8040
om_1        | 2019-11-15 10:28:29,591 INFO ipc.Server: Starting Socket Reader #1 for port 9862
datanode_3  | 2019-11-15 10:28:30,958 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2019-11-15 10:28:29,429 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2019-11-15 10:28:22,618 INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
rm_1        | maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
datanode_1  | 2019-11-15 10:28:30,100 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  IndexCache:47 - IndexCache created with max memory = 10485760
om_1        | 2019-11-15 10:28:29,625 INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.18.0.4:9862
datanode_3  | 2019-11-15 10:28:30,961 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm_1       | 2019-11-15 10:28:22,697 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
rm_1        | minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
datanode_1  | 2019-11-15 10:28:30,101 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-15 10:28:29,432 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  ShuffleHandler:510 - httpshuffle listening on port 13562
om_1        | 2019-11-15 10:28:29,701 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3  | 2019-11-15 10:28:30,962 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 2019-11-15 10:28:22,748 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
rm_1        | maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
datanode_1  | 2019-11-15 10:28:30,101 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
datanode_2  | 2019-11-15 10:28:29,441 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1        | 2019-11-15 10:28:29,738 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3  | 2019-11-15 10:28:30,987 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:22,748 INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
rm_1        | numContainers = 0 [= currentNumContainers ]
datanode_1  | 2019-11-15 10:28:30,102 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
nm_1        | 2019-11-15 10:28:18 INFO  ContainerManagerImpl:472 - ContainerManager started at nm/172.18.0.2:42530
datanode_2  | 2019-11-15 10:28:29,446 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
om_1        | 2019-11-15 10:28:29,738 INFO impl.MetricsSystemImpl: OzoneManager metrics system started
datanode_3  | 2019-11-15 10:28:30,989 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:22,984 INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
datanode_1  | 2019-11-15 10:28:30,102 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
rm_1        | state = RUNNING [= configuredState ]
nm_1        | 2019-11-15 10:28:18 INFO  ContainerManagerImpl:473 - ContainerManager bound to 0.0.0.0/0.0.0.0:0
datanode_2  | 2019-11-15 10:28:29,452 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1        | 2019-11-15 10:28:29,779 INFO ipc.Server: IPC Server listener on 9862: starting
datanode_3  | 2019-11-15 10:28:30,991 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: start as a follower, conf=-1: [875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858], old=null
scm_1       | 2019-11-15 10:28:22,985 INFO ipc.Server: IPC Server Responder: starting
datanode_1  | 2019-11-15 10:28:30,103 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
rm_1        | acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nm_1        | 2019-11-15 10:28:18 INFO  WebServer:70 - Instantiating NMWebApp at 0.0.0.0:8042
datanode_2  | 2019-11-15 10:28:29,458 INFO segmented.SegmentedRaftLogWorker: new b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/ea248aff-1952-46bf-9756-a8ca3ed0f381
om_1        | 2019-11-15 10:28:29,779 INFO ipc.Server: IPC Server Responder: starting
datanode_3  | 2019-11-15 10:28:30,993 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 2019-11-15 10:28:22,985 INFO ipc.Server: IPC Server listener on 9860: starting
rm_1        | nodeLocalityDelay = 40
datanode_1  | 2019-11-15 10:28:30,103 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
nm_1        | 2019-11-15 10:28:19 INFO  log:67 - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
datanode_2  | 2019-11-15 10:28:29,459 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
om_1        | 2019-11-15 10:28:29,805 INFO hdfs.DFSUtil: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
datanode_3  | 2019-11-15 10:28:30,994 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start FollowerState
scm_1       | 2019-11-15 10:28:22,997 INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
rm_1        | labels=*,
datanode_1  | 2019-11-15 10:28:30,104 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
nm_1        | 2019-11-15 10:28:19 INFO  AuthenticationFilter:283 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2019-11-15 10:28:29,459 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om_1        | 2019-11-15 10:28:29,900 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2019-11-15 10:28:31,002 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EDE257EB84A1,id=875be6b7-c5fb-4532-80cc-37eaca7f756f
scm_1       | 2019-11-15 10:28:22,998 INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
rm_1        | nodeLocalityDelay = 40
datanode_1  | 2019-11-15 10:28:30,111 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
nm_1        | 2019-11-15 10:28:19 INFO  HttpRequestLog:80 - Http request log for http.requests.nodemanager is not defined
datanode_2  | 2019-11-15 10:28:29,460 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om_1        | 2019-11-15 10:28:29,903 INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
datanode_3  | 2019-11-15 10:28:31,004 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:22,998 INFO ipc.Server: IPC Server Responder: starting
rm_1        | reservationsContinueLooking = true
datanode_1  | 2019-11-15 10:28:30,115 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:730 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2019-11-15 10:28:29,461 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om_1        | 2019-11-15 10:28:29,914 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
scm_1       | 2019-11-15 10:28:22,998 INFO ipc.Server: IPC Server listener on 9863: starting
rm_1        | preemptionDisabled = true
datanode_3  | 2019-11-15 10:28:31,074 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS ONE #id: "ad181c1b-8514-4aa5-a236-ede257eb84a1"
datanode_1  | 2019-11-15 10:28:30,121 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:705 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
datanode_2  | 2019-11-15 10:28:29,461 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
om_1        | 2019-11-15 10:28:29,917 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm_1       | 2019-11-15 10:28:23,001 INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
datanode_3  |  command succeed on datanode 875be6b7-c5fb-4532-80cc-37eaca7f756f.
rm_1        | 
datanode_1  | 2019-11-15 10:28:30,122 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:713 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2019-11-15 10:28:29,462 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1        | 2019-11-15 10:28:29,917 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3  | 2019-11-15 10:28:31,079 INFO impl.RaftServerProxy: 875be6b7-c5fb-4532-80cc-37eaca7f756f: addNew group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] returns group-16869120CD2F:java.util.concurrent.CompletableFuture@1cf40bb0[Not completed]
rm_1        | 2019-11-15 10:28:21 INFO  CapacityScheduler:634 - Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
scm_1       | 2019-11-15 10:28:23,001 INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
datanode_1  | 2019-11-15 10:28:30,122 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2019-11-15 10:28:29,463 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1        | 2019-11-15 10:28:29,917 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
rm_1        | 2019-11-15 10:28:21 INFO  CapacityScheduler:634 - Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
datanode_3  | 2019-11-15 10:28:31,083 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f: new RaftServerImpl for group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] with ContainerStateMachine:uninitialized
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:713 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2019-11-15 10:28:23,002 INFO ipc.Server: IPC Server Responder: starting
datanode_1  | 2019-11-15 10:28:30,124 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2019-11-15 10:28:29,463 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1        | 2019-11-15 10:28:29,951 INFO http.HttpServer2: Jetty bound to port 9874
rm_1        | 2019-11-15 10:28:21 INFO  CapacityScheduler:478 - Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
datanode_3  | 2019-11-15 10:28:31,084 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:424 - adding path spec: /node/*
datanode_1  | 2019-11-15 10:28:30,138 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:23,002 INFO ipc.Server: IPC Server listener on 9861: starting
datanode_2  | 2019-11-15 10:28:29,464 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1        | 2019-11-15 10:28:29,952 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
rm_1        | 2019-11-15 10:28:21 INFO  CapacityScheduler:447 - Initialized queue mappings, override: false
datanode_3  | 2019-11-15 10:28:31,084 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:424 - adding path spec: /ws/*
scm_1       | 2019-11-15 10:28:23,016 INFO http.HttpServer2: Jetty bound to port 9876
datanode_2  | 2019-11-15 10:28:29,473 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om_1        | 2019-11-15 10:28:29,985 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f94e148{/logs,file:///var/log/hadoop/,AVAILABLE}
rm_1        | 2019-11-15 10:28:21 INFO  CapacityScheduler:314 - Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
datanode_3  | 2019-11-15 10:28:31,084 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
datanode_1  | 2019-11-15 10:28:30,140 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
nm_1        | 2019-11-15 10:28:19 INFO  WebApps:288 - Registered webapp guice modules
scm_1       | 2019-11-15 10:28:23,018 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
datanode_2  | 2019-11-15 10:28:29,547 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1        | 2019-11-15 10:28:29,985 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f37b6d9{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
rm_1        | 2019-11-15 10:28:21 INFO  SystemMetricsPublisher:93 - YARN system metrics publishing service is not enabled
datanode_3  | 2019-11-15 10:28:31,084 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2019-11-15 10:28:30,141 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: start as a follower, conf=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858], old=null
nm_1        | 2019-11-15 10:28:19 INFO  HttpServer2:935 - Jetty bound to port 8042
scm_1       | 2019-11-15 10:28:23,069 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@540dbda9{/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2019-11-15 10:28:29,551 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1        | 2019-11-15 10:28:30,071 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5984feef{/,file:///tmp/jetty-0.0.0.0-9874-ozoneManager-_-any-8873201644722709019.dir/webapp/,AVAILABLE}{/ozoneManager}
datanode_3  | 2019-11-15 10:28:31,084 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
rm_1        | 2019-11-15 10:28:21 INFO  ResourceManager:1009 - Transitioning to active state
datanode_1  | 2019-11-15 10:28:30,142 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: changes role from      null to FOLLOWER at term 0 for startAsFollower
nm_1        | 2019-11-15 10:28:19 INFO  log:67 - jetty-6.1.26
scm_1       | 2019-11-15 10:28:23,070 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@545e57d7{/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2019-11-15 10:28:29,552 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om_1        | 2019-11-15 10:28:30,076 INFO server.AbstractConnector: Started ServerConnector@2ae62bb6{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
datanode_3  | 2019-11-15 10:28:31,085 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: ConfigurationManager, init=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2019-11-15 10:28:30,143 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start FollowerState
nm_1        | 2019-11-15 10:28:19 INFO  log:67 - Extract jar:file:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar!/webapps/node to /tmp/Jetty_0_0_0_0_8042_node____19tj0x/webapp
rm_1        | 2019-11-15 10:28:21 INFO  RMStateStore:466 - Updating AMRMToken
datanode_2  | 2019-11-15 10:28:29,552 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om_1        | 2019-11-15 10:28:30,076 INFO server.Server: Started @4027ms
scm_1       | 2019-11-15 10:28:23,178 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@13d9261f{/,file:///tmp/jetty-0.0.0.0-9876-scm-_-any-15985827709428632662.dir/webapp/,AVAILABLE}{/scm}
datanode_3  | 2019-11-15 10:28:31,085 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2019-11-15 10:28:30,147 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A5565A46256,id=90a6e036-396c-4a54-9c5c-b27150a4cad0
nm_1        | Nov 15, 2019 10:28:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
datanode_2  | 2019-11-15 10:28:29,553 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om_1        | 2019-11-15 10:28:30,079 INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1       | 2019-11-15 10:28:23,186 INFO server.AbstractConnector: Started ServerConnector@200260e7{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
rm_1        | 2019-11-15 10:28:21 INFO  RMContainerTokenSecretManager:105 - Rolling master-key for container-tokens
datanode_3  | 2019-11-15 10:28:31,085 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
nm_1        | INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
datanode_2  | 2019-11-15 10:28:29,576 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
om_1        | 2019-11-15 10:28:30,079 INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  | 2019-11-15 10:28:30,148 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:23,186 INFO server.Server: Started @3984ms
datanode_3  | 2019-11-15 10:28:31,085 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f does not exist. Creating ...
nm_1        | Nov 15, 2019 10:28:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
datanode_2  | 2019-11-15 10:28:29,578 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:28:21 INFO  NMTokenSecretManagerInRM:95 - Rolling master-key for nm-tokens
om_1        | 2019-11-15 10:28:30,080 INFO server.BaseHttpServer: HTTP server of OZONEMANAGER is listening at http://0.0.0.0:9874
scm_1       | 2019-11-15 10:28:23,189 INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1  | 2019-11-15 10:28:30,201 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS ONE #id: "794f1888-dc9b-4333-bb31-7a5565a46256"
datanode_3  | 2019-11-15 10:28:31,125 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/in_use.lock acquired by nodename 7@a9d6018f5bef
nm_1        | INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
datanode_2  | 2019-11-15 10:28:29,580 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: start as a follower, conf=-1: [b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
rm_1        | 2019-11-15 10:28:21 INFO  AbstractDelegationTokenSecretManager:326 - Updating the current master key for generating delegation tokens
om_1        | 2019-11-15 10:28:54,556 INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
scm_1       | 2019-11-15 10:28:23,189 INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  |  command succeed on datanode 90a6e036-396c-4a54-9c5c-b27150a4cad0.
datanode_3  | 2019-11-15 10:28:31,150 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f has been successfully formatted.
nm_1        | Nov 15, 2019 10:28:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
datanode_2  | 2019-11-15 10:28:29,581 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: changes role from      null to FOLLOWER at term 0 for startAsFollower
rm_1        | 2019-11-15 10:28:21 INFO  RMDelegationTokenSecretManager:85 - storing master key with keyID 1
datanode_1  | 2019-11-15 10:28:30,204 INFO impl.RaftServerProxy: 90a6e036-396c-4a54-9c5c-b27150a4cad0: addNew group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] returns group-16869120CD2F:java.util.concurrent.CompletableFuture@661e8a3b[Not completed]
scm_1       | 2019-11-15 10:28:23,192 INFO server.BaseHttpServer: HTTP server of SCM is listening at http://0.0.0.0:9876
datanode_3  | 2019-11-15 10:28:31,150 INFO ratis.ContainerStateMachine: group-16869120CD2F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
nm_1        | INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
datanode_2  | 2019-11-15 10:28:29,582 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start FollowerState
rm_1        | 2019-11-15 10:28:21 INFO  RMStateStore:417 - Storing RMDTMasterKey.
datanode_1  | 2019-11-15 10:28:30,207 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0: new RaftServerImpl for group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
nm_1        | Nov 15, 2019 10:28:19 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
scm_1       | 2019-11-15 10:28:23,209 INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 2019-11-15 10:28:29,586 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8CA3ED0F381,id=b3b9007b-f7fa-4b53-9c52-6da6d084318c
rm_1        | 2019-11-15 10:28:21 INFO  AbstractDelegationTokenSecretManager:644 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
datanode_1  | 2019-11-15 10:28:30,207 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
nm_1        | INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
scm_1       | 2019-11-15 10:28:26,259 INFO net.NetworkTopology: Added a new node: /default-rack/b3b9007b-f7fa-4b53-9c52-6da6d084318c
datanode_2  | 2019-11-15 10:28:29,588 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:28:21 INFO  AbstractDelegationTokenSecretManager:326 - Updating the current master key for generating delegation tokens
datanode_1  | 2019-11-15 10:28:30,208 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
nm_1        | Nov 15, 2019 10:28:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
scm_1       | 2019-11-15 10:28:26,260 INFO node.SCMNodeManager: Registered Data node : b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
rm_1        | 2019-11-15 10:28:21 INFO  RMDelegationTokenSecretManager:85 - storing master key with keyID 2
datanode_2  | 2019-11-15 10:28:29,645 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS ONE #id: "ea248aff-1952-46bf-9756-a8ca3ed0f381"
datanode_1  | 2019-11-15 10:28:30,208 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
nm_1        | INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
scm_1       | 2019-11-15 10:28:26,267 INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 1 required.
rm_1        | 2019-11-15 10:28:21 INFO  RMStateStore:417 - Storing RMDTMasterKey.
datanode_2  |  command succeed on datanode b3b9007b-f7fa-4b53-9c52-6da6d084318c.
datanode_1  | 2019-11-15 10:28:30,208 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
nm_1        | Nov 15, 2019 10:28:20 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
scm_1       | 2019-11-15 10:28:26,267 INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
rm_1        | 2019-11-15 10:28:21 INFO  AsyncDispatcher:209 - Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
datanode_2  | 2019-11-15 10:28:29,648 INFO impl.RaftServerProxy: b3b9007b-f7fa-4b53-9c52-6da6d084318c: addNew group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] returns group-16869120CD2F:java.util.concurrent.CompletableFuture@7e13a98d[Not completed]
datanode_1  | 2019-11-15 10:28:30,208 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2019-11-15 10:28:31,151 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
nm_1        | INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
rm_1        | 2019-11-15 10:28:21 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000
datanode_2  | 2019-11-15 10:28:29,650 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c: new RaftServerImpl for group-16869120CD2F:[90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858] with ContainerStateMachine:uninitialized
scm_1       | 2019-11-15 10:28:26,268 INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
datanode_1  | 2019-11-15 10:28:30,208 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: ConfigurationManager, init=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null, confs=<EMPTY_MAP>
nm_1        | Nov 15, 2019 10:28:20 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
rm_1        | 2019-11-15 10:28:21 INFO  Server:722 - Starting Socket Reader #1 for port 8031
datanode_3  | 2019-11-15 10:28:31,152 INFO segmented.SegmentedRaftLogWorker: new 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f
datanode_2  | 2019-11-15 10:28:29,650 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1       | 2019-11-15 10:28:26,270 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ea248aff-1952-46bf-9756-a8ca3ed0f381 create command to datanode b3b9007b-f7fa-4b53-9c52-6da6d084318c
datanode_1  | 2019-11-15 10:28:30,209 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
nm_1        | INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
rm_1        | 2019-11-15 10:28:21 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
datanode_3  | 2019-11-15 10:28:31,152 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2019-11-15 10:28:29,650 INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2019-11-15 10:28:26,288 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ea248aff-1952-46bf-9756-a8ca3ed0f381, Nodes: b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
datanode_1  | 2019-11-15 10:28:30,209 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
nm_1        | 2019-11-15 10:28:20 INFO  log:67 - Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
rm_1        | 2019-11-15 10:28:21 INFO  Server:962 - IPC Server Responder: starting
datanode_3  | 2019-11-15 10:28:31,152 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2019-11-15 10:28:29,650 INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
scm_1       | 2019-11-15 10:28:26,883 INFO net.NetworkTopology: Added a new node: /default-rack/90a6e036-396c-4a54-9c5c-b27150a4cad0
datanode_1  | 2019-11-15 10:28:30,209 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f does not exist. Creating ...
nm_1        | 2019-11-15 10:28:20 INFO  WebApps:307 - Web app node started at 8042
rm_1        | 2019-11-15 10:28:21 INFO  Server:801 - IPC Server listener on 8031: starting
datanode_3  | 2019-11-15 10:28:31,152 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2019-11-15 10:28:29,650 INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
scm_1       | 2019-11-15 10:28:26,883 INFO node.SCMNodeManager: Registered Data node : 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_1  | 2019-11-15 10:28:30,263 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/in_use.lock acquired by nodename 7@65f0630e46c9
nm_1        | 2019-11-15 10:28:20 INFO  RMProxy:98 - Connecting to ResourceManager at rm/172.18.0.8:8031
rm_1        | 2019-11-15 10:28:21 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000
datanode_3  | 2019-11-15 10:28:31,152 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2019-11-15 10:28:29,651 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2019-11-15 10:28:26,883 INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
datanode_1  | 2019-11-15 10:28:30,326 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f has been successfully formatted.
nm_1        | 2019-11-15 10:28:21 INFO  NodeStatusUpdaterImpl:435 - Sending out 0 NM container statuses: []
rm_1        | 2019-11-15 10:28:21 INFO  Server:722 - Starting Socket Reader #1 for port 8030
datanode_3  | 2019-11-15 10:28:31,152 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
datanode_2  | 2019-11-15 10:28:29,651 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: ConfigurationManager, init=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null, confs=<EMPTY_MAP>
scm_1       | 2019-11-15 10:28:26,883 INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
datanode_1  | 2019-11-15 10:28:30,326 INFO ratis.ContainerStateMachine: group-16869120CD2F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
nm_1        | 2019-11-15 10:28:21 INFO  NodeStatusUpdaterImpl:268 - Registering with RM using containers :[]
rm_1        | 2019-11-15 10:28:21 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
datanode_2  | 2019-11-15 10:28:29,651 INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2019-11-15 10:28:31,153 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1       | 2019-11-15 10:28:26,887 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=794f1888-dc9b-4333-bb31-7a5565a46256 create command to datanode 90a6e036-396c-4a54-9c5c-b27150a4cad0
datanode_1  | 2019-11-15 10:28:30,327 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
nm_1        | 2019-11-15 10:28:22 INFO  Client:872 - Retrying connect to server: rm/172.18.0.8:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
rm_1        | 2019-11-15 10:28:21 INFO  Server:962 - IPC Server Responder: starting
datanode_2  | 2019-11-15 10:28:29,651 INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2019-11-15 10:28:31,153 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2019-11-15 10:28:26,888 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 794f1888-dc9b-4333-bb31-7a5565a46256, Nodes: 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
datanode_1  | 2019-11-15 10:28:30,327 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
nm_1        | 2019-11-15 10:28:22 INFO  NMContainerTokenSecretManager:138 - Rolling master-key for container-tokens, got key with id 1644215314
rm_1        | 2019-11-15 10:28:21 INFO  Server:801 - IPC Server listener on 8030: starting
datanode_2  | 2019-11-15 10:28:29,651 INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f does not exist. Creating ...
datanode_3  | 2019-11-15 10:28:31,153 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       | 2019-11-15 10:28:27,734 INFO net.NetworkTopology: Added a new node: /default-rack/875be6b7-c5fb-4532-80cc-37eaca7f756f
datanode_1  | 2019-11-15 10:28:30,328 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
nm_1        | 2019-11-15 10:28:22 INFO  NMTokenSecretManagerInNM:135 - Rolling master-key for container-tokens, got key with id -1502602633
rm_1        | 2019-11-15 10:28:21 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000
datanode_2  | 2019-11-15 10:28:29,708 INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/in_use.lock acquired by nodename 7@bfd1a25ed5e9
datanode_3  | 2019-11-15 10:28:31,153 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2019-11-15 10:28:27,734 INFO node.SCMNodeManager: Registered Data node : 875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_1  | 2019-11-15 10:28:30,328 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
nm_1        | 2019-11-15 10:28:22 INFO  NodeStatusUpdaterImpl:317 - Registered with ResourceManager as nm:42530 with total resource of <memory:8192, vCores:8>
rm_1        | 2019-11-15 10:28:21 INFO  Server:722 - Starting Socket Reader #1 for port 8032
datanode_2  | 2019-11-15 10:28:29,780 INFO storage.RaftStorage: Storage directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f has been successfully formatted.
datanode_3  | 2019-11-15 10:28:31,153 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2019-11-15 10:28:27,734 INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
datanode_1  | 2019-11-15 10:28:30,328 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
nm_1        | 2019-11-15 10:28:22 INFO  NodeStatusUpdaterImpl:319 - Notifying ContainerManager to unblock new container-requests
rm_1        | 2019-11-15 10:28:21 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
datanode_2  | 2019-11-15 10:28:29,780 INFO ratis.ContainerStateMachine: group-16869120CD2F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2019-11-15 10:28:31,153 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       | 2019-11-15 10:28:27,734 INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
nm_1        | 2019-11-15 10:29:37 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
datanode_1  | 2019-11-15 10:28:30,328 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  Server:962 - IPC Server Responder: starting
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
datanode_3  | 2019-11-15 10:28:31,154 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | 2019-11-15 10:28:27,735 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ad181c1b-8514-4aa5-a236-ede257eb84a1 create command to datanode 875be6b7-c5fb-4532-80cc-37eaca7f756f
nm_1        | 2019-11-15 10:29:37 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0001_01_000001 by user hadoop
datanode_1  | 2019-11-15 10:28:30,329 INFO segmented.SegmentedRaftLogWorker: new 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f
rm_1        | 2019-11-15 10:28:21 INFO  Server:801 - IPC Server listener on 8032: starting
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2019-11-15 10:28:31,154 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       | 2019-11-15 10:28:27,736 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ad181c1b-8514-4aa5-a236-ede257eb84a1, Nodes: 875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED]
nm_1        | 2019-11-15 10:29:37 INFO  ContainerManagerImpl:850 - Creating a new application reference for app application_1573813701275_0001
datanode_1  | 2019-11-15 10:28:30,329 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  ResourceManager:1025 - Transitioned to active state
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2019-11-15 10:28:31,155 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm_1       | 2019-11-15 10:28:27,738 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f create command to datanode 90a6e036-396c-4a54-9c5c-b27150a4cad0
nm_1        | 2019-11-15 10:29:37 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.8	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000001
datanode_1  | 2019-11-15 10:28:30,329 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  log:67 - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
datanode_3  | 2019-11-15 10:28:31,155 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 2019-11-15 10:28:27,738 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f create command to datanode 875be6b7-c5fb-4532-80cc-37eaca7f756f
nm_1        | 2019-11-15 10:29:37 INFO  ApplicationImpl:464 - Application application_1573813701275_0001 transitioned from NEW to INITING
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2019-11-15 10:28:30,329 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  AuthenticationFilter:283 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2019-11-15 10:28:31,155 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:27,738 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f create command to datanode b3b9007b-f7fa-4b53-9c52-6da6d084318c
nm_1        | 2019-11-15 10:29:37 INFO  ApplicationImpl:304 - Adding container_1573813701275_0001_01_000001 to application application_1573813701275_0001
datanode_1  | 2019-11-15 10:28:30,329 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  HttpRequestLog:80 - Http request log for http.requests.resourcemanager is not defined
datanode_3  | 2019-11-15 10:28:31,155 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
scm_1       | 2019-11-15 10:28:27,739 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 38036a6e-5a45-44cd-bb65-16869120cd2f, Nodes: 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED]
nm_1        | 2019-11-15 10:29:38 INFO  ApplicationImpl:464 - Application application_1573813701275_0001 transitioned from INITING to RUNNING
datanode_2  | 2019-11-15 10:28:29,781 INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2019-11-15 10:28:30,329 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:730 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
scm_1       | 2019-11-15 10:28:29,674 INFO pipeline.PipelineReportHandler: Pipeline ONE PipelineID=ea248aff-1952-46bf-9756-a8ca3ed0f381 reported by b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2019-11-15 10:28:31,155 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: start as a follower, conf=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
nm_1        | 2019-11-15 10:29:38 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000001 transitioned from NEW to LOCALIZING
datanode_2  | 2019-11-15 10:28:29,782 INFO segmented.SegmentedRaftLogWorker: new b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f
datanode_1  | 2019-11-15 10:28:30,330 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:705 - Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
scm_1       | 2019-11-15 10:28:29,674 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ea248aff-1952-46bf-9756-a8ca3ed0f381, Nodes: b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED] moved to OPEN state
datanode_3  | 2019-11-15 10:28:31,156 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: changes role from      null to FOLLOWER at term 0 for startAsFollower
nm_1        | 2019-11-15 10:29:38 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0001
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2019-11-15 10:28:30,330 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:713 - Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
scm_1       | 2019-11-15 10:28:29,676 INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
datanode_3  | 2019-11-15 10:28:31,156 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start FollowerState
nm_1        | 2019-11-15 10:29:38 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.splitmetainfo transitioned from INIT to DOWNLOADING
datanode_1  | 2019-11-15 10:28:30,331 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:713 - Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
scm_1       | 2019-11-15 10:28:29,677 INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2019-11-15 10:28:31,157 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-16869120CD2F,id=875be6b7-c5fb-4532-80cc-37eaca7f756f
nm_1        | 2019-11-15 10:29:38 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.jar transitioned from INIT to DOWNLOADING
datanode_1  | 2019-11-15 10:28:30,331 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:705 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
scm_1       | 2019-11-15 10:28:29,788 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2019-11-15 10:28:31,157 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
nm_1        | 2019-11-15 10:29:38 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.split transitioned from INIT to DOWNLOADING
datanode_1  | 2019-11-15 10:28:30,332 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:713 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2019-11-15 10:28:30,240 INFO pipeline.PipelineReportHandler: Pipeline ONE PipelineID=794f1888-dc9b-4333-bb31-7a5565a46256 reported by 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2019-11-15 10:28:31,157 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS THREE #id: "38036a6e-5a45-44cd-bb65-16869120cd2f"
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
nm_1        | 2019-11-15 10:29:38 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.xml transitioned from INIT to DOWNLOADING
datanode_1  | 2019-11-15 10:28:30,333 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:713 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2019-11-15 10:28:30,240 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 794f1888-dc9b-4333-bb31-7a5565a46256, Nodes: 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED] moved to OPEN state
datanode_3  |  command succeed on datanode 875be6b7-c5fb-4532-80cc-37eaca7f756f.
nm_1        | 2019-11-15 10:29:38 INFO  ResourceLocalizationService:711 - Created localizer for container_1573813701275_0001_01_000001
datanode_1  | 2019-11-15 10:28:30,334 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:424 - adding path spec: /cluster/*
scm_1       | 2019-11-15 10:28:30,241 INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554432 (custom)
datanode_3  | 2019-11-15 10:28:35,373 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:b3b9007b-f7fa-4b53-9c52-6da6d084318c
nm_1        | 2019-11-15 10:29:38 INFO  ResourceLocalizationService:1195 - Writing credentials to the nmPrivate file /tmp/hadoop-hadoop/nm-local-dir/nmPrivate/container_1573813701275_0001_01_000001.tokens. Credentials list: 
datanode_1  | 2019-11-15 10:28:30,334 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
rm_1        | 2019-11-15 10:28:21 INFO  HttpServer2:424 - adding path spec: /ws/*
scm_1       | 2019-11-15 10:28:30,241 INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2019-11-15 10:28:35,373 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: shutdown FollowerState
nm_1        | 2019-11-15 10:29:38 INFO  DefaultContainerExecutor:612 - Initializing user hadoop
datanode_1  | 2019-11-15 10:28:30,334 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
rm_1        | 2019-11-15 10:28:22 INFO  RackResolver:109 - Resolved nm to /default-rack
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2019-11-15 10:28:30,343 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2019-11-15 10:28:35,374 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start FollowerState
nm_1        | 2019-11-15 10:29:38 INFO  DefaultContainerExecutor:117 - Copying from /tmp/hadoop-hadoop/nm-local-dir/nmPrivate/container_1573813701275_0001_01_000001.tokens to /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000001.tokens
datanode_1  | 2019-11-15 10:28:30,335 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
rm_1        | 2019-11-15 10:28:22 INFO  ResourceTrackerService:345 - NodeManager from node nm(cmPort: 42530 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId nm:42530
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       | 2019-11-15 10:28:31,104 INFO pipeline.PipelineReportHandler: Pipeline ONE PipelineID=ad181c1b-8514-4aa5-a236-ede257eb84a1 reported by 875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
nm_1        | 2019-11-15 10:29:38 INFO  DefaultContainerExecutor:124 - Localizer CWD set to /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001 = file:/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001
datanode_1  | 2019-11-15 10:28:30,335 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:28:22 INFO  WebApps:288 - Registered webapp guice modules
datanode_2  | 2019-11-15 10:28:29,782 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2019-11-15 10:28:35,374 INFO impl.FollowerState: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
scm_1       | 2019-11-15 10:28:31,104 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ad181c1b-8514-4aa5-a236-ede257eb84a1, Nodes: 875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED] moved to OPEN state
nm_1        | 2019-11-15 10:29:38 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1  | 2019-11-15 10:28:30,336 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:28:22 INFO  HttpServer2:935 - Jetty bound to port 8088
datanode_2  | 2019-11-15 10:28:29,783 INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | WARNING: An illegal reflective access operation has occurred
nm_1        | 2019-11-15 10:29:39 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1  | 2019-11-15 10:28:30,336 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: start as a follower, conf=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
rm_1        | 2019-11-15 10:28:22 INFO  log:67 - jetty-6.1.26
scm_1       | 2019-11-15 10:28:31,105 INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
datanode_2  | 2019-11-15 10:28:29,783 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
nm_1        | 2019-11-15 10:29:39 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2019-11-15 10:28:30,337 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: changes role from      null to FOLLOWER at term 0 for startAsFollower
rm_1        | 2019-11-15 10:28:22 INFO  log:67 - Extract jar:file:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar!/webapps/cluster to /tmp/Jetty_rm_8088_cluster____oilmo9/webapp
scm_1       | 2019-11-15 10:28:31,105 INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_2  | 2019-11-15 10:28:29,783 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
nm_1        | 2019-11-15 10:29:39 INFO  MetricsSystemImpl:191 - XceiverClientMetrics metrics system started
datanode_1  | 2019-11-15 10:28:30,337 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start FollowerState
rm_1        | 2019-11-15 10:28:22 INFO  AbstractDelegationTokenSecretManager:326 - Updating the current master key for generating delegation tokens
scm_1       | 2019-11-15 10:28:31,160 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by 875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2019-11-15 10:28:29,784 INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
nm_1        | 2019-11-15 10:29:40 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.splitmetainfo(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
datanode_1  | 2019-11-15 10:28:30,338 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-16869120CD2F,id=90a6e036-396c-4a54-9c5c-b27150a4cad0
rm_1        | 2019-11-15 10:28:22 INFO  AbstractDelegationTokenSecretManager:644 - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
scm_1       | 2019-11-15 10:28:34,786 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2019-11-15 10:28:29,784 INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | WARNING: All illegal access operations will be denied in a future release
nm_1        | 2019-11-15 10:29:40 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.jar(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/filecache/11/job.jar) transitioned from DOWNLOADING to LOCALIZED
datanode_1  | 2019-11-15 10:28:30,338 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:28:22 INFO  AbstractDelegationTokenSecretManager:326 - Updating the current master key for generating delegation tokens
scm_1       | 2019-11-15 10:28:35,186 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2019-11-15 10:28:29,784 INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2019-11-15 10:28:35,539 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-16869120CD2F with new leaderId: b3b9007b-f7fa-4b53-9c52-6da6d084318c
nm_1        | 2019-11-15 10:29:40 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.split(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/filecache/12/job.split) transitioned from DOWNLOADING to LOCALIZED
datanode_1  | 2019-11-15 10:28:30,339 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS THREE #id: "38036a6e-5a45-44cd-bb65-16869120cd2f"
rm_1        | Nov 15, 2019 10:28:22 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
scm_1       | 2019-11-15 10:28:35,452 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=38036a6e-5a45-44cd-bb65-16869120cd2f reported by b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2019-11-15 10:28:29,784 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-15 10:28:35,539 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: change Leader from null to b3b9007b-f7fa-4b53-9c52-6da6d084318c at term 1 for appendEntries, leader elected after 4388ms
datanode_1  |  command succeed on datanode 90a6e036-396c-4a54-9c5c-b27150a4cad0.
rm_1        | INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
scm_1       | 2019-11-15 10:28:35,452 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 38036a6e-5a45-44cd-bb65-16869120cd2f, Nodes: 90a6e036-396c-4a54-9c5c-b27150a4cad0{ip: 172.18.0.7, host: hadoop27_datanode_1.hadoop27_default, networkLocation: /default-rack, certSerialId: null}875be6b7-c5fb-4532-80cc-37eaca7f756f{ip: 172.18.0.9, host: hadoop27_datanode_3.hadoop27_default, networkLocation: /default-rack, certSerialId: null}b3b9007b-f7fa-4b53-9c52-6da6d084318c{ip: 172.18.0.6, host: hadoop27_datanode_2.hadoop27_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED] moved to OPEN state
nm_1        | 2019-11-15 10:29:40 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0001/job.xml(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/filecache/13/job.xml) transitioned from DOWNLOADING to LOCALIZED
datanode_2  | 2019-11-15 10:28:29,784 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_3  | 2019-11-15 10:28:35,553 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F- FOLLOWER: Withhold vote from candidate 90a6e036-396c-4a54-9c5c-b27150a4cad0 with term 1. State: leader=b3b9007b-f7fa-4b53-9c52-6da6d084318c, term=1, lastRpcElapsed=9ms
datanode_1  | 2019-11-15 10:28:35,156 INFO impl.FollowerState: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-FollowerState: change to CANDIDATE, lastRpcTime:5012ms, electionTimeout:5012ms
rm_1        | Nov 15, 2019 10:28:22 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
scm_1       | 2019-11-15 10:28:35,452 INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
nm_1        | 2019-11-15 10:29:40 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000001 transitioned from LOCALIZING to LOCALIZED
datanode_2  | 2019-11-15 10:28:29,784 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: start as a follower, conf=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
datanode_1  | 2019-11-15 10:28:35,158 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: shutdown FollowerState
datanode_3  | 2019-11-15 10:28:35,563 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F: set configuration 0: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null at 0
rm_1        | INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
scm_1       | 2019-11-15 10:28:35,452 INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
nm_1        | 2019-11-15 10:29:41 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000001 transitioned from LOCALIZED to RUNNING
datanode_2  | 2019-11-15 10:28:29,785 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2019-11-15 10:28:35,158 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2019-11-15 10:28:35,568 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F-SegmentedRaftLogWorker: Starting segment from index:0
rm_1        | Nov 15, 2019 10:28:22 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
scm_1       | 2019-11-15 10:28:35,453 INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
nm_1        | 2019-11-15 10:29:41 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
datanode_2  | 2019-11-15 10:28:29,785 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start FollowerState
datanode_1  | 2019-11-15 10:28:35,162 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start LeaderElection
datanode_3  | 2019-11-15 10:28:35,667 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-16869120CD2F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/current/log_inprogress_0
rm_1        | INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
scm_1       | 2019-11-15 10:28:35,453 INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
nm_1        | 2019-11-15 10:29:41 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000001/default_container_executor.sh]
datanode_2  | 2019-11-15 10:28:29,785 INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-16869120CD2F,id=b3b9007b-f7fa-4b53-9c52-6da6d084318c
datanode_1  | 2019-11-15 10:28:35,179 INFO impl.LeaderElection: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-LeaderElection1: begin an election at term 1 for -1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858], old=null
datanode_3  | 2019-11-15 10:28:36,173 INFO impl.FollowerState: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-FollowerState: change to CANDIDATE, lastRpcTime:5179ms, electionTimeout:5178ms
rm_1        | Nov 15, 2019 10:28:22 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
scm_1       | 2019-11-15 10:28:35,453 INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
nm_1        | 2019-11-15 10:29:56 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
datanode_2  | 2019-11-15 10:28:29,785 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
datanode_1  | 2019-11-15 10:28:35,180 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: shutdown LeaderElection
datanode_3  | 2019-11-15 10:28:36,174 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: shutdown FollowerState
rm_1        | INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
scm_1       | 2019-11-15 10:30:29,813 INFO server.SCMBlockProtocolServer: SCM is informed by OM to delete 11 blocks
nm_1        | 2019-11-15 10:29:56 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0001_01_000002 by user hadoop
datanode_2  | 2019-11-15 10:28:29,786 INFO commandhandler.CreatePipelineCommandHandler: Create Pipeline RATIS THREE #id: "38036a6e-5a45-44cd-bb65-16869120cd2f"
datanode_1  | 2019-11-15 10:28:35,181 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2019-11-15 10:28:36,174 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
rm_1        | Nov 15, 2019 10:28:22 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
scm_1       | 2019-11-15 10:30:29,814 INFO block.BlockManagerImpl: Deleting blocks conID: 3 locID: 103141459394035717 bcsId: 0
datanode_2  |  command succeed on datanode b3b9007b-f7fa-4b53-9c52-6da6d084318c.
nm_1        | 2019-11-15 10:29:56 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000002
datanode_1  | 2019-11-15 10:28:35,181 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7A5565A46256 with new leaderId: 90a6e036-396c-4a54-9c5c-b27150a4cad0
datanode_3  | 2019-11-15 10:28:36,178 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start LeaderElection
rm_1        | INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
scm_1       | 2019-11-15 10:30:29,821 INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 103141459411795974 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,753 INFO impl.FollowerState: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-FollowerState: change to CANDIDATE, lastRpcTime:5171ms, electionTimeout:5170ms
nm_1        | 2019-11-15 10:29:56 INFO  ApplicationImpl:304 - Adding container_1573813701275_0001_01_000002 to application application_1573813701275_0001
datanode_3  | 2019-11-15 10:28:36,194 INFO impl.LeaderElection: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-LeaderElection1: begin an election at term 1 for -1: [875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858], old=null
rm_1        | Nov 15, 2019 10:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
scm_1       | 2019-11-15 10:30:29,821 INFO block.BlockManagerImpl: Deleting blocks conID: 2 locID: 103141459418284039 bcsId: 0
nm_1        | 2019-11-15 10:29:56 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from NEW to LOCALIZING
rm_1        | INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
datanode_1  | 2019-11-15 10:28:35,182 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: change Leader from null to 90a6e036-396c-4a54-9c5c-b27150a4cad0 at term 1 for becomeLeader, leader elected after 5156ms
datanode_3  | 2019-11-15 10:28:36,195 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: shutdown LeaderElection
scm_1       | 2019-11-15 10:30:29,822 INFO block.BlockManagerImpl: Deleting blocks conID: 3 locID: 103141459426541576 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,755 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: shutdown FollowerState
nm_1        | 2019-11-15 10:29:56 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0001
rm_1        | Nov 15, 2019 10:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
datanode_1  | 2019-11-15 10:28:35,188 INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2019-11-15 10:28:36,196 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1       | 2019-11-15 10:30:29,822 INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 103141460785102857 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,756 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
nm_1        | 2019-11-15 10:29:56 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0001
rm_1        | INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
datanode_1  | 2019-11-15 10:28:35,189 INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2019-11-15 10:28:36,196 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EDE257EB84A1 with new leaderId: 875be6b7-c5fb-4532-80cc-37eaca7f756f
scm_1       | 2019-11-15 10:30:29,822 INFO block.BlockManagerImpl: Deleting blocks conID: 2 locID: 103141460797161482 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,760 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start LeaderElection
nm_1        | 2019-11-15 10:29:56 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
rm_1        | 2019-11-15 10:28:23 INFO  log:67 - Started HttpServer2$SelectChannelConnectorWithSafeStartup@rm:8088
datanode_1  | 2019-11-15 10:28:35,193 INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
datanode_3  | 2019-11-15 10:28:36,197 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: change Leader from null to 875be6b7-c5fb-4532-80cc-37eaca7f756f at term 1 for becomeLeader, leader elected after 5355ms
scm_1       | 2019-11-15 10:30:29,822 INFO block.BlockManagerImpl: Deleting blocks conID: 3 locID: 103141459132153858 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,777 INFO impl.LeaderElection: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-LeaderElection1: begin an election at term 1 for -1: [b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
nm_1        | 2019-11-15 10:29:56 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0001
rm_1        | 2019-11-15 10:28:23 INFO  WebApps:307 - Web app cluster started at 8088
datanode_1  | 2019-11-15 10:28:35,200 INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
datanode_3  | 2019-11-15 10:28:36,200 INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1       | 2019-11-15 10:30:29,822 INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 103141459258572803 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,779 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: shutdown LeaderElection
nm_1        | 2019-11-15 10:29:56 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from LOCALIZING to LOCALIZED
datanode_1  | 2019-11-15 10:28:35,200 INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2019-11-15 10:28:36,200 INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1       | 2019-11-15 10:30:29,823 INFO block.BlockManagerImpl: Deleting blocks conID: 2 locID: 103141459267878916 bcsId: 0
datanode_2  | 2019-11-15 10:28:34,779 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
rm_1        | 2019-11-15 10:28:23 INFO  RMNodeImpl:441 - nm:42530 Node Transitioned from NEW to RUNNING
nm_1        | 2019-11-15 10:29:57 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from LOCALIZED to RUNNING
datanode_1  | 2019-11-15 10:28:35,201 INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2019-11-15 10:28:36,206 INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
datanode_2  | 2019-11-15 10:28:34,780 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A8CA3ED0F381 with new leaderId: b3b9007b-f7fa-4b53-9c52-6da6d084318c
scm_1       | 2019-11-15 10:30:29,823 INFO block.BlockManagerImpl: Deleting blocks conID: 3 locID: 103141462327885835 bcsId: 0
rm_1        | 2019-11-15 10:28:23 INFO  CapacityScheduler:1353 - Added node nm:42530 clusterResource: <memory:8192, vCores:8>
nm_1        | 2019-11-15 10:29:57 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
datanode_1  | 2019-11-15 10:28:35,211 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start LeaderState
datanode_3  | 2019-11-15 10:28:36,215 INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
datanode_2  | 2019-11-15 10:28:34,782 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: change Leader from null to b3b9007b-f7fa-4b53-9c52-6da6d084318c at term 1 for becomeLeader, leader elected after 5361ms
scm_1       | 2019-11-15 10:30:29,823 INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 103141462415245324 bcsId: 0
nm_1        | 2019-11-15 10:29:57 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000002/default_container_executor.sh]
datanode_1  | 2019-11-15 10:28:35,233 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-SegmentedRaftLogWorker: Starting segment from index:0
rm_1        | 2019-11-15 10:28:23 INFO  CallQueueManager:57 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100
datanode_3  | 2019-11-15 10:28:36,215 INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2019-11-15 10:28:34,787 INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
nm_1        | 2019-11-15 10:30:01 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
datanode_1  | WARNING: An illegal reflective access operation has occurred
datanode_3  | 2019-11-15 10:28:36,216 INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
rm_1        | 2019-11-15 10:28:23 INFO  Server:722 - Starting Socket Reader #1 for port 8033
datanode_2  | 2019-11-15 10:28:34,788 INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
nm_1        | 2019-11-15 10:30:01 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0001_01_000002
datanode_1  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
rm_1        | 2019-11-15 10:28:23 INFO  RpcServerFactoryPBImpl:174 - Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
datanode_3  | 2019-11-15 10:28:36,229 INFO impl.RoleInfo: 875be6b7-c5fb-4532-80cc-37eaca7f756f: start LeaderState
datanode_2  | 2019-11-15 10:28:34,791 INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
nm_1        | 2019-11-15 10:30:01 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000002
datanode_1  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
rm_1        | 2019-11-15 10:28:23 INFO  Server:962 - IPC Server Responder: starting
datanode_2  | 2019-11-15 10:28:34,797 INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
nm_1        | 2019-11-15 10:30:01 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from RUNNING to KILLING
datanode_3  | 2019-11-15 10:28:36,240 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
rm_1        | 2019-11-15 10:28:23 INFO  Server:801 - IPC Server listener on 8033: starting
datanode_2  | 2019-11-15 10:28:34,797 INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
nm_1        | 2019-11-15 10:30:01 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0001_01_000002
datanode_1  | WARNING: All illegal access operations will be denied in a future release
rm_1        | 2019-11-15 10:29:32 INFO  ClientRMService:289 - Allocated new applicationId: 1
datanode_3  | 2019-11-15 10:28:36,241 INFO impl.RaftServerImpl: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1: set configuration 0: [875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858], old=null at 0
datanode_2  | 2019-11-15 10:28:34,798 INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
nm_1        | 2019-11-15 10:30:01 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0001_01_000002 is : 143
datanode_1  | 2019-11-15 10:28:35,254 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256: set configuration 0: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858], old=null at 0
rm_1        | 2019-11-15 10:29:37 INFO  ClientRMService:583 - Application with id 1 submitted by user hadoop
datanode_2  | 2019-11-15 10:28:34,806 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start LeaderState
nm_1        | 2019-11-15 10:30:01 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
datanode_1  | 2019-11-15 10:28:35,361 INFO impl.FollowerState: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-FollowerState: change to CANDIDATE, lastRpcTime:5024ms, electionTimeout:5019ms
rm_1        | 2019-11-15 10:29:37 INFO  RMAppImpl:977 - Storing application with id application_1573813701275_0001
datanode_2  | 2019-11-15 10:28:34,809 INFO impl.FollowerState: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-FollowerState: change to CANDIDATE, lastRpcTime:5024ms, electionTimeout:5024ms
nm_1        | 2019-11-15 10:30:01 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000002
datanode_1  | 2019-11-15 10:28:35,362 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: shutdown FollowerState
rm_1        | 2019-11-15 10:29:37 INFO  RMAuditLogger:158 - USER=hadoop	IP=172.18.0.8	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1573813701275_0001
datanode_3  | 2019-11-15 10:28:36,301 INFO segmented.SegmentedRaftLogWorker: 875be6b7-c5fb-4532-80cc-37eaca7f756f@group-EDE257EB84A1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ad181c1b-8514-4aa5-a236-ede257eb84a1/current/log_inprogress_0
datanode_2  | 2019-11-15 10:28:34,810 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: shutdown FollowerState
nm_1        | 2019-11-15 10:30:01 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
datanode_1  | 2019-11-15 10:28:35,362 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
rm_1        | 2019-11-15 10:29:37 INFO  RMStateStore:194 - Storing info for app: application_1573813701275_0001
datanode_2  | 2019-11-15 10:28:34,810 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2019-11-15 10:28:35,362 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start LeaderElection
nm_1        | 2019-11-15 10:30:01 INFO  ApplicationImpl:347 - Removing container_1573813701275_0001_01_000002 from application application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from NEW to NEW_SAVING on event=START
datanode_2  | 2019-11-15 10:28:34,811 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start LeaderElection
datanode_1  | 2019-11-15 10:28:35,378 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-7A5565A46256-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/794f1888-dc9b-4333-bb31-7a5565a46256/current/log_inprogress_0
nm_1        | 2019-11-15 10:30:01 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:29:37 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from NEW_SAVING to SUBMITTED on event=APP_NEW_SAVED
datanode_2  | 2019-11-15 10:28:34,827 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2019-11-15 10:28:35,378 INFO impl.LeaderElection: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-LeaderElection2: begin an election at term 1 for -1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
nm_1        | 2019-11-15 10:30:01 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 INFO  ParentQueue:345 - Application added - appId: application_1573813701275_0001 user: hadoop leaf-queue of parent: root #applications: 1
datanode_2  | 2019-11-15 10:28:34,836 INFO impl.LeaderElection: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-LeaderElection2: begin an election at term 1 for -1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
nm_1        | 2019-11-15 10:30:02 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:29:37 INFO  CapacityScheduler:814 - Accepted application application_1573813701275_0001 from user: hadoop, in queue: default
datanode_2  | WARNING: An illegal reflective access operation has occurred
nm_1        | 2019-11-15 10:30:02 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0001_01_000003 by user hadoop
datanode_1  | 2019-11-15 10:28:35,536 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: changes role from CANDIDATE to FOLLOWER at term 1 for appendEntries
rm_1        | 2019-11-15 10:29:37 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from SUBMITTED to ACCEPTED on event=APP_ACCEPTED
datanode_2  | WARNING: Illegal reflective access by org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil (file:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.2.0.jar) to field java.nio.Buffer.address
nm_1        | 2019-11-15 10:30:02 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000003
datanode_1  | 2019-11-15 10:28:35,536 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: shutdown LeaderElection
rm_1        | 2019-11-15 10:29:37 INFO  ApplicationMasterService:678 - Registering app attempt : appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:02 INFO  ApplicationImpl:304 - Adding container_1573813701275_0001_01_000003 to application application_1573813701275_0001
datanode_2  | WARNING: Please consider reporting this to the maintainers of org.apache.ratis.thirdparty.com.google.protobuf.UnsafeUtil
datanode_1  | 2019-11-15 10:28:35,536 INFO impl.RoleInfo: 90a6e036-396c-4a54-9c5c-b27150a4cad0: start FollowerState
nm_1        | 2019-11-15 10:30:02 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from NEW to LOCALIZING
rm_1        | 2019-11-15 10:29:37 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from NEW to SUBMITTED
datanode_2  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_1  | 2019-11-15 10:28:35,536 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-16869120CD2F with new leaderId: b3b9007b-f7fa-4b53-9c52-6da6d084318c
nm_1        | 2019-11-15 10:30:02 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 WARN  LeafQueue:632 - maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
datanode_2  | WARNING: All illegal access operations will be denied in a future release
nm_1        | 2019-11-15 10:30:02 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 WARN  LeafQueue:653 - maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
datanode_2  | 2019-11-15 10:28:34,852 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381: set configuration 0: [b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null at 0
rm_1        | 2019-11-15 10:29:37 INFO  LeafQueue:667 - Application application_1573813701275_0001 from user: hadoop activated in queue: default
nm_1        | 2019-11-15 10:30:02 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
datanode_1  | 2019-11-15 10:28:35,537 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: change Leader from null to b3b9007b-f7fa-4b53-9c52-6da6d084318c at term 1 for appendEntries, leader elected after 5210ms
rm_1        | 2019-11-15 10:29:37 INFO  LeafQueue:683 - Application added - appId: application_1573813701275_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@241c118a, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
nm_1        | 2019-11-15 10:30:02 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0001
datanode_1  | 2019-11-15 10:28:35,559 INFO impl.RaftServerImpl: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F: set configuration 0: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null at 0
datanode_2  | 2019-11-15 10:28:34,992 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-A8CA3ED0F381-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ea248aff-1952-46bf-9756-a8ca3ed0f381/current/log_inprogress_0
nm_1        | 2019-11-15 10:30:02 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:29:37 INFO  CapacityScheduler:843 - Added Application Attempt appattempt_1573813701275_0001_000001 to scheduler from user hadoop in queue default
datanode_1  | 2019-11-15 10:28:35,559 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-SegmentedRaftLogWorker: Starting segment from index:0
nm_1        | 2019-11-15 10:30:02 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from LOCALIZED to RUNNING
datanode_2  | 2019-11-15 10:28:35,445 INFO impl.LeaderElection: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-LeaderElection2: Election PASSED; received 2 response(s) [b3b9007b-f7fa-4b53-9c52-6da6d084318c<-90a6e036-396c-4a54-9c5c-b27150a4cad0#0:FAIL-t1, b3b9007b-f7fa-4b53-9c52-6da6d084318c<-875be6b7-c5fb-4532-80cc-37eaca7f756f#0:OK-t1] and 0 exception(s); b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F:t1, leader=null, voted=b3b9007b-f7fa-4b53-9c52-6da6d084318c, raftlog=b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
rm_1        | 2019-11-15 10:29:37 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from SUBMITTED to SCHEDULED
datanode_1  | 2019-11-15 10:28:35,589 INFO impl.LeaderElection: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-LeaderElection2: Election REJECTED; received 1 response(s) [90a6e036-396c-4a54-9c5c-b27150a4cad0<-875be6b7-c5fb-4532-80cc-37eaca7f756f#0:FAIL-t1] and 0 exception(s); 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F:t1, leader=b3b9007b-f7fa-4b53-9c52-6da6d084318c, voted=90a6e036-396c-4a54-9c5c-b27150a4cad0, raftlog=90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=0: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null
nm_1        | 2019-11-15 10:30:02 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:29:37 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000001 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:02 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000003/default_container_executor.sh]
datanode_1  | 2019-11-15 10:28:35,598 INFO segmented.SegmentedRaftLogWorker: 90a6e036-396c-4a54-9c5c-b27150a4cad0@group-16869120CD2F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/current/log_inprogress_0
datanode_2  | 2019-11-15 10:28:35,446 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: shutdown LeaderElection
rm_1        | 2019-11-15 10:29:37 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000001
nm_1        | 2019-11-15 10:30:03 INFO  NodeStatusUpdaterImpl:490 - Removed completed containers from NM context: [container_1573813701275_0001_01_000002]
datanode_2  | 2019-11-15 10:28:35,447 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
rm_1        | 2019-11-15 10:29:37 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0001_01_000001 of capacity <memory:2048, vCores:1> on host nm:42530, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
nm_1        | 2019-11-15 10:30:06 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:29:37 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0001_000001 container=Container: [ContainerId: container_1573813701275_0001_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH requestedPartition=
nm_1        | 2019-11-15 10:30:06 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0001_01_000003
datanode_2  | 2019-11-15 10:28:35,447 INFO ratis.XceiverServerRatis: Leader change notification received for group: group-16869120CD2F with new leaderId: b3b9007b-f7fa-4b53-9c52-6da6d084318c
rm_1        | 2019-11-15 10:29:37 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:06 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000003
datanode_2  | 2019-11-15 10:28:35,447 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: change Leader from null to b3b9007b-f7fa-4b53-9c52-6da6d084318c at term 1 for becomeLeader, leader elected after 5666ms
nm_1        | 2019-11-15 10:30:06 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from RUNNING to KILLING
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
rm_1        | 2019-11-15 10:29:37 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:06 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0001_01_000003
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
rm_1        | 2019-11-15 10:29:37 INFO  NMTokenSecretManagerInRM:200 - Sending NMToken for nodeId : nm:42530 for container : container_1573813701275_0001_01_000001
nm_1        | 2019-11-15 10:30:06 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0001_01_000003 is : 143
rm_1        | 2019-11-15 10:29:37 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
nm_1        | 2019-11-15 10:30:07 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
rm_1        | 2019-11-15 10:29:37 INFO  NMTokenSecretManagerInRM:146 - Clear node set for appattempt_1573813701275_0001_000001
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
rm_1        | 2019-11-15 10:29:37 INFO  RMAppAttemptImpl:1924 - Storing attempt: AppId: application_1573813701275_0001 AttemptId: appattempt_1573813701275_0001_000001 MasterContainer: Container: [ContainerId: container_1573813701275_0001_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ]
nm_1        | 2019-11-15 10:30:07 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000003
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
rm_1        | 2019-11-15 10:29:37 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
nm_1        | 2019-11-15 10:30:07 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
datanode_2  | 2019-11-15 10:28:35,448 INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
nm_1        | 2019-11-15 10:30:07 INFO  ApplicationImpl:347 - Removing container_1573813701275_0001_01_000003 from application application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
datanode_2  | 2019-11-15 10:28:35,452 INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
nm_1        | 2019-11-15 10:30:07 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:29:37 INFO  AMLauncher:249 - Launching masterappattempt_1573813701275_0001_000001
datanode_2  | 2019-11-15 10:28:35,452 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
nm_1        | 2019-11-15 10:30:07 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:29:37 INFO  AMLauncher:105 - Setting up container Container: [ContainerId: container_1573813701275_0001_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] for AM appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:08 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
datanode_2  | 2019-11-15 10:28:35,453 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
rm_1        | 2019-11-15 10:29:37 INFO  AMLauncher:187 - Command to launch container container_1573813701275_0001_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
nm_1        | 2019-11-15 10:30:08 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0001_01_000004 by user hadoop
datanode_2  | 2019-11-15 10:28:35,456 INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
rm_1        | 2019-11-15 10:29:37 INFO  AMRMTokenSecretManager:195 - Create AMRMToken for ApplicationAttempt: appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:08 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000004
datanode_2  | 2019-11-15 10:28:35,457 INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
rm_1        | 2019-11-15 10:29:37 INFO  AMRMTokenSecretManager:307 - Creating password for appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:08 INFO  ApplicationImpl:304 - Adding container_1573813701275_0001_01_000004 to application application_1573813701275_0001
datanode_2  | 2019-11-15 10:28:35,457 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
rm_1        | 2019-11-15 10:29:38 INFO  AMLauncher:126 - Done launching container Container: [ContainerId: container_1573813701275_0001_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] for AM appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:08 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from NEW to LOCALIZING
datanode_2  | 2019-11-15 10:28:35,458 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReportRegistration(...) before.
rm_1        | 2019-11-15 10:29:38 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from ALLOCATED to LAUNCHED
nm_1        | 2019-11-15 10:30:08 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0001
datanode_2  | 2019-11-15 10:28:35,459 INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
nm_1        | 2019-11-15 10:30:08 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:29:38 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
datanode_2  | 2019-11-15 10:28:35,460 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
nm_1        | 2019-11-15 10:30:08 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
rm_1        | 2019-11-15 10:29:53 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
datanode_2  | 2019-11-15 10:28:35,460 INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
rm_1        | 2019-11-15 10:29:53 INFO  ApplicationMasterService:274 - AM registration appattempt_1573813701275_0001_000001
datanode_2  | 2019-11-15 10:28:35,460 INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
nm_1        | 2019-11-15 10:30:08 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0001
rm_1        | 2019-11-15 10:29:53 INFO  RMAuditLogger:137 - USER=hadoop	IP=172.18.0.2	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1573813701275_0001	APPATTEMPTID=appattempt_1573813701275_0001_000001
datanode_2  | 2019-11-15 10:28:35,460 INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
nm_1        | 2019-11-15 10:30:08 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:29:53 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from LAUNCHED to RUNNING
datanode_2  | 2019-11-15 10:28:35,460 INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
nm_1        | 2019-11-15 10:30:08 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from LOCALIZED to RUNNING
rm_1        | 2019-11-15 10:29:53 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from ACCEPTED to RUNNING on event=ATTEMPT_REGISTERED
datanode_2  | 2019-11-15 10:28:35,463 INFO impl.RoleInfo: b3b9007b-f7fa-4b53-9c52-6da6d084318c: start LeaderState
nm_1        | 2019-11-15 10:30:08 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:29:56 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000002 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:08 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000004/default_container_executor.sh]
datanode_2  | 2019-11-15 10:28:35,464 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-SegmentedRaftLogWorker: Starting segment from index:0
rm_1        | 2019-11-15 10:29:56 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000002
nm_1        | 2019-11-15 10:30:09 INFO  NodeStatusUpdaterImpl:490 - Removed completed containers from NM context: [container_1573813701275_0001_01_000003]
datanode_2  | 2019-11-15 10:28:35,466 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F: set configuration 0: [90a6e036-396c-4a54-9c5c-b27150a4cad0:172.18.0.7:9858, 875be6b7-c5fb-4532-80cc-37eaca7f756f:172.18.0.9:9858, b3b9007b-f7fa-4b53-9c52-6da6d084318c:172.18.0.6:9858], old=null at 0
rm_1        | 2019-11-15 10:29:56 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0001_01_000002 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
nm_1        | 2019-11-15 10:30:13 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:29:56 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0001_000001 container=Container: [ContainerId: container_1573813701275_0001_01_000002, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=RACK_LOCAL requestedPartition=
datanode_2  | 2019-11-15 10:28:35,525 INFO segmented.SegmentedRaftLogWorker: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/38036a6e-5a45-44cd-bb65-16869120cd2f/current/log_inprogress_0
nm_1        | 2019-11-15 10:30:13 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0001_01_000004
rm_1        | 2019-11-15 10:29:56 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
datanode_2  | 2019-11-15 10:28:35,600 INFO impl.RaftServerImpl: b3b9007b-f7fa-4b53-9c52-6da6d084318c@group-16869120CD2F-   LEADER: Withhold vote from candidate 90a6e036-396c-4a54-9c5c-b27150a4cad0 with term 1. State: leader=b3b9007b-f7fa-4b53-9c52-6da6d084318c, term=1, lastRpcElapsed=null
nm_1        | 2019-11-15 10:30:13 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000004
rm_1        | 2019-11-15 10:29:56 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
datanode_2  | 2019-11-15 10:29:04,326 WARN client.GrpcClientProtocolService: 0-OrderedRequestStreamObserver0: onError: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: cancelled before receiving half close
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from RUNNING to KILLING
rm_1        | 2019-11-15 10:29:56 INFO  NMTokenSecretManagerInRM:200 - Sending NMToken for nodeId : nm:42530 for container : container_1573813701275_0001_01_000002
nm_1        | 2019-11-15 10:30:13 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0001_01_000004
rm_1        | 2019-11-15 10:29:56 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
nm_1        | 2019-11-15 10:30:13 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0001_01_000004 is : 143
rm_1        | 2019-11-15 10:29:57 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
rm_1        | 2019-11-15 10:30:01 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
nm_1        | 2019-11-15 10:30:13 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000004
rm_1        | 2019-11-15 10:30:01 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0001_01_000002 in state: COMPLETED event:FINISHED
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000004 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
rm_1        | 2019-11-15 10:30:01 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000002
nm_1        | 2019-11-15 10:30:13 INFO  ApplicationImpl:347 - Removing container_1573813701275_0001_01_000004 from application application_1573813701275_0001
rm_1        | 2019-11-15 10:30:01 INFO  SchedulerNode:216 - Released container container_1573813701275_0001_01_000002 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
rm_1        | 2019-11-15 10:30:01 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
rm_1        | 2019-11-15 10:30:01 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0001_01_000002, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:01 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:01 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:13 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:01 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0001_000001 released container container_1573813701275_0001_01_000002 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
nm_1        | 2019-11-15 10:30:13 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:30:02 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000003 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:13 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:02 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000003
rm_1        | 2019-11-15 10:30:02 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0001_01_000003 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
rm_1        | 2019-11-15 10:30:02 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0001_000001 container=Container: [ContainerId: container_1573813701275_0001_01_000003, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=RACK_LOCAL requestedPartition=
rm_1        | 2019-11-15 10:30:02 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
rm_1        | 2019-11-15 10:30:02 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:02 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
rm_1        | 2019-11-15 10:30:03 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
rm_1        | 2019-11-15 10:30:07 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
rm_1        | 2019-11-15 10:30:07 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0001_01_000003 in state: COMPLETED event:FINISHED
nm_1        | 2019-11-15 10:30:13 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0001_01_000005 by user hadoop
rm_1        | 2019-11-15 10:30:07 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000003
nm_1        | 2019-11-15 10:30:13 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000005
rm_1        | 2019-11-15 10:30:07 INFO  SchedulerNode:216 - Released container container_1573813701275_0001_01_000003 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
nm_1        | 2019-11-15 10:30:13 INFO  ApplicationImpl:304 - Adding container_1573813701275_0001_01_000005 to application application_1573813701275_0001
rm_1        | 2019-11-15 10:30:07 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from NEW to LOCALIZING
nm_1        | 2019-11-15 10:30:13 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0001
nm_1        | 2019-11-15 10:30:13 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0001
nm_1        | 2019-11-15 10:30:13 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
rm_1        | 2019-11-15 10:30:07 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0001_01_000003, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:13 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0001
rm_1        | 2019-11-15 10:30:07 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:30:07 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:13 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from LOCALIZED to RUNNING
rm_1        | 2019-11-15 10:30:07 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0001_000001 released container container_1573813701275_0001_01_000003 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
nm_1        | 2019-11-15 10:30:13 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:08 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000004 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:13 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0001/container_1573813701275_0001_01_000005/default_container_executor.sh]
rm_1        | 2019-11-15 10:30:08 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000004
nm_1        | 2019-11-15 10:30:15 INFO  NodeStatusUpdaterImpl:490 - Removed completed containers from NM context: [container_1573813701275_0001_01_000004]
rm_1        | 2019-11-15 10:30:08 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0001_01_000004 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
nm_1        | 2019-11-15 10:30:17 INFO  ShuffleHandler:1053 - Setting connection close header...
rm_1        | 2019-11-15 10:30:08 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0001_000001 container=Container: [ContainerId: container_1573813701275_0001_01_000004, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=RACK_LOCAL requestedPartition=
nm_1        | 2019-11-15 10:30:17 INFO  ShuffleHandler:1053 - Setting connection close header...
rm_1        | 2019-11-15 10:30:08 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
nm_1        | 2019-11-15 10:30:23 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
nm_1        | 2019-11-15 10:30:23 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0001_01_000005
rm_1        | 2019-11-15 10:30:08 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:23 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000005
nm_1        | 2019-11-15 10:30:23 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from RUNNING to KILLING
rm_1        | 2019-11-15 10:30:08 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
nm_1        | 2019-11-15 10:30:23 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0001_01_000005
rm_1        | 2019-11-15 10:30:09 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000004 Container Transitioned from ACQUIRED to RUNNING
nm_1        | 2019-11-15 10:30:23 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0001_01_000005 is : 143
rm_1        | 2019-11-15 10:30:09 INFO  AppSchedulingInfo:204 - checking for deactivate of application :application_1573813701275_0001
nm_1        | 2019-11-15 10:30:24 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
nm_1        | 2019-11-15 10:30:24 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000005
rm_1        | 2019-11-15 10:30:13 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000004 Container Transitioned from RUNNING to COMPLETED
nm_1        | 2019-11-15 10:30:24 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000005 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
nm_1        | 2019-11-15 10:30:24 INFO  ApplicationImpl:347 - Removing container_1573813701275_0001_01_000005 from application application_1573813701275_0001
rm_1        | 2019-11-15 10:30:13 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0001_01_000004 in state: COMPLETED event:FINISHED
nm_1        | 2019-11-15 10:30:24 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
nm_1        | 2019-11-15 10:30:24 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:30:13 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000004
nm_1        | 2019-11-15 10:30:31 INFO  ContainerLaunch:346 - Container container_1573813701275_0001_01_000001 succeeded 
rm_1        | 2019-11-15 10:30:13 INFO  SchedulerNode:216 - Released container container_1573813701275_0001_01_000004 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
nm_1        | 2019-11-15 10:30:31 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
rm_1        | 2019-11-15 10:30:13 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
nm_1        | 2019-11-15 10:30:31 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0001_01_000001
rm_1        | 2019-11-15 10:30:13 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0001_01_000004, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:31 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000001
rm_1        | 2019-11-15 10:30:13 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:31 INFO  ContainerImpl:1135 - Container container_1573813701275_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
rm_1        | 2019-11-15 10:30:13 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:31 INFO  ApplicationImpl:347 - Removing container_1573813701275_0001_01_000001 from application application_1573813701275_0001
nm_1        | 2019-11-15 10:30:31 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:13 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0001_000001 released container container_1573813701275_0001_01_000004 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
nm_1        | 2019-11-15 10:30:31 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:30:13 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000005 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:31 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0001_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:13 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000005
nm_1        | 2019-11-15 10:30:31 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0001_01_000001
nm_1        | 2019-11-15 10:30:31 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.8	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000001
rm_1        | 2019-11-15 10:30:13 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0001_01_000005 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
nm_1        | 2019-11-15 10:30:32 INFO  NodeStatusUpdaterImpl:490 - Removed completed containers from NM context: [container_1573813701275_0001_01_000001, container_1573813701275_0001_01_000005]
rm_1        | 2019-11-15 10:30:13 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0001_000001 container=Container: [ContainerId: container_1573813701275_0001_01_000005, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH requestedPartition=
rm_1        | 2019-11-15 10:30:13 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
rm_1        | 2019-11-15 10:30:13 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:13 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000005 Container Transitioned from ALLOCATED to ACQUIRED
rm_1        | 2019-11-15 10:30:14 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000005 Container Transitioned from ACQUIRED to RUNNING
nm_1        | 2019-11-15 10:30:32 INFO  ApplicationImpl:464 - Application application_1573813701275_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
rm_1        | 2019-11-15 10:30:14 INFO  AppSchedulingInfo:204 - checking for deactivate of application :application_1573813701275_0001
nm_1        | 2019-11-15 10:30:32 INFO  AuxServices:196 - Got event APPLICATION_STOP for appId application_1573813701275_0001
rm_1        | 2019-11-15 10:30:24 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000005 Container Transitioned from RUNNING to COMPLETED
nm_1        | 2019-11-15 10:30:32 INFO  ApplicationImpl:464 - Application application_1573813701275_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
nm_1        | 2019-11-15 10:30:32 INFO  NonAggregatingLogHandler:167 - Scheduling Log Deletion for application: application_1573813701275_0001, with delay of 10800 seconds
rm_1        | 2019-11-15 10:30:24 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0001_01_000005 in state: COMPLETED event:FINISHED
nm_1        | 2019-11-15 10:30:34 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:24 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000005
nm_1        | 2019-11-15 10:30:34 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0002_01_000001 by user hadoop
rm_1        | 2019-11-15 10:30:24 INFO  SchedulerNode:216 - Released container container_1573813701275_0001_01_000005 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
nm_1        | 2019-11-15 10:30:34 INFO  ContainerManagerImpl:850 - Creating a new application reference for app application_1573813701275_0002
rm_1        | 2019-11-15 10:30:24 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
nm_1        | 2019-11-15 10:30:34 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.8	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000001
nm_1        | 2019-11-15 10:30:34 INFO  ApplicationImpl:464 - Application application_1573813701275_0002 transitioned from NEW to INITING
rm_1        | 2019-11-15 10:30:24 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0001_01_000005, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:34 INFO  ApplicationImpl:304 - Adding container_1573813701275_0002_01_000001 to application application_1573813701275_0002
rm_1        | 2019-11-15 10:30:24 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:34 INFO  ApplicationImpl:464 - Application application_1573813701275_0002 transitioned from INITING to RUNNING
rm_1        | 2019-11-15 10:30:24 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:34 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000001 transitioned from NEW to LOCALIZING
rm_1        | 2019-11-15 10:30:24 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0001_000001 released container container_1573813701275_0001_01_000005 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
nm_1        | 2019-11-15 10:30:34 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0002
rm_1        | 2019-11-15 10:30:25 INFO  RMAppAttemptImpl:1210 - Updating application attempt appattempt_1573813701275_0001_000001 with final state: FINISHING, and exit status: -1000
nm_1        | 2019-11-15 10:30:34 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.splitmetainfo transitioned from INIT to DOWNLOADING
rm_1        | 2019-11-15 10:30:25 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from RUNNING to FINAL_SAVING
nm_1        | 2019-11-15 10:30:34 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.jar transitioned from INIT to DOWNLOADING
rm_1        | 2019-11-15 10:30:25 INFO  RMAppImpl:996 - Updating application application_1573813701275_0001 with final state: FINISHING
nm_1        | 2019-11-15 10:30:34 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.split transitioned from INIT to DOWNLOADING
rm_1        | 2019-11-15 10:30:25 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from RUNNING to FINAL_SAVING on event=ATTEMPT_UNREGISTERED
nm_1        | 2019-11-15 10:30:34 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.xml transitioned from INIT to DOWNLOADING
rm_1        | 2019-11-15 10:30:25 INFO  RMStateStore:223 - Updating info for app: application_1573813701275_0001
rm_1        | 2019-11-15 10:30:25 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from FINAL_SAVING to FINISHING
nm_1        | 2019-11-15 10:30:34 INFO  ResourceLocalizationService:711 - Created localizer for container_1573813701275_0002_01_000001
rm_1        | 2019-11-15 10:30:25 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from FINAL_SAVING to FINISHING on event=APP_UPDATE_SAVED
rm_1        | 2019-11-15 10:30:26 INFO  ApplicationMasterService:354 - application_1573813701275_0001 unregistered successfully. 
rm_1        | 2019-11-15 10:30:31 INFO  ClientRMService:289 - Allocated new applicationId: 2
rm_1        | 2019-11-15 10:30:31 INFO  RMContainerImpl:410 - container_1573813701275_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
rm_1        | 2019-11-15 10:30:31 INFO  ApplicationMasterService:684 - Unregistering app attempt : appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:34 INFO  ResourceLocalizationService:1195 - Writing credentials to the nmPrivate file /tmp/hadoop-hadoop/nm-local-dir/nmPrivate/container_1573813701275_0002_01_000001.tokens. Credentials list: 
rm_1        | 2019-11-15 10:30:31 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0001_01_000001 in state: COMPLETED event:FINISHED
nm_1        | 2019-11-15 10:30:34 INFO  DefaultContainerExecutor:612 - Initializing user hadoop
nm_1        | 2019-11-15 10:30:34 INFO  DefaultContainerExecutor:117 - Copying from /tmp/hadoop-hadoop/nm-local-dir/nmPrivate/container_1573813701275_0002_01_000001.tokens to /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/container_1573813701275_0002_01_000001.tokens
rm_1        | 2019-11-15 10:30:31 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0001	CONTAINERID=container_1573813701275_0001_01_000001
nm_1        | 2019-11-15 10:30:34 INFO  DefaultContainerExecutor:124 - Localizer CWD set to /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002 = file:/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002
rm_1        | 2019-11-15 10:30:31 INFO  SchedulerNode:216 - Released container container_1573813701275_0001_01_000001 of capacity <memory:2048, vCores:1> on host nm:42530, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
nm_1        | 2019-11-15 10:30:35 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
rm_1        | 2019-11-15 10:30:31 INFO  LeafQueue:1712 - default used=<memory:0, vCores:0> numContainers=0 user=hadoop user-resources=<memory:0, vCores:0>
nm_1        | 2019-11-15 10:30:36 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
rm_1        | 2019-11-15 10:30:31 INFO  AMRMTokenSecretManager:124 - Application finished, removing password for appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:36 WARN  MBeans:105 - Failed to register MBean "Hadoop:service=XceiverClientMetrics,name=MetricsSystem,sub=Stats": Instance already exists.
rm_1        | 2019-11-15 10:30:31 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0001_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:36 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
rm_1        | 2019-11-15 10:30:31 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0001_000001 State change from FINISHING to FINISHED
rm_1        | 2019-11-15 10:30:31 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:36 INFO  MetricsSystemImpl:191 - XceiverClientMetrics metrics system started
rm_1        | 2019-11-15 10:30:31 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
nm_1        | 2019-11-15 10:30:36 WARN  MBeans:105 - Failed to register MBean "Hadoop:service=XceiverClientMetrics,name=UgiMetrics": Instance already exists.
rm_1        | 2019-11-15 10:30:31 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0001_000001 released container container_1573813701275_0001_01_000001 on node: host: nm:42530 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
nm_1        | 2019-11-15 10:30:36 WARN  MBeans:105 - Failed to register MBean "Hadoop:service=XceiverClientMetrics,name=MetricsSystem,sub=Control": Instance already exists.
nm_1        | 2019-11-15 10:30:37 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.splitmetainfo(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
rm_1        | 2019-11-15 10:30:31 INFO  RMAppImpl:718 - application_1573813701275_0001 State change from FINISHING to FINISHED on event=ATTEMPT_FINISHED
nm_1        | 2019-11-15 10:30:37 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.jar(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/filecache/11/job.jar) transitioned from DOWNLOADING to LOCALIZED
rm_1        | 2019-11-15 10:30:31 INFO  CapacityScheduler:882 - Application Attempt appattempt_1573813701275_0001_000001 is done. finalState=FINISHED
rm_1        | 2019-11-15 10:30:31 INFO  AppSchedulingInfo:115 - Application application_1573813701275_0001 requests cleared
rm_1        | 2019-11-15 10:30:31 INFO  AMLauncher:263 - Cleaning master appattempt_1573813701275_0001_000001
nm_1        | 2019-11-15 10:30:37 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.split(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/filecache/12/job.split) transitioned from DOWNLOADING to LOCALIZED
rm_1        | 2019-11-15 10:30:31 INFO  LeafQueue:729 - Application removed - appId: application_1573813701275_0001 user: hadoop queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
rm_1        | 2019-11-15 10:30:31 INFO  RMAuditLogger:158 - USER=hadoop	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1573813701275_0001
nm_1        | 2019-11-15 10:30:37 INFO  LocalizedResource:203 - Resource o3fs://bucket1.vol1/user/hadoop/.staging/job_1573813701275_0002/job.xml(->/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/filecache/13/job.xml) transitioned from DOWNLOADING to LOCALIZED
rm_1        | 2019-11-15 10:30:31 INFO  ParentQueue:370 - Application removed - appId: application_1573813701275_0001 user: hadoop leaf-queue of parent: root #applications: 0
nm_1        | 2019-11-15 10:30:37 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000001 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:30:31 INFO  RMAppManager$ApplicationSummary:186 - appId=application_1573813701275_0001,name=QuasiMonteCarlo,user=hadoop,queue=default,state=FINISHED,trackingUrl=http://rm:8088/proxy/application_1573813701275_0001/,appMasterHost=nm,startTime=1573813777045,finishTime=1573813825063,finalStatus=SUCCEEDED,memorySeconds=216571,vcoreSeconds=77,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
nm_1        | 2019-11-15 10:30:37 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000001 transitioned from LOCALIZED to RUNNING
rm_1        | 2019-11-15 10:30:34 INFO  ClientRMService:583 - Application with id 2 submitted by user hadoop
nm_1        | 2019-11-15 10:30:37 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:34 INFO  RMAppImpl:977 - Storing application with id application_1573813701275_0002
nm_1        | 2019-11-15 10:30:37 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/container_1573813701275_0002_01_000001/default_container_executor.sh]
rm_1        | 2019-11-15 10:30:34 INFO  RMAuditLogger:158 - USER=hadoop	IP=172.18.0.8	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1573813701275_0002
nm_1        | 2019-11-15 10:30:52 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:34 INFO  RMStateStore:194 - Storing info for app: application_1573813701275_0002
nm_1        | 2019-11-15 10:30:52 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0002_01_000002 by user hadoop
rm_1        | 2019-11-15 10:30:34 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from NEW to NEW_SAVING on event=START
nm_1        | 2019-11-15 10:30:52 INFO  ApplicationImpl:304 - Adding container_1573813701275_0002_01_000002 to application application_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from NEW_SAVING to SUBMITTED on event=APP_NEW_SAVED
nm_1        | 2019-11-15 10:30:52 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000002
rm_1        | 2019-11-15 10:30:34 INFO  ParentQueue:345 - Application added - appId: application_1573813701275_0002 user: hadoop leaf-queue of parent: root #applications: 1
nm_1        | 2019-11-15 10:30:52 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from NEW to LOCALIZING
rm_1        | 2019-11-15 10:30:34 INFO  CapacityScheduler:814 - Accepted application application_1573813701275_0002 from user: hadoop, in queue: default
nm_1        | 2019-11-15 10:30:52 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0002
nm_1        | 2019-11-15 10:30:52 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0002
nm_1        | 2019-11-15 10:30:52 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
rm_1        | 2019-11-15 10:30:34 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from SUBMITTED to ACCEPTED on event=APP_ACCEPTED
nm_1        | 2019-11-15 10:30:52 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  ApplicationMasterService:678 - Registering app attempt : appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:30:52 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from NEW to SUBMITTED
nm_1        | 2019-11-15 10:30:52 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from LOCALIZED to RUNNING
rm_1        | 2019-11-15 10:30:34 WARN  LeafQueue:632 - maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
nm_1        | 2019-11-15 10:30:52 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:34 WARN  LeafQueue:653 - maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
nm_1        | 2019-11-15 10:30:52 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/container_1573813701275_0002_01_000002/default_container_executor.sh]
rm_1        | 2019-11-15 10:30:34 INFO  LeafQueue:667 - Application application_1573813701275_0002 from user: hadoop activated in queue: default
nm_1        | 2019-11-15 10:30:57 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
nm_1        | 2019-11-15 10:30:57 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0002_01_000002
nm_1        | 2019-11-15 10:30:57 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000002
nm_1        | 2019-11-15 10:30:57 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from RUNNING to KILLING
nm_1        | 2019-11-15 10:30:57 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0002_01_000002
nm_1        | 2019-11-15 10:30:57 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0002_01_000002 is : 143
rm_1        | 2019-11-15 10:30:34 INFO  LeafQueue:683 - Application added - appId: application_1573813701275_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@5cae3a1b, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
nm_1        | 2019-11-15 10:30:57 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
rm_1        | 2019-11-15 10:30:34 INFO  CapacityScheduler:843 - Added Application Attempt appattempt_1573813701275_0002_000001 to scheduler from user hadoop in queue default
nm_1        | 2019-11-15 10:30:57 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000002
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from SUBMITTED to SCHEDULED
nm_1        | 2019-11-15 10:30:57 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
rm_1        | 2019-11-15 10:30:34 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000001 Container Transitioned from NEW to ALLOCATED
nm_1        | 2019-11-15 10:30:57 INFO  ApplicationImpl:347 - Removing container_1573813701275_0002_01_000002 from application application_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000001
nm_1        | 2019-11-15 10:30:57 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:34 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0002_01_000001 of capacity <memory:2048, vCores:1> on host nm:42530, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
nm_1        | 2019-11-15 10:30:57 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0002_000001 container=Container: [ContainerId: container_1573813701275_0002_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH requestedPartition=
nm_1        | 2019-11-15 10:30:59 INFO  NodeStatusUpdaterImpl:490 - Removed completed containers from NM context: [container_1573813701275_0002_01_000002]
rm_1        | 2019-11-15 10:30:34 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
nm_1        | 2019-11-15 10:30:59 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:34 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
nm_1        | 2019-11-15 10:30:59 INFO  ContainerManagerImpl:810 - Start request for container_1573813701275_0002_01_000003 by user hadoop
nm_1        | 2019-11-15 10:30:59 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000003
nm_1        | 2019-11-15 10:30:59 INFO  ApplicationImpl:304 - Adding container_1573813701275_0002_01_000003 to application application_1573813701275_0002
nm_1        | 2019-11-15 10:30:59 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from NEW to LOCALIZING
rm_1        | 2019-11-15 10:30:34 INFO  NMTokenSecretManagerInRM:200 - Sending NMToken for nodeId : nm:42530 for container : container_1573813701275_0002_01_000001
nm_1        | 2019-11-15 10:30:59 INFO  AuxServices:196 - Got event CONTAINER_INIT for appId application_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
nm_1        | 2019-11-15 10:30:59 INFO  AuxServices:196 - Got event APPLICATION_INIT for appId application_1573813701275_0002
nm_1        | 2019-11-15 10:30:59 INFO  AuxServices:200 - Got APPLICATION_INIT for service mapreduce_shuffle
nm_1        | 2019-11-15 10:30:59 INFO  ShuffleHandler:677 - Added token for job_1573813701275_0002
rm_1        | 2019-11-15 10:30:34 INFO  NMTokenSecretManagerInRM:146 - Clear node set for appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:30:59 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from LOCALIZING to LOCALIZED
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:1924 - Storing attempt: AppId: application_1573813701275_0002 AttemptId: appattempt_1573813701275_0002_000001 MasterContainer: Container: [ContainerId: container_1573813701275_0002_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ]
nm_1        | 2019-11-15 10:30:59 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from LOCALIZED to RUNNING
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED
nm_1        | 2019-11-15 10:30:59 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:34 INFO  AMLauncher:249 - Launching masterappattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:30:59 INFO  DefaultContainerExecutor:268 - launchContainer: [bash, /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1573813701275_0002/container_1573813701275_0002_01_000003/default_container_executor.sh]
rm_1        | 2019-11-15 10:30:34 INFO  AMLauncher:105 - Setting up container Container: [ContainerId: container_1573813701275_0002_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] for AM appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:31:02 INFO  ShuffleHandler:1053 - Setting connection close header...
rm_1        | 2019-11-15 10:30:34 INFO  AMLauncher:187 - Command to launch container container_1573813701275_0002_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
nm_1        | 2019-11-15 10:31:04 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
rm_1        | 2019-11-15 10:30:34 INFO  AMRMTokenSecretManager:195 - Create AMRMToken for ApplicationAttempt: appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:31:04 INFO  ContainerManagerImpl:960 - Stopping container with container Id: container_1573813701275_0002_01_000003
nm_1        | 2019-11-15 10:31:04 INFO  NMAuditLogger:89 - USER=hadoop	IP=172.18.0.2	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000003
rm_1        | 2019-11-15 10:30:34 INFO  AMRMTokenSecretManager:307 - Creating password for appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:31:04 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from RUNNING to KILLING
rm_1        | 2019-11-15 10:30:34 INFO  AMLauncher:126 - Done launching container Container: [ContainerId: container_1573813701275_0002_01_000001, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] for AM appattempt_1573813701275_0002_000001
rm_1        | 2019-11-15 10:30:34 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from ALLOCATED to LAUNCHED
nm_1        | 2019-11-15 10:31:04 INFO  ContainerLaunch:370 - Cleaning up container container_1573813701275_0002_01_000003
rm_1        | 2019-11-15 10:30:35 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000001 Container Transitioned from ACQUIRED to RUNNING
nm_1        | 2019-11-15 10:31:04 WARN  DefaultContainerExecutor:224 - Exit code from container container_1573813701275_0002_01_000003 is : 143
rm_1        | 2019-11-15 10:30:49 INFO  Server:1454 - Auth successful for appattempt_1573813701275_0002_000001 (auth:SIMPLE)
nm_1        | 2019-11-15 10:31:04 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
rm_1        | 2019-11-15 10:30:49 INFO  ApplicationMasterService:274 - AM registration appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:31:04 INFO  NMAuditLogger:89 - USER=hadoop	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000003
rm_1        | 2019-11-15 10:30:49 INFO  RMAuditLogger:137 - USER=hadoop	IP=172.18.0.2	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1573813701275_0002	APPATTEMPTID=appattempt_1573813701275_0002_000001
nm_1        | 2019-11-15 10:31:04 INFO  ContainerImpl:1135 - Container container_1573813701275_0002_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
rm_1        | 2019-11-15 10:30:49 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from LAUNCHED to RUNNING
nm_1        | 2019-11-15 10:31:04 INFO  ApplicationImpl:347 - Removing container_1573813701275_0002_01_000003 from application application_1573813701275_0002
nm_1        | 2019-11-15 10:31:04 INFO  ContainersMonitorImpl:196 - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
rm_1        | 2019-11-15 10:30:49 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from ACCEPTED to RUNNING on event=ATTEMPT_REGISTERED
nm_1        | 2019-11-15 10:31:04 INFO  AuxServices:196 - Got event CONTAINER_STOP for appId application_1573813701275_0002
rm_1        | 2019-11-15 10:30:51 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000002 Container Transitioned from NEW to ALLOCATED
rm_1        | 2019-11-15 10:30:51 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000002
rm_1        | 2019-11-15 10:30:51 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0002_01_000002 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
rm_1        | 2019-11-15 10:30:51 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0002_000001 container=Container: [ContainerId: container_1573813701275_0002_01_000002, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=RACK_LOCAL requestedPartition=
rm_1        | 2019-11-15 10:30:51 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
rm_1        | 2019-11-15 10:30:51 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:52 INFO  NMTokenSecretManagerInRM:200 - Sending NMToken for nodeId : nm:42530 for container : container_1573813701275_0002_01_000002
rm_1        | 2019-11-15 10:30:52 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
rm_1        | 2019-11-15 10:30:52 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000002 Container Transitioned from ACQUIRED to RUNNING
rm_1        | 2019-11-15 10:30:53 INFO  AppSchedulingInfo:204 - checking for deactivate of application :application_1573813701275_0002
rm_1        | 2019-11-15 10:30:57 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000002 Container Transitioned from RUNNING to COMPLETED
rm_1        | 2019-11-15 10:30:57 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0002_01_000002 in state: COMPLETED event:FINISHED
rm_1        | 2019-11-15 10:30:57 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000002
rm_1        | 2019-11-15 10:30:57 INFO  SchedulerNode:216 - Released container container_1573813701275_0002_01_000002 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
rm_1        | 2019-11-15 10:30:57 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
rm_1        | 2019-11-15 10:30:57 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0002_01_000002, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:57 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:57 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
rm_1        | 2019-11-15 10:30:57 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0002_000001 released container container_1573813701275_0002_01_000002 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
rm_1        | 2019-11-15 10:30:59 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000003 Container Transitioned from NEW to ALLOCATED
rm_1        | 2019-11-15 10:30:59 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000003
rm_1        | 2019-11-15 10:30:59 INFO  SchedulerNode:153 - Assigned container container_1573813701275_0002_01_000003 of capacity <memory:4096, vCores:1> on host nm:42530, which has 2 containers, <memory:6144, vCores:2> used and <memory:2048, vCores:6> available after allocation
rm_1        | 2019-11-15 10:30:59 INFO  LeafQueue:1558 - assignedContainer application attempt=appattempt_1573813701275_0002_000001 container=Container: [ContainerId: container_1573813701275_0002_01_000003, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH requestedPartition=
rm_1        | 2019-11-15 10:30:59 INFO  ParentQueue:523 - Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:6144, vCores:2>, usedCapacity=0.75, absoluteUsedCapacity=0.75, numApps=1, numContainers=2
rm_1        | 2019-11-15 10:30:59 INFO  ParentQueue:420 - assignedContainer queue=root usedCapacity=0.75 absoluteUsedCapacity=0.75 used=<memory:6144, vCores:2> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:30:59 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
rm_1        | 2019-11-15 10:31:00 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000003 Container Transitioned from ACQUIRED to RUNNING
rm_1        | 2019-11-15 10:31:00 INFO  AppSchedulingInfo:204 - checking for deactivate of application :application_1573813701275_0002
rm_1        | 2019-11-15 10:31:04 INFO  RMContainerImpl:410 - container_1573813701275_0002_01_000003 Container Transitioned from RUNNING to COMPLETED
rm_1        | 2019-11-15 10:31:04 INFO  FiCaSchedulerApp:113 - Completed container: container_1573813701275_0002_01_000003 in state: COMPLETED event:FINISHED
rm_1        | 2019-11-15 10:31:04 INFO  RMAuditLogger:116 - USER=hadoop	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1573813701275_0002	CONTAINERID=container_1573813701275_0002_01_000003
rm_1        | 2019-11-15 10:31:04 INFO  SchedulerNode:216 - Released container container_1573813701275_0002_01_000003 of capacity <memory:4096, vCores:1> on host nm:42530, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
rm_1        | 2019-11-15 10:31:04 INFO  LeafQueue:1712 - default used=<memory:2048, vCores:1> numContainers=1 user=hadoop user-resources=<memory:2048, vCores:1>
rm_1        | 2019-11-15 10:31:04 INFO  LeafQueue:1663 - completedContainer container=Container: [ContainerId: container_1573813701275_0002_01_000003, NodeId: nm:42530, NodeHttpAddress: nm:8042, Resource: <memory:4096, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 172.18.0.2:42530 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:31:04 INFO  ParentQueue:568 - completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
rm_1        | 2019-11-15 10:31:04 INFO  ParentQueue:585 - Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
rm_1        | 2019-11-15 10:31:04 INFO  CapacityScheduler:1443 - Application attempt appattempt_1573813701275_0002_000001 released container container_1573813701275_0002_01_000003 on node: host: nm:42530 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
rm_1        | 2019-11-15 10:31:05 INFO  RMAppAttemptImpl:1210 - Updating application attempt appattempt_1573813701275_0002_000001 with final state: FINISHING, and exit status: -1000
rm_1        | 2019-11-15 10:31:05 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from RUNNING to FINAL_SAVING
rm_1        | 2019-11-15 10:31:05 INFO  RMAppImpl:996 - Updating application application_1573813701275_0002 with final state: FINISHING
rm_1        | 2019-11-15 10:31:05 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from RUNNING to FINAL_SAVING on event=ATTEMPT_UNREGISTERED
rm_1        | 2019-11-15 10:31:05 INFO  RMStateStore:223 - Updating info for app: application_1573813701275_0002
rm_1        | 2019-11-15 10:31:05 INFO  RMAppAttemptImpl:815 - appattempt_1573813701275_0002_000001 State change from FINAL_SAVING to FINISHING
rm_1        | 2019-11-15 10:31:05 INFO  RMAppImpl:718 - application_1573813701275_0002 State change from FINAL_SAVING to FINISHING on event=APP_UPDATE_SAVED
rm_1        | 2019-11-15 10:31:06 INFO  ApplicationMasterService:354 - application_1573813701275_0002 unregistered successfully. 
